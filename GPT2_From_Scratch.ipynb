{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lập Trình GPT2 TỪ ĐẦU"
      ],
      "metadata": {
        "id": "4Ky_pY0kQs_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "RpZE2i2IQ7n2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "2UZu5OaRPa6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ddce2f-d391-4c18-edb1-82bd50ffabe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --q torch tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown 1-5UxJb_lGFf6iUXGi2-1yfrYy2WpApZT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVSc_OpoRUkZ",
        "outputId": "0889cbfb-4142-46e3-ee30-fd77e68cc4ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-5UxJb_lGFf6iUXGi2-1yfrYy2WpApZT\n",
            "To: /content/chantuoi_docs.txt\n",
            "100% 82.2M/82.2M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chuẩn bị dữ liệu"
      ],
      "metadata": {
        "id": "F6FUkGISTQBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/chantuoi_docs.txt' , 'r', encoding='utf-8') as f:\n",
        "  text = f.read().lower()\n",
        "print(len(text))\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "chXCgYSHR-i1",
        "outputId": "779d76f1-68f2-4e69-d12f-aa2e88ca8201"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62946178\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6 cách massage mắt giúp giảm mệt mỏi, trẻ hóa hiệu quả\\nchia sẻ các cách massage mắt giúp giảm mệt mỏ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'the device is {device}')\n",
        "\n",
        "class GPT2Dataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, block_size = 512, overlap_size = 128 ):\n",
        "    self.text = text\n",
        "    self.tokenizer = tokenizer\n",
        "    self.block_size = block_size - 1 # trừ 1 vì thêm token BOS và EOS\n",
        "    self.overlap_size = overlap_size\n",
        "    self.device = device\n",
        "    # lấy 2 id cuối cùng làm token_id bắt dầu và kết thúc\n",
        "    self.BOS_token = 50249\n",
        "    self.EOS_token = 50250\n",
        "    # tokenize\n",
        "    self.token_ids = tokenizer.encode(text)\n",
        "    print(f'token_ids range {min(self.token_ids)} to {max(self.token_ids)}')\n",
        "    # chia chunks dữ liệu có overlap\n",
        "    self.input_ids , self.target_ids = self.create_chunks()\n",
        "  def create_chunks(self):\n",
        "    # input\n",
        "    input_ids = []\n",
        "    # nhãn\n",
        "    target_ids = []\n",
        "    for i in range(0 , len(self.token_ids), self.overlap_size):\n",
        "      if i + self.block_size + 1 < len(self.token_ids):\n",
        "        input_ids.append(\n",
        "            torch.concat( (torch.tensor([self.BOS_token]), torch.tensor(self.token_ids[i:i+self.block_size]))).to(self.device)\n",
        "        )\n",
        "        target_ids.append(\n",
        "            torch.concat( (torch.tensor(self.token_ids[i+1:i+self.block_size+1]), torch.tensor([self.EOS_token]))).to(self.device)\n",
        "        )\n",
        "    return input_ids , target_ids\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx] , self.target_ids[idx]\n",
        "\n",
        "def train_test_split(dataset , train_ratio = 0.8):\n",
        "  train_size = int(train_ratio * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  print(f'split dataset with train size : {train_size} | test size : {test_size}')\n",
        "  train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "  return train_dataset , test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2LR0ydKWOzX",
        "outputId": "2f77e65e-97cc-48ac-ebfe-65e237dfde20"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "dataset = GPT2Dataset(text , tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWTwjnc9c-nY",
        "outputId": "2115c4f6-2f8d-4186-ef05-5fb2d3cc3655"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_ids range 0 to 50248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset , test_dataset = train_test_split(dataset, 0.8)\n",
        "train_loader = DataLoader(train_dataset , batch_size=16 , shuffle=True)\n",
        "test_loader = DataLoader(test_dataset , batch_size=16 , shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZq3sa9hLJ9",
        "outputId": "65814557-a15c-4606-9132-2f6119e8edfd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split dataset with train size : 341438 | test size : 85360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFzFJcyZwSSN",
        "outputId": "158c2d20-7c4f-4bcc-a1bf-c6ab6701e8a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21340, 5335)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Xây dựng kiến trúc mô hình GPT2\n",
        "* Embedding layer\n",
        "* Positional Embedding layer\n",
        "* Multi-head Attention\n",
        "* Feed-Forward\n",
        "* layer Normalization\n",
        "* Transformer block\n",
        "* GPT2"
      ],
      "metadata": {
        "id": "dNgxacfOniBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self, d_model, max_length = 512 ):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.positional_emb = nn.Embedding(max_length, d_model)\n",
        "\n",
        "  def forward(self, x ):\n",
        "    seq_len = x.shape[1]\n",
        "    positions = torch.arange(0, seq_len, device= x.device).unsqueeze(0)\n",
        "    return x + self.positional_emb(positions)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_heads , d_model):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.fc = nn.Linear(self.n_heads * self.head_dim , self.d_model)\n",
        "  def forward(self , Q, K, V, mask = None):\n",
        "    # các kích thước\n",
        "    batch_size = Q.shape[0]\n",
        "    Q_seq_length = Q.shape[1]\n",
        "    K_seq_length = K.shape[1]\n",
        "    V_seq_length = V.shape[1]\n",
        "    # chia Q, K, V thành n_head\n",
        "    Q_heads = torch.reshape(Q, (batch_size, Q_seq_length, self.n_heads, self.head_dim)).permute(0, 2 , 1 , 3).to(device) #[N, n_head, Q_seq_length , head_dim]\n",
        "    K_heads = torch.reshape(K, (batch_size, K_seq_length, self.n_heads, self.head_dim)).permute(0, 2 , 1 , 3).to(device) #[N, n_head, K_seq_length , head_dim]\n",
        "    V_heads = torch.reshape(V, (batch_size, V_seq_length, self.n_heads, self.head_dim)).permute(0, 2 , 1 , 3).to(device) #[N, n_head, V_seq_length, head_dim]\n",
        "\n",
        "    energy =torch.matmul(Q_heads , K_heads.permute(0, 1 , 3 , 2 )) / (self.d_model ** 0.5) # [N, n_head, Q_seq_length , K_seq_length]\n",
        "\n",
        "    # áp dụng mask\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, -1e20)\n",
        "\n",
        "    attention = torch.softmax(energy , dim = -1) # [N, n_head, Q_seq_length , K_seq_length]\n",
        "\n",
        "    attention = torch.matmul(attention , V_heads) # [N , n_head, Q_seq_length, head_dim ]\n",
        "\n",
        "    # đưa out trờ về kích thước giống với đầu vào ban đầu\n",
        "    out = attention.permute(0 , 2 , 1 , 3 )\n",
        "    out = out.reshape(batch_size , Q_seq_length , self.n_heads * self.head_dim )\n",
        "    return self.fc(out) # [N , Q_seq_length, d_model]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, expand_dim ):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.expand_dim = expand_dim\n",
        "    self.fc_block = nn.Sequential(\n",
        "        nn.Linear(d_model , expand_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(expand_dim , d_model)\n",
        "    )\n",
        "  def forward(self , x):\n",
        "    return self.fc_block(x)\n",
        "\n",
        "class TransformersBlock(nn.Module):\n",
        "  def __init__(self , d_model, expand_dim, n_heads, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.expand_dim = expand_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "    # lần lượt các lớp của transformer\n",
        "    self.attention = MultiHeadAttention(n_heads , d_model)\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "    self.feed_forward = FeedForward(d_model , expand_dim)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "  def forward( self , Q, K , V , mask = None):\n",
        "    \"\"\"\n",
        "      Q, K, V : [N , seq_len, d_model]\n",
        "      mask : [N, 1 , seq_len, seq_len]\n",
        "\n",
        "      return x : [N , seq_len, d_model]\n",
        "    \"\"\"\n",
        "    x = self.attention(Q , K , V , mask)\n",
        "    x = self.dropout(self.layer_norm1(x + Q))\n",
        "    x = self.feed_forward(x)\n",
        "    x = self.dropout(self.layer_norm2(x + Q))\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "1KZZhysvkdD6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2( nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers ,\n",
        "        n_heads ,\n",
        "        d_model ,\n",
        "        expand_dim ,\n",
        "        vocab_size ,\n",
        "        max_length,\n",
        "        dropout_rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.expand_dim = expand_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # các lớp của GPT2\n",
        "        self.embedding = nn.Embedding(vocab_size , d_model)\n",
        "        self.positional_embedding = PositionalEmbedding(d_model , max_length)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformersBlock(d_model , expand_dim , n_heads , dropout_rate) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(d_model , vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self , x , mask = None):\n",
        "        \"\"\"\n",
        "          x : [N, seq_len]\n",
        "          mask : [N, 1, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        N, seq_len = x.shape[ 0 ], x.shape[ 1 ]\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        # cho qua các lớp transformer\n",
        "        for block in self.transformer_blocks:\n",
        "          x = block(x , x , x , mask)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KAJINNX6wqD4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(seq_len):\n",
        "  return torch.tril(torch.ones(seq_len , seq_len)).unsqueeze(0).unsqueeze(0).to(device) # [1, 1 , seq_len, seq_len ]\n"
      ],
      "metadata": {
        "id": "BMT0FVAuysrY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = GPT2(\n",
        "    n_layers = 1,\n",
        "    n_heads = 8,\n",
        "    d_model = 512,\n",
        "    expand_dim = 1024,\n",
        "    vocab_size = tokenizer.n_vocab + 2,\n",
        "    max_length = 512,\n",
        "    dropout_rate = 0.1\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "x0f-Ayru4Kyg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thiết lập quá trình huấn luyện"
      ],
      "metadata": {
        "id": "cKbM4_289PIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters() , lr = 3e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "QXPZp2fT6qD-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "def calculate_perplexity(loss):\n",
        "    return math.exp(loss)\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=3):\n",
        "    best_val_perplexity = float('inf')\n",
        "    best_model_path = 'best_model.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Create a causal mask for the current batch sequence length\n",
        "            batch_size, seq_length = inputs.shape\n",
        "            heads = model.n_heads\n",
        "\n",
        "            mask = create_mask(seq_length).to(device)\n",
        "            mask = mask.expand(batch_size, 1, seq_length, seq_length)\n",
        "\n",
        "            # print(f\"Mask shape: {mask.shape}\")\n",
        "\n",
        "            outputs = model(inputs, mask=mask)\n",
        "            loss = criterion(outputs.transpose(1, 2), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            batch_perplexity = calculate_perplexity(loss.item())\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], \"\n",
        "                  f\"Train Loss: {loss.item():.4f}, Train Perplexity: {batch_perplexity:.4f}\")\n",
        "\n",
        "        # Validation loop with masking\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "                mask = create_mask(inputs.size(1)).to(device)\n",
        "                outputs = model(inputs, mask=mask)\n",
        "                loss = criterion(outputs.transpose(1, 2), targets)\n",
        "                val_loss += loss.item()\n",
        "                batch_perplexity = calculate_perplexity(loss.item())\n",
        "                print(f\"Validation, Batch [{batch_idx + 1}/{len(val_loader)}], \"\n",
        "                      f\"Validation Loss: {loss.item():.4f}, Validation Perplexity: {batch_perplexity:.4f}\")\n",
        "\n",
        "        # Calculate average validation perplexity\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_perplexity = calculate_perplexity(avg_val_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Validation Summary: \"\n",
        "              f\"Average Validation Loss: {avg_val_loss:.4f}, Average Validation Perplexity: {avg_val_perplexity:.4f}\")\n",
        "\n",
        "        # Check if this is the best model so far based on validation perplexity\n",
        "        if avg_val_perplexity < best_val_perplexity:\n",
        "            best_val_perplexity = avg_val_perplexity\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved with validation perplexity: {best_val_perplexity:.4f}\")\n",
        "\n",
        "    print(\"Training complete. Best model saved at:\", best_model_path)\n"
      ],
      "metadata": {
        "id": "eLnvv8_O9w6f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and validate\n",
        "train(model, train_loader, test_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZgDUz1CR8j",
        "outputId": "0b5ec128-95b9-4224-99f9-2b76f7f5c640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [1/3], Batch [6758/21340], Train Loss: 3.3089, Train Perplexity: 27.3560\n",
            "Epoch [1/3], Batch [6759/21340], Train Loss: 3.2738, Train Perplexity: 26.4103\n",
            "Epoch [1/3], Batch [6760/21340], Train Loss: 3.3226, Train Perplexity: 27.7323\n",
            "Epoch [1/3], Batch [6761/21340], Train Loss: 3.2187, Train Perplexity: 24.9957\n",
            "Epoch [1/3], Batch [6762/21340], Train Loss: 3.2357, Train Perplexity: 25.4241\n",
            "Epoch [1/3], Batch [6763/21340], Train Loss: 3.2290, Train Perplexity: 25.2539\n",
            "Epoch [1/3], Batch [6764/21340], Train Loss: 3.2792, Train Perplexity: 26.5536\n",
            "Epoch [1/3], Batch [6765/21340], Train Loss: 3.1489, Train Perplexity: 23.3106\n",
            "Epoch [1/3], Batch [6766/21340], Train Loss: 3.2783, Train Perplexity: 26.5308\n",
            "Epoch [1/3], Batch [6767/21340], Train Loss: 3.2192, Train Perplexity: 25.0072\n",
            "Epoch [1/3], Batch [6768/21340], Train Loss: 3.2610, Train Perplexity: 26.0752\n",
            "Epoch [1/3], Batch [6769/21340], Train Loss: 3.3132, Train Perplexity: 27.4741\n",
            "Epoch [1/3], Batch [6770/21340], Train Loss: 3.2392, Train Perplexity: 25.5131\n",
            "Epoch [1/3], Batch [6771/21340], Train Loss: 3.2084, Train Perplexity: 24.7401\n",
            "Epoch [1/3], Batch [6772/21340], Train Loss: 3.2867, Train Perplexity: 26.7552\n",
            "Epoch [1/3], Batch [6773/21340], Train Loss: 3.1547, Train Perplexity: 23.4461\n",
            "Epoch [1/3], Batch [6774/21340], Train Loss: 3.2595, Train Perplexity: 26.0353\n",
            "Epoch [1/3], Batch [6775/21340], Train Loss: 3.3085, Train Perplexity: 27.3441\n",
            "Epoch [1/3], Batch [6776/21340], Train Loss: 3.3173, Train Perplexity: 27.5848\n",
            "Epoch [1/3], Batch [6777/21340], Train Loss: 3.3551, Train Perplexity: 28.6491\n",
            "Epoch [1/3], Batch [6778/21340], Train Loss: 3.3384, Train Perplexity: 28.1753\n",
            "Epoch [1/3], Batch [6779/21340], Train Loss: 3.3192, Train Perplexity: 27.6379\n",
            "Epoch [1/3], Batch [6780/21340], Train Loss: 3.2785, Train Perplexity: 26.5347\n",
            "Epoch [1/3], Batch [6781/21340], Train Loss: 3.3600, Train Perplexity: 28.7906\n",
            "Epoch [1/3], Batch [6782/21340], Train Loss: 3.2472, Train Perplexity: 25.7180\n",
            "Epoch [1/3], Batch [6783/21340], Train Loss: 3.2601, Train Perplexity: 26.0510\n",
            "Epoch [1/3], Batch [6784/21340], Train Loss: 3.2900, Train Perplexity: 26.8441\n",
            "Epoch [1/3], Batch [6785/21340], Train Loss: 3.2305, Train Perplexity: 25.2921\n",
            "Epoch [1/3], Batch [6786/21340], Train Loss: 3.2202, Train Perplexity: 25.0343\n",
            "Epoch [1/3], Batch [6787/21340], Train Loss: 3.3156, Train Perplexity: 27.5376\n",
            "Epoch [1/3], Batch [6788/21340], Train Loss: 3.2493, Train Perplexity: 25.7723\n",
            "Epoch [1/3], Batch [6789/21340], Train Loss: 3.5312, Train Perplexity: 34.1650\n",
            "Epoch [1/3], Batch [6790/21340], Train Loss: 3.2373, Train Perplexity: 25.4647\n",
            "Epoch [1/3], Batch [6791/21340], Train Loss: 3.3626, Train Perplexity: 28.8648\n",
            "Epoch [1/3], Batch [6792/21340], Train Loss: 3.4304, Train Perplexity: 30.8880\n",
            "Epoch [1/3], Batch [6793/21340], Train Loss: 3.1995, Train Perplexity: 24.5194\n",
            "Epoch [1/3], Batch [6794/21340], Train Loss: 3.2869, Train Perplexity: 26.7607\n",
            "Epoch [1/3], Batch [6795/21340], Train Loss: 3.2142, Train Perplexity: 24.8822\n",
            "Epoch [1/3], Batch [6796/21340], Train Loss: 3.2473, Train Perplexity: 25.7205\n",
            "Epoch [1/3], Batch [6797/21340], Train Loss: 3.3495, Train Perplexity: 28.4891\n",
            "Epoch [1/3], Batch [6798/21340], Train Loss: 3.2167, Train Perplexity: 24.9450\n",
            "Epoch [1/3], Batch [6799/21340], Train Loss: 3.2828, Train Perplexity: 26.6496\n",
            "Epoch [1/3], Batch [6800/21340], Train Loss: 3.1571, Train Perplexity: 23.5028\n",
            "Epoch [1/3], Batch [6801/21340], Train Loss: 3.1839, Train Perplexity: 24.1407\n",
            "Epoch [1/3], Batch [6802/21340], Train Loss: 3.3092, Train Perplexity: 27.3623\n",
            "Epoch [1/3], Batch [6803/21340], Train Loss: 3.2905, Train Perplexity: 26.8561\n",
            "Epoch [1/3], Batch [6804/21340], Train Loss: 3.1795, Train Perplexity: 24.0349\n",
            "Epoch [1/3], Batch [6805/21340], Train Loss: 3.3404, Train Perplexity: 28.2299\n",
            "Epoch [1/3], Batch [6806/21340], Train Loss: 3.2201, Train Perplexity: 25.0310\n",
            "Epoch [1/3], Batch [6807/21340], Train Loss: 3.1908, Train Perplexity: 24.3069\n",
            "Epoch [1/3], Batch [6808/21340], Train Loss: 3.2439, Train Perplexity: 25.6341\n",
            "Epoch [1/3], Batch [6809/21340], Train Loss: 3.2887, Train Perplexity: 26.8091\n",
            "Epoch [1/3], Batch [6810/21340], Train Loss: 3.3609, Train Perplexity: 28.8139\n",
            "Epoch [1/3], Batch [6811/21340], Train Loss: 3.2457, Train Perplexity: 25.6806\n",
            "Epoch [1/3], Batch [6812/21340], Train Loss: 3.2971, Train Perplexity: 27.0341\n",
            "Epoch [1/3], Batch [6813/21340], Train Loss: 3.3701, Train Perplexity: 29.0819\n",
            "Epoch [1/3], Batch [6814/21340], Train Loss: 3.2480, Train Perplexity: 25.7385\n",
            "Epoch [1/3], Batch [6815/21340], Train Loss: 3.2845, Train Perplexity: 26.6962\n",
            "Epoch [1/3], Batch [6816/21340], Train Loss: 3.3541, Train Perplexity: 28.6189\n",
            "Epoch [1/3], Batch [6817/21340], Train Loss: 3.3932, Train Perplexity: 29.7602\n",
            "Epoch [1/3], Batch [6818/21340], Train Loss: 3.1881, Train Perplexity: 24.2414\n",
            "Epoch [1/3], Batch [6819/21340], Train Loss: 3.2295, Train Perplexity: 25.2675\n",
            "Epoch [1/3], Batch [6820/21340], Train Loss: 3.2841, Train Perplexity: 26.6861\n",
            "Epoch [1/3], Batch [6821/21340], Train Loss: 3.3096, Train Perplexity: 27.3733\n",
            "Epoch [1/3], Batch [6822/21340], Train Loss: 3.3913, Train Perplexity: 29.7036\n",
            "Epoch [1/3], Batch [6823/21340], Train Loss: 3.1812, Train Perplexity: 24.0754\n",
            "Epoch [1/3], Batch [6824/21340], Train Loss: 3.3813, Train Perplexity: 29.4093\n",
            "Epoch [1/3], Batch [6825/21340], Train Loss: 3.2730, Train Perplexity: 26.3899\n",
            "Epoch [1/3], Batch [6826/21340], Train Loss: 3.2546, Train Perplexity: 25.9084\n",
            "Epoch [1/3], Batch [6827/21340], Train Loss: 3.1962, Train Perplexity: 24.4391\n",
            "Epoch [1/3], Batch [6828/21340], Train Loss: 3.1776, Train Perplexity: 23.9893\n",
            "Epoch [1/3], Batch [6829/21340], Train Loss: 3.2292, Train Perplexity: 25.2595\n",
            "Epoch [1/3], Batch [6830/21340], Train Loss: 3.2045, Train Perplexity: 24.6423\n",
            "Epoch [1/3], Batch [6831/21340], Train Loss: 3.2604, Train Perplexity: 26.0612\n",
            "Epoch [1/3], Batch [6832/21340], Train Loss: 3.2239, Train Perplexity: 25.1260\n",
            "Epoch [1/3], Batch [6833/21340], Train Loss: 3.1607, Train Perplexity: 23.5872\n",
            "Epoch [1/3], Batch [6834/21340], Train Loss: 3.2381, Train Perplexity: 25.4854\n",
            "Epoch [1/3], Batch [6835/21340], Train Loss: 3.2617, Train Perplexity: 26.0937\n",
            "Epoch [1/3], Batch [6836/21340], Train Loss: 3.3931, Train Perplexity: 29.7569\n",
            "Epoch [1/3], Batch [6837/21340], Train Loss: 3.2112, Train Perplexity: 24.8094\n",
            "Epoch [1/3], Batch [6838/21340], Train Loss: 3.1388, Train Perplexity: 23.0768\n",
            "Epoch [1/3], Batch [6839/21340], Train Loss: 3.3148, Train Perplexity: 27.5172\n",
            "Epoch [1/3], Batch [6840/21340], Train Loss: 3.2966, Train Perplexity: 27.0198\n",
            "Epoch [1/3], Batch [6841/21340], Train Loss: 3.2676, Train Perplexity: 26.2483\n",
            "Epoch [1/3], Batch [6842/21340], Train Loss: 3.3986, Train Perplexity: 29.9237\n",
            "Epoch [1/3], Batch [6843/21340], Train Loss: 3.3474, Train Perplexity: 28.4286\n",
            "Epoch [1/3], Batch [6844/21340], Train Loss: 3.2824, Train Perplexity: 26.6406\n",
            "Epoch [1/3], Batch [6845/21340], Train Loss: 3.2674, Train Perplexity: 26.2422\n",
            "Epoch [1/3], Batch [6846/21340], Train Loss: 3.2161, Train Perplexity: 24.9317\n",
            "Epoch [1/3], Batch [6847/21340], Train Loss: 3.2213, Train Perplexity: 25.0600\n",
            "Epoch [1/3], Batch [6848/21340], Train Loss: 3.2223, Train Perplexity: 25.0853\n",
            "Epoch [1/3], Batch [6849/21340], Train Loss: 3.1895, Train Perplexity: 24.2767\n",
            "Epoch [1/3], Batch [6850/21340], Train Loss: 3.2056, Train Perplexity: 24.6709\n",
            "Epoch [1/3], Batch [6851/21340], Train Loss: 3.1919, Train Perplexity: 24.3337\n",
            "Epoch [1/3], Batch [6852/21340], Train Loss: 3.2198, Train Perplexity: 25.0232\n",
            "Epoch [1/3], Batch [6853/21340], Train Loss: 3.3076, Train Perplexity: 27.3188\n",
            "Epoch [1/3], Batch [6854/21340], Train Loss: 3.2977, Train Perplexity: 27.0500\n",
            "Epoch [1/3], Batch [6855/21340], Train Loss: 3.1898, Train Perplexity: 24.2842\n",
            "Epoch [1/3], Batch [6856/21340], Train Loss: 3.3592, Train Perplexity: 28.7666\n",
            "Epoch [1/3], Batch [6857/21340], Train Loss: 3.1421, Train Perplexity: 23.1527\n",
            "Epoch [1/3], Batch [6858/21340], Train Loss: 3.2459, Train Perplexity: 25.6838\n",
            "Epoch [1/3], Batch [6859/21340], Train Loss: 3.1830, Train Perplexity: 24.1185\n",
            "Epoch [1/3], Batch [6860/21340], Train Loss: 3.3746, Train Perplexity: 29.2137\n",
            "Epoch [1/3], Batch [6861/21340], Train Loss: 3.3965, Train Perplexity: 29.8584\n",
            "Epoch [1/3], Batch [6862/21340], Train Loss: 3.1956, Train Perplexity: 24.4245\n",
            "Epoch [1/3], Batch [6863/21340], Train Loss: 3.4105, Train Perplexity: 30.2794\n",
            "Epoch [1/3], Batch [6864/21340], Train Loss: 3.5360, Train Perplexity: 34.3279\n",
            "Epoch [1/3], Batch [6865/21340], Train Loss: 3.1932, Train Perplexity: 24.3665\n",
            "Epoch [1/3], Batch [6866/21340], Train Loss: 3.2264, Train Perplexity: 25.1889\n",
            "Epoch [1/3], Batch [6867/21340], Train Loss: 3.1345, Train Perplexity: 22.9771\n",
            "Epoch [1/3], Batch [6868/21340], Train Loss: 3.2158, Train Perplexity: 24.9229\n",
            "Epoch [1/3], Batch [6869/21340], Train Loss: 3.2526, Train Perplexity: 25.8568\n",
            "Epoch [1/3], Batch [6870/21340], Train Loss: 3.2297, Train Perplexity: 25.2717\n",
            "Epoch [1/3], Batch [6871/21340], Train Loss: 3.2251, Train Perplexity: 25.1571\n",
            "Epoch [1/3], Batch [6872/21340], Train Loss: 3.2147, Train Perplexity: 24.8954\n",
            "Epoch [1/3], Batch [6873/21340], Train Loss: 3.1846, Train Perplexity: 24.1587\n",
            "Epoch [1/3], Batch [6874/21340], Train Loss: 3.2659, Train Perplexity: 26.2038\n",
            "Epoch [1/3], Batch [6875/21340], Train Loss: 3.2257, Train Perplexity: 25.1721\n",
            "Epoch [1/3], Batch [6876/21340], Train Loss: 3.2317, Train Perplexity: 25.3228\n",
            "Epoch [1/3], Batch [6877/21340], Train Loss: 3.3120, Train Perplexity: 27.4393\n",
            "Epoch [1/3], Batch [6878/21340], Train Loss: 3.3350, Train Perplexity: 28.0779\n",
            "Epoch [1/3], Batch [6879/21340], Train Loss: 3.3259, Train Perplexity: 27.8241\n",
            "Epoch [1/3], Batch [6880/21340], Train Loss: 3.1455, Train Perplexity: 23.2317\n",
            "Epoch [1/3], Batch [6881/21340], Train Loss: 3.2757, Train Perplexity: 26.4611\n",
            "Epoch [1/3], Batch [6882/21340], Train Loss: 3.2146, Train Perplexity: 24.8941\n",
            "Epoch [1/3], Batch [6883/21340], Train Loss: 3.3113, Train Perplexity: 27.4205\n",
            "Epoch [1/3], Batch [6884/21340], Train Loss: 3.2820, Train Perplexity: 26.6280\n",
            "Epoch [1/3], Batch [6885/21340], Train Loss: 3.3009, Train Perplexity: 27.1360\n",
            "Epoch [1/3], Batch [6886/21340], Train Loss: 3.2162, Train Perplexity: 24.9325\n",
            "Epoch [1/3], Batch [6887/21340], Train Loss: 3.1923, Train Perplexity: 24.3441\n",
            "Epoch [1/3], Batch [6888/21340], Train Loss: 3.2395, Train Perplexity: 25.5216\n",
            "Epoch [1/3], Batch [6889/21340], Train Loss: 3.1740, Train Perplexity: 23.9031\n",
            "Epoch [1/3], Batch [6890/21340], Train Loss: 3.2197, Train Perplexity: 25.0217\n",
            "Epoch [1/3], Batch [6891/21340], Train Loss: 3.3082, Train Perplexity: 27.3370\n",
            "Epoch [1/3], Batch [6892/21340], Train Loss: 3.2901, Train Perplexity: 26.8446\n",
            "Epoch [1/3], Batch [6893/21340], Train Loss: 3.1378, Train Perplexity: 23.0523\n",
            "Epoch [1/3], Batch [6894/21340], Train Loss: 3.2236, Train Perplexity: 25.1175\n",
            "Epoch [1/3], Batch [6895/21340], Train Loss: 3.1989, Train Perplexity: 24.5057\n",
            "Epoch [1/3], Batch [6896/21340], Train Loss: 3.3143, Train Perplexity: 27.5037\n",
            "Epoch [1/3], Batch [6897/21340], Train Loss: 3.3285, Train Perplexity: 27.8966\n",
            "Epoch [1/3], Batch [6898/21340], Train Loss: 3.1724, Train Perplexity: 23.8639\n",
            "Epoch [1/3], Batch [6899/21340], Train Loss: 3.2789, Train Perplexity: 26.5459\n",
            "Epoch [1/3], Batch [6900/21340], Train Loss: 3.2180, Train Perplexity: 24.9784\n",
            "Epoch [1/3], Batch [6901/21340], Train Loss: 3.2226, Train Perplexity: 25.0936\n",
            "Epoch [1/3], Batch [6902/21340], Train Loss: 3.2481, Train Perplexity: 25.7407\n",
            "Epoch [1/3], Batch [6903/21340], Train Loss: 3.2392, Train Perplexity: 25.5123\n",
            "Epoch [1/3], Batch [6904/21340], Train Loss: 3.2638, Train Perplexity: 26.1476\n",
            "Epoch [1/3], Batch [6905/21340], Train Loss: 3.1803, Train Perplexity: 24.0542\n",
            "Epoch [1/3], Batch [6906/21340], Train Loss: 3.2957, Train Perplexity: 26.9976\n",
            "Epoch [1/3], Batch [6907/21340], Train Loss: 3.4050, Train Perplexity: 30.1154\n",
            "Epoch [1/3], Batch [6908/21340], Train Loss: 3.1654, Train Perplexity: 23.6974\n",
            "Epoch [1/3], Batch [6909/21340], Train Loss: 3.1544, Train Perplexity: 23.4399\n",
            "Epoch [1/3], Batch [6910/21340], Train Loss: 3.5531, Train Perplexity: 34.9203\n",
            "Epoch [1/3], Batch [6911/21340], Train Loss: 3.2991, Train Perplexity: 27.0886\n",
            "Epoch [1/3], Batch [6912/21340], Train Loss: 3.2245, Train Perplexity: 25.1422\n",
            "Epoch [1/3], Batch [6913/21340], Train Loss: 3.2418, Train Perplexity: 25.5807\n",
            "Epoch [1/3], Batch [6914/21340], Train Loss: 3.2679, Train Perplexity: 26.2550\n",
            "Epoch [1/3], Batch [6915/21340], Train Loss: 3.2526, Train Perplexity: 25.8571\n",
            "Epoch [1/3], Batch [6916/21340], Train Loss: 3.3224, Train Perplexity: 27.7268\n",
            "Epoch [1/3], Batch [6917/21340], Train Loss: 3.5174, Train Perplexity: 33.6976\n",
            "Epoch [1/3], Batch [6918/21340], Train Loss: 3.2172, Train Perplexity: 24.9571\n",
            "Epoch [1/3], Batch [6919/21340], Train Loss: 3.2946, Train Perplexity: 26.9671\n",
            "Epoch [1/3], Batch [6920/21340], Train Loss: 3.2247, Train Perplexity: 25.1454\n",
            "Epoch [1/3], Batch [6921/21340], Train Loss: 3.4100, Train Perplexity: 30.2658\n",
            "Epoch [1/3], Batch [6922/21340], Train Loss: 3.1894, Train Perplexity: 24.2745\n",
            "Epoch [1/3], Batch [6923/21340], Train Loss: 3.2802, Train Perplexity: 26.5803\n",
            "Epoch [1/3], Batch [6924/21340], Train Loss: 3.4521, Train Perplexity: 31.5673\n",
            "Epoch [1/3], Batch [6925/21340], Train Loss: 3.3572, Train Perplexity: 28.7076\n",
            "Epoch [1/3], Batch [6926/21340], Train Loss: 3.3202, Train Perplexity: 27.6647\n",
            "Epoch [1/3], Batch [6927/21340], Train Loss: 3.2545, Train Perplexity: 25.9058\n",
            "Epoch [1/3], Batch [6928/21340], Train Loss: 3.1896, Train Perplexity: 24.2790\n",
            "Epoch [1/3], Batch [6929/21340], Train Loss: 3.2626, Train Perplexity: 26.1164\n",
            "Epoch [1/3], Batch [6930/21340], Train Loss: 3.2438, Train Perplexity: 25.6320\n",
            "Epoch [1/3], Batch [6931/21340], Train Loss: 3.1751, Train Perplexity: 23.9293\n",
            "Epoch [1/3], Batch [6932/21340], Train Loss: 3.2528, Train Perplexity: 25.8620\n",
            "Epoch [1/3], Batch [6933/21340], Train Loss: 3.2606, Train Perplexity: 26.0661\n",
            "Epoch [1/3], Batch [6934/21340], Train Loss: 3.2269, Train Perplexity: 25.2017\n",
            "Epoch [1/3], Batch [6935/21340], Train Loss: 3.2635, Train Perplexity: 26.1414\n",
            "Epoch [1/3], Batch [6936/21340], Train Loss: 3.2361, Train Perplexity: 25.4340\n",
            "Epoch [1/3], Batch [6937/21340], Train Loss: 3.2586, Train Perplexity: 26.0135\n",
            "Epoch [1/3], Batch [6938/21340], Train Loss: 3.3862, Train Perplexity: 29.5539\n",
            "Epoch [1/3], Batch [6939/21340], Train Loss: 3.3743, Train Perplexity: 29.2037\n",
            "Epoch [1/3], Batch [6940/21340], Train Loss: 3.2658, Train Perplexity: 26.2007\n",
            "Epoch [1/3], Batch [6941/21340], Train Loss: 3.2874, Train Perplexity: 26.7718\n",
            "Epoch [1/3], Batch [6942/21340], Train Loss: 3.3540, Train Perplexity: 28.6178\n",
            "Epoch [1/3], Batch [6943/21340], Train Loss: 3.1600, Train Perplexity: 23.5702\n",
            "Epoch [1/3], Batch [6944/21340], Train Loss: 3.2151, Train Perplexity: 24.9060\n",
            "Epoch [1/3], Batch [6945/21340], Train Loss: 3.1595, Train Perplexity: 23.5594\n",
            "Epoch [1/3], Batch [6946/21340], Train Loss: 3.2512, Train Perplexity: 25.8217\n",
            "Epoch [1/3], Batch [6947/21340], Train Loss: 3.3766, Train Perplexity: 29.2714\n",
            "Epoch [1/3], Batch [6948/21340], Train Loss: 3.3580, Train Perplexity: 28.7312\n",
            "Epoch [1/3], Batch [6949/21340], Train Loss: 3.2328, Train Perplexity: 25.3498\n",
            "Epoch [1/3], Batch [6950/21340], Train Loss: 3.2384, Train Perplexity: 25.4923\n",
            "Epoch [1/3], Batch [6951/21340], Train Loss: 3.2071, Train Perplexity: 24.7065\n",
            "Epoch [1/3], Batch [6952/21340], Train Loss: 3.2445, Train Perplexity: 25.6479\n",
            "Epoch [1/3], Batch [6953/21340], Train Loss: 3.2389, Train Perplexity: 25.5062\n",
            "Epoch [1/3], Batch [6954/21340], Train Loss: 3.3419, Train Perplexity: 28.2729\n",
            "Epoch [1/3], Batch [6955/21340], Train Loss: 3.2391, Train Perplexity: 25.5110\n",
            "Epoch [1/3], Batch [6956/21340], Train Loss: 3.2475, Train Perplexity: 25.7265\n",
            "Epoch [1/3], Batch [6957/21340], Train Loss: 3.2200, Train Perplexity: 25.0273\n",
            "Epoch [1/3], Batch [6958/21340], Train Loss: 3.2389, Train Perplexity: 25.5056\n",
            "Epoch [1/3], Batch [6959/21340], Train Loss: 3.4085, Train Perplexity: 30.2188\n",
            "Epoch [1/3], Batch [6960/21340], Train Loss: 3.3024, Train Perplexity: 27.1765\n",
            "Epoch [1/3], Batch [6961/21340], Train Loss: 3.2743, Train Perplexity: 26.4257\n",
            "Epoch [1/3], Batch [6962/21340], Train Loss: 3.2580, Train Perplexity: 25.9967\n",
            "Epoch [1/3], Batch [6963/21340], Train Loss: 3.2341, Train Perplexity: 25.3830\n",
            "Epoch [1/3], Batch [6964/21340], Train Loss: 3.2385, Train Perplexity: 25.4946\n",
            "Epoch [1/3], Batch [6965/21340], Train Loss: 3.3169, Train Perplexity: 27.5757\n",
            "Epoch [1/3], Batch [6966/21340], Train Loss: 3.3096, Train Perplexity: 27.3746\n",
            "Epoch [1/3], Batch [6967/21340], Train Loss: 3.2450, Train Perplexity: 25.6608\n",
            "Epoch [1/3], Batch [6968/21340], Train Loss: 3.2351, Train Perplexity: 25.4088\n",
            "Epoch [1/3], Batch [6969/21340], Train Loss: 3.3357, Train Perplexity: 28.0972\n",
            "Epoch [1/3], Batch [6970/21340], Train Loss: 3.2832, Train Perplexity: 26.6600\n",
            "Epoch [1/3], Batch [6971/21340], Train Loss: 3.2408, Train Perplexity: 25.5550\n",
            "Epoch [1/3], Batch [6972/21340], Train Loss: 3.3199, Train Perplexity: 27.6579\n",
            "Epoch [1/3], Batch [6973/21340], Train Loss: 3.1891, Train Perplexity: 24.2660\n",
            "Epoch [1/3], Batch [6974/21340], Train Loss: 3.2704, Train Perplexity: 26.3217\n",
            "Epoch [1/3], Batch [6975/21340], Train Loss: 3.2463, Train Perplexity: 25.6941\n",
            "Epoch [1/3], Batch [6976/21340], Train Loss: 3.2905, Train Perplexity: 26.8561\n",
            "Epoch [1/3], Batch [6977/21340], Train Loss: 3.2346, Train Perplexity: 25.3970\n",
            "Epoch [1/3], Batch [6978/21340], Train Loss: 3.3513, Train Perplexity: 28.5401\n",
            "Epoch [1/3], Batch [6979/21340], Train Loss: 3.2804, Train Perplexity: 26.5876\n",
            "Epoch [1/3], Batch [6980/21340], Train Loss: 3.2888, Train Perplexity: 26.8096\n",
            "Epoch [1/3], Batch [6981/21340], Train Loss: 3.1760, Train Perplexity: 23.9518\n",
            "Epoch [1/3], Batch [6982/21340], Train Loss: 3.2598, Train Perplexity: 26.0431\n",
            "Epoch [1/3], Batch [6983/21340], Train Loss: 3.3139, Train Perplexity: 27.4917\n",
            "Epoch [1/3], Batch [6984/21340], Train Loss: 3.3148, Train Perplexity: 27.5174\n",
            "Epoch [1/3], Batch [6985/21340], Train Loss: 3.2518, Train Perplexity: 25.8364\n",
            "Epoch [1/3], Batch [6986/21340], Train Loss: 3.2301, Train Perplexity: 25.2812\n",
            "Epoch [1/3], Batch [6987/21340], Train Loss: 3.2255, Train Perplexity: 25.1660\n",
            "Epoch [1/3], Batch [6988/21340], Train Loss: 3.2864, Train Perplexity: 26.7460\n",
            "Epoch [1/3], Batch [6989/21340], Train Loss: 3.3748, Train Perplexity: 29.2192\n",
            "Epoch [1/3], Batch [6990/21340], Train Loss: 3.2722, Train Perplexity: 26.3692\n",
            "Epoch [1/3], Batch [6991/21340], Train Loss: 3.4929, Train Perplexity: 32.8819\n",
            "Epoch [1/3], Batch [6992/21340], Train Loss: 3.2716, Train Perplexity: 26.3529\n",
            "Epoch [1/3], Batch [6993/21340], Train Loss: 3.2382, Train Perplexity: 25.4868\n",
            "Epoch [1/3], Batch [6994/21340], Train Loss: 3.3189, Train Perplexity: 27.6300\n",
            "Epoch [1/3], Batch [6995/21340], Train Loss: 3.3165, Train Perplexity: 27.5641\n",
            "Epoch [1/3], Batch [6996/21340], Train Loss: 3.2173, Train Perplexity: 24.9612\n",
            "Epoch [1/3], Batch [6997/21340], Train Loss: 3.2050, Train Perplexity: 24.6558\n",
            "Epoch [1/3], Batch [6998/21340], Train Loss: 3.3246, Train Perplexity: 27.7890\n",
            "Epoch [1/3], Batch [6999/21340], Train Loss: 3.3246, Train Perplexity: 27.7877\n",
            "Epoch [1/3], Batch [7000/21340], Train Loss: 3.2250, Train Perplexity: 25.1544\n",
            "Epoch [1/3], Batch [7001/21340], Train Loss: 3.3224, Train Perplexity: 27.7255\n",
            "Epoch [1/3], Batch [7002/21340], Train Loss: 3.1883, Train Perplexity: 24.2467\n",
            "Epoch [1/3], Batch [7003/21340], Train Loss: 3.1616, Train Perplexity: 23.6086\n",
            "Epoch [1/3], Batch [7004/21340], Train Loss: 3.2423, Train Perplexity: 25.5928\n",
            "Epoch [1/3], Batch [7005/21340], Train Loss: 3.2037, Train Perplexity: 24.6227\n",
            "Epoch [1/3], Batch [7006/21340], Train Loss: 3.3827, Train Perplexity: 29.4506\n",
            "Epoch [1/3], Batch [7007/21340], Train Loss: 3.3273, Train Perplexity: 27.8629\n",
            "Epoch [1/3], Batch [7008/21340], Train Loss: 3.1821, Train Perplexity: 24.0968\n",
            "Epoch [1/3], Batch [7009/21340], Train Loss: 3.2230, Train Perplexity: 25.1040\n",
            "Epoch [1/3], Batch [7010/21340], Train Loss: 3.3020, Train Perplexity: 27.1675\n",
            "Epoch [1/3], Batch [7011/21340], Train Loss: 3.2762, Train Perplexity: 26.4758\n",
            "Epoch [1/3], Batch [7012/21340], Train Loss: 3.2674, Train Perplexity: 26.2430\n",
            "Epoch [1/3], Batch [7013/21340], Train Loss: 3.2299, Train Perplexity: 25.2767\n",
            "Epoch [1/3], Batch [7014/21340], Train Loss: 3.2129, Train Perplexity: 24.8506\n",
            "Epoch [1/3], Batch [7015/21340], Train Loss: 3.1854, Train Perplexity: 24.1774\n",
            "Epoch [1/3], Batch [7016/21340], Train Loss: 3.3331, Train Perplexity: 28.0243\n",
            "Epoch [1/3], Batch [7017/21340], Train Loss: 3.1878, Train Perplexity: 24.2361\n",
            "Epoch [1/3], Batch [7018/21340], Train Loss: 3.2685, Train Perplexity: 26.2724\n",
            "Epoch [1/3], Batch [7019/21340], Train Loss: 3.3241, Train Perplexity: 27.7737\n",
            "Epoch [1/3], Batch [7020/21340], Train Loss: 3.3856, Train Perplexity: 29.5363\n",
            "Epoch [1/3], Batch [7021/21340], Train Loss: 3.3317, Train Perplexity: 27.9852\n",
            "Epoch [1/3], Batch [7022/21340], Train Loss: 3.2837, Train Perplexity: 26.6746\n",
            "Epoch [1/3], Batch [7023/21340], Train Loss: 3.2035, Train Perplexity: 24.6180\n",
            "Epoch [1/3], Batch [7024/21340], Train Loss: 3.2553, Train Perplexity: 25.9262\n",
            "Epoch [1/3], Batch [7025/21340], Train Loss: 3.2498, Train Perplexity: 25.7846\n",
            "Epoch [1/3], Batch [7026/21340], Train Loss: 3.2592, Train Perplexity: 26.0280\n",
            "Epoch [1/3], Batch [7027/21340], Train Loss: 3.3600, Train Perplexity: 28.7883\n",
            "Epoch [1/3], Batch [7028/21340], Train Loss: 3.2210, Train Perplexity: 25.0532\n",
            "Epoch [1/3], Batch [7029/21340], Train Loss: 3.4709, Train Perplexity: 32.1652\n",
            "Epoch [1/3], Batch [7030/21340], Train Loss: 3.2543, Train Perplexity: 25.9026\n",
            "Epoch [1/3], Batch [7031/21340], Train Loss: 3.2637, Train Perplexity: 26.1466\n",
            "Epoch [1/3], Batch [7032/21340], Train Loss: 3.3310, Train Perplexity: 27.9667\n",
            "Epoch [1/3], Batch [7033/21340], Train Loss: 3.2640, Train Perplexity: 26.1546\n",
            "Epoch [1/3], Batch [7034/21340], Train Loss: 3.3132, Train Perplexity: 27.4737\n",
            "Epoch [1/3], Batch [7035/21340], Train Loss: 3.1807, Train Perplexity: 24.0632\n",
            "Epoch [1/3], Batch [7036/21340], Train Loss: 3.3807, Train Perplexity: 29.3913\n",
            "Epoch [1/3], Batch [7037/21340], Train Loss: 3.2864, Train Perplexity: 26.7461\n",
            "Epoch [1/3], Batch [7038/21340], Train Loss: 3.2574, Train Perplexity: 25.9827\n",
            "Epoch [1/3], Batch [7039/21340], Train Loss: 3.1634, Train Perplexity: 23.6514\n",
            "Epoch [1/3], Batch [7040/21340], Train Loss: 3.2272, Train Perplexity: 25.2086\n",
            "Epoch [1/3], Batch [7041/21340], Train Loss: 3.2417, Train Perplexity: 25.5780\n",
            "Epoch [1/3], Batch [7042/21340], Train Loss: 3.1081, Train Perplexity: 22.3785\n",
            "Epoch [1/3], Batch [7043/21340], Train Loss: 3.2186, Train Perplexity: 24.9922\n",
            "Epoch [1/3], Batch [7044/21340], Train Loss: 3.2322, Train Perplexity: 25.3353\n",
            "Epoch [1/3], Batch [7045/21340], Train Loss: 3.2807, Train Perplexity: 26.5932\n",
            "Epoch [1/3], Batch [7046/21340], Train Loss: 3.3618, Train Perplexity: 28.8423\n",
            "Epoch [1/3], Batch [7047/21340], Train Loss: 3.3005, Train Perplexity: 27.1249\n",
            "Epoch [1/3], Batch [7048/21340], Train Loss: 3.3003, Train Perplexity: 27.1195\n",
            "Epoch [1/3], Batch [7049/21340], Train Loss: 3.4684, Train Perplexity: 32.0862\n",
            "Epoch [1/3], Batch [7050/21340], Train Loss: 3.1721, Train Perplexity: 23.8566\n",
            "Epoch [1/3], Batch [7051/21340], Train Loss: 3.2715, Train Perplexity: 26.3501\n",
            "Epoch [1/3], Batch [7052/21340], Train Loss: 3.2532, Train Perplexity: 25.8740\n",
            "Epoch [1/3], Batch [7053/21340], Train Loss: 3.3676, Train Perplexity: 29.0092\n",
            "Epoch [1/3], Batch [7054/21340], Train Loss: 3.2628, Train Perplexity: 26.1237\n",
            "Epoch [1/3], Batch [7055/21340], Train Loss: 3.2445, Train Perplexity: 25.6502\n",
            "Epoch [1/3], Batch [7056/21340], Train Loss: 3.1957, Train Perplexity: 24.4274\n",
            "Epoch [1/3], Batch [7057/21340], Train Loss: 3.2238, Train Perplexity: 25.1233\n",
            "Epoch [1/3], Batch [7058/21340], Train Loss: 3.2840, Train Perplexity: 26.6813\n",
            "Epoch [1/3], Batch [7059/21340], Train Loss: 3.2222, Train Perplexity: 25.0829\n",
            "Epoch [1/3], Batch [7060/21340], Train Loss: 3.2939, Train Perplexity: 26.9473\n",
            "Epoch [1/3], Batch [7061/21340], Train Loss: 3.2281, Train Perplexity: 25.2324\n",
            "Epoch [1/3], Batch [7062/21340], Train Loss: 3.1418, Train Perplexity: 23.1453\n",
            "Epoch [1/3], Batch [7063/21340], Train Loss: 3.2824, Train Perplexity: 26.6402\n",
            "Epoch [1/3], Batch [7064/21340], Train Loss: 3.2128, Train Perplexity: 24.8489\n",
            "Epoch [1/3], Batch [7065/21340], Train Loss: 3.3994, Train Perplexity: 29.9470\n",
            "Epoch [1/3], Batch [7066/21340], Train Loss: 3.3241, Train Perplexity: 27.7744\n",
            "Epoch [1/3], Batch [7067/21340], Train Loss: 3.2970, Train Perplexity: 27.0306\n",
            "Epoch [1/3], Batch [7068/21340], Train Loss: 3.2609, Train Perplexity: 26.0721\n",
            "Epoch [1/3], Batch [7069/21340], Train Loss: 3.2561, Train Perplexity: 25.9482\n",
            "Epoch [1/3], Batch [7070/21340], Train Loss: 3.2151, Train Perplexity: 24.9059\n",
            "Epoch [1/3], Batch [7071/21340], Train Loss: 3.2505, Train Perplexity: 25.8032\n",
            "Epoch [1/3], Batch [7072/21340], Train Loss: 3.2391, Train Perplexity: 25.5101\n",
            "Epoch [1/3], Batch [7073/21340], Train Loss: 3.3099, Train Perplexity: 27.3825\n",
            "Epoch [1/3], Batch [7074/21340], Train Loss: 3.2410, Train Perplexity: 25.5604\n",
            "Epoch [1/3], Batch [7075/21340], Train Loss: 3.2750, Train Perplexity: 26.4445\n",
            "Epoch [1/3], Batch [7076/21340], Train Loss: 3.2424, Train Perplexity: 25.5960\n",
            "Epoch [1/3], Batch [7077/21340], Train Loss: 3.2042, Train Perplexity: 24.6348\n",
            "Epoch [1/3], Batch [7078/21340], Train Loss: 3.1982, Train Perplexity: 24.4893\n",
            "Epoch [1/3], Batch [7079/21340], Train Loss: 3.2489, Train Perplexity: 25.7626\n",
            "Epoch [1/3], Batch [7080/21340], Train Loss: 3.2731, Train Perplexity: 26.3938\n",
            "Epoch [1/3], Batch [7081/21340], Train Loss: 3.2396, Train Perplexity: 25.5222\n",
            "Epoch [1/3], Batch [7082/21340], Train Loss: 3.1955, Train Perplexity: 24.4212\n",
            "Epoch [1/3], Batch [7083/21340], Train Loss: 3.2104, Train Perplexity: 24.7881\n",
            "Epoch [1/3], Batch [7084/21340], Train Loss: 3.2582, Train Perplexity: 26.0035\n",
            "Epoch [1/3], Batch [7085/21340], Train Loss: 3.3128, Train Perplexity: 27.4623\n",
            "Epoch [1/3], Batch [7086/21340], Train Loss: 3.2682, Train Perplexity: 26.2640\n",
            "Epoch [1/3], Batch [7087/21340], Train Loss: 3.4483, Train Perplexity: 31.4457\n",
            "Epoch [1/3], Batch [7088/21340], Train Loss: 3.2424, Train Perplexity: 25.5957\n",
            "Epoch [1/3], Batch [7089/21340], Train Loss: 3.2953, Train Perplexity: 26.9849\n",
            "Epoch [1/3], Batch [7090/21340], Train Loss: 3.2130, Train Perplexity: 24.8540\n",
            "Epoch [1/3], Batch [7091/21340], Train Loss: 3.2676, Train Perplexity: 26.2491\n",
            "Epoch [1/3], Batch [7092/21340], Train Loss: 3.2059, Train Perplexity: 24.6787\n",
            "Epoch [1/3], Batch [7093/21340], Train Loss: 3.1897, Train Perplexity: 24.2810\n",
            "Epoch [1/3], Batch [7094/21340], Train Loss: 3.5340, Train Perplexity: 34.2599\n",
            "Epoch [1/3], Batch [7095/21340], Train Loss: 3.2978, Train Perplexity: 27.0540\n",
            "Epoch [1/3], Batch [7096/21340], Train Loss: 3.1586, Train Perplexity: 23.5386\n",
            "Epoch [1/3], Batch [7097/21340], Train Loss: 3.4883, Train Perplexity: 32.7287\n",
            "Epoch [1/3], Batch [7098/21340], Train Loss: 3.1507, Train Perplexity: 23.3531\n",
            "Epoch [1/3], Batch [7099/21340], Train Loss: 3.2563, Train Perplexity: 25.9544\n",
            "Epoch [1/3], Batch [7100/21340], Train Loss: 3.1980, Train Perplexity: 24.4845\n",
            "Epoch [1/3], Batch [7101/21340], Train Loss: 3.3904, Train Perplexity: 29.6782\n",
            "Epoch [1/3], Batch [7102/21340], Train Loss: 3.1647, Train Perplexity: 23.6810\n",
            "Epoch [1/3], Batch [7103/21340], Train Loss: 3.2320, Train Perplexity: 25.3298\n",
            "Epoch [1/3], Batch [7104/21340], Train Loss: 3.4202, Train Perplexity: 30.5760\n",
            "Epoch [1/3], Batch [7105/21340], Train Loss: 3.2080, Train Perplexity: 24.7284\n",
            "Epoch [1/3], Batch [7106/21340], Train Loss: 3.3161, Train Perplexity: 27.5514\n",
            "Epoch [1/3], Batch [7107/21340], Train Loss: 3.2714, Train Perplexity: 26.3493\n",
            "Epoch [1/3], Batch [7108/21340], Train Loss: 3.2469, Train Perplexity: 25.7116\n",
            "Epoch [1/3], Batch [7109/21340], Train Loss: 3.3126, Train Perplexity: 27.4562\n",
            "Epoch [1/3], Batch [7110/21340], Train Loss: 3.2603, Train Perplexity: 26.0567\n",
            "Epoch [1/3], Batch [7111/21340], Train Loss: 3.2766, Train Perplexity: 26.4854\n",
            "Epoch [1/3], Batch [7112/21340], Train Loss: 3.2326, Train Perplexity: 25.3461\n",
            "Epoch [1/3], Batch [7113/21340], Train Loss: 3.3012, Train Perplexity: 27.1457\n",
            "Epoch [1/3], Batch [7114/21340], Train Loss: 3.2267, Train Perplexity: 25.1953\n",
            "Epoch [1/3], Batch [7115/21340], Train Loss: 3.2369, Train Perplexity: 25.4544\n",
            "Epoch [1/3], Batch [7116/21340], Train Loss: 3.3018, Train Perplexity: 27.1627\n",
            "Epoch [1/3], Batch [7117/21340], Train Loss: 3.2759, Train Perplexity: 26.4676\n",
            "Epoch [1/3], Batch [7118/21340], Train Loss: 3.1557, Train Perplexity: 23.4699\n",
            "Epoch [1/3], Batch [7119/21340], Train Loss: 3.3616, Train Perplexity: 28.8359\n",
            "Epoch [1/3], Batch [7120/21340], Train Loss: 3.3057, Train Perplexity: 27.2688\n",
            "Epoch [1/3], Batch [7121/21340], Train Loss: 3.3596, Train Perplexity: 28.7775\n",
            "Epoch [1/3], Batch [7122/21340], Train Loss: 3.1676, Train Perplexity: 23.7508\n",
            "Epoch [1/3], Batch [7123/21340], Train Loss: 3.2509, Train Perplexity: 25.8137\n",
            "Epoch [1/3], Batch [7124/21340], Train Loss: 3.4600, Train Perplexity: 31.8184\n",
            "Epoch [1/3], Batch [7125/21340], Train Loss: 3.2423, Train Perplexity: 25.5935\n",
            "Epoch [1/3], Batch [7126/21340], Train Loss: 3.2489, Train Perplexity: 25.7618\n",
            "Epoch [1/3], Batch [7127/21340], Train Loss: 3.2589, Train Perplexity: 26.0222\n",
            "Epoch [1/3], Batch [7128/21340], Train Loss: 3.2101, Train Perplexity: 24.7811\n",
            "Epoch [1/3], Batch [7129/21340], Train Loss: 3.3750, Train Perplexity: 29.2236\n",
            "Epoch [1/3], Batch [7130/21340], Train Loss: 3.2473, Train Perplexity: 25.7216\n",
            "Epoch [1/3], Batch [7131/21340], Train Loss: 3.2106, Train Perplexity: 24.7944\n",
            "Epoch [1/3], Batch [7132/21340], Train Loss: 3.3038, Train Perplexity: 27.2155\n",
            "Epoch [1/3], Batch [7133/21340], Train Loss: 3.2167, Train Perplexity: 24.9457\n",
            "Epoch [1/3], Batch [7134/21340], Train Loss: 3.3842, Train Perplexity: 29.4944\n",
            "Epoch [1/3], Batch [7135/21340], Train Loss: 3.2809, Train Perplexity: 26.5992\n",
            "Epoch [1/3], Batch [7136/21340], Train Loss: 3.1856, Train Perplexity: 24.1818\n",
            "Epoch [1/3], Batch [7137/21340], Train Loss: 3.2412, Train Perplexity: 25.5645\n",
            "Epoch [1/3], Batch [7138/21340], Train Loss: 3.4059, Train Perplexity: 30.1415\n",
            "Epoch [1/3], Batch [7139/21340], Train Loss: 3.1993, Train Perplexity: 24.5162\n",
            "Epoch [1/3], Batch [7140/21340], Train Loss: 3.2663, Train Perplexity: 26.2135\n",
            "Epoch [1/3], Batch [7141/21340], Train Loss: 3.2955, Train Perplexity: 26.9913\n",
            "Epoch [1/3], Batch [7142/21340], Train Loss: 3.4324, Train Perplexity: 30.9518\n",
            "Epoch [1/3], Batch [7143/21340], Train Loss: 3.2411, Train Perplexity: 25.5609\n",
            "Epoch [1/3], Batch [7144/21340], Train Loss: 3.2085, Train Perplexity: 24.7410\n",
            "Epoch [1/3], Batch [7145/21340], Train Loss: 3.2318, Train Perplexity: 25.3242\n",
            "Epoch [1/3], Batch [7146/21340], Train Loss: 3.3398, Train Perplexity: 28.2142\n",
            "Epoch [1/3], Batch [7147/21340], Train Loss: 3.1913, Train Perplexity: 24.3195\n",
            "Epoch [1/3], Batch [7148/21340], Train Loss: 3.2770, Train Perplexity: 26.4961\n",
            "Epoch [1/3], Batch [7149/21340], Train Loss: 3.2969, Train Perplexity: 27.0281\n",
            "Epoch [1/3], Batch [7150/21340], Train Loss: 3.2761, Train Perplexity: 26.4726\n",
            "Epoch [1/3], Batch [7151/21340], Train Loss: 3.2250, Train Perplexity: 25.1524\n",
            "Epoch [1/3], Batch [7152/21340], Train Loss: 3.0809, Train Perplexity: 21.7781\n",
            "Epoch [1/3], Batch [7153/21340], Train Loss: 3.3095, Train Perplexity: 27.3711\n",
            "Epoch [1/3], Batch [7154/21340], Train Loss: 3.2095, Train Perplexity: 24.7667\n",
            "Epoch [1/3], Batch [7155/21340], Train Loss: 3.2398, Train Perplexity: 25.5281\n",
            "Epoch [1/3], Batch [7156/21340], Train Loss: 3.2654, Train Perplexity: 26.1914\n",
            "Epoch [1/3], Batch [7157/21340], Train Loss: 3.3558, Train Perplexity: 28.6694\n",
            "Epoch [1/3], Batch [7158/21340], Train Loss: 3.2462, Train Perplexity: 25.6922\n",
            "Epoch [1/3], Batch [7159/21340], Train Loss: 3.2272, Train Perplexity: 25.2095\n",
            "Epoch [1/3], Batch [7160/21340], Train Loss: 3.2770, Train Perplexity: 26.4972\n",
            "Epoch [1/3], Batch [7161/21340], Train Loss: 3.2487, Train Perplexity: 25.7562\n",
            "Epoch [1/3], Batch [7162/21340], Train Loss: 3.3616, Train Perplexity: 28.8357\n",
            "Epoch [1/3], Batch [7163/21340], Train Loss: 3.2480, Train Perplexity: 25.7378\n",
            "Epoch [1/3], Batch [7164/21340], Train Loss: 3.2525, Train Perplexity: 25.8543\n",
            "Epoch [1/3], Batch [7165/21340], Train Loss: 3.2189, Train Perplexity: 25.0016\n",
            "Epoch [1/3], Batch [7166/21340], Train Loss: 3.2243, Train Perplexity: 25.1355\n",
            "Epoch [1/3], Batch [7167/21340], Train Loss: 3.1615, Train Perplexity: 23.6068\n",
            "Epoch [1/3], Batch [7168/21340], Train Loss: 3.2696, Train Perplexity: 26.3001\n",
            "Epoch [1/3], Batch [7169/21340], Train Loss: 3.2850, Train Perplexity: 26.7082\n",
            "Epoch [1/3], Batch [7170/21340], Train Loss: 3.2914, Train Perplexity: 26.8807\n",
            "Epoch [1/3], Batch [7171/21340], Train Loss: 3.2050, Train Perplexity: 24.6557\n",
            "Epoch [1/3], Batch [7172/21340], Train Loss: 3.3930, Train Perplexity: 29.7563\n",
            "Epoch [1/3], Batch [7173/21340], Train Loss: 3.2686, Train Perplexity: 26.2753\n",
            "Epoch [1/3], Batch [7174/21340], Train Loss: 3.2836, Train Perplexity: 26.6704\n",
            "Epoch [1/3], Batch [7175/21340], Train Loss: 3.2659, Train Perplexity: 26.2046\n",
            "Epoch [1/3], Batch [7176/21340], Train Loss: 3.2210, Train Perplexity: 25.0528\n",
            "Epoch [1/3], Batch [7177/21340], Train Loss: 3.2684, Train Perplexity: 26.2698\n",
            "Epoch [1/3], Batch [7178/21340], Train Loss: 3.2743, Train Perplexity: 26.4240\n",
            "Epoch [1/3], Batch [7179/21340], Train Loss: 3.2966, Train Perplexity: 27.0215\n",
            "Epoch [1/3], Batch [7180/21340], Train Loss: 3.2286, Train Perplexity: 25.2453\n",
            "Epoch [1/3], Batch [7181/21340], Train Loss: 3.2477, Train Perplexity: 25.7307\n",
            "Epoch [1/3], Batch [7182/21340], Train Loss: 3.1884, Train Perplexity: 24.2503\n",
            "Epoch [1/3], Batch [7183/21340], Train Loss: 3.2563, Train Perplexity: 25.9526\n",
            "Epoch [1/3], Batch [7184/21340], Train Loss: 3.4531, Train Perplexity: 31.5995\n",
            "Epoch [1/3], Batch [7185/21340], Train Loss: 3.2473, Train Perplexity: 25.7196\n",
            "Epoch [1/3], Batch [7186/21340], Train Loss: 3.4058, Train Perplexity: 30.1391\n",
            "Epoch [1/3], Batch [7187/21340], Train Loss: 3.2673, Train Perplexity: 26.2402\n",
            "Epoch [1/3], Batch [7188/21340], Train Loss: 3.4208, Train Perplexity: 30.5927\n",
            "Epoch [1/3], Batch [7189/21340], Train Loss: 3.2087, Train Perplexity: 24.7474\n",
            "Epoch [1/3], Batch [7190/21340], Train Loss: 3.2474, Train Perplexity: 25.7224\n",
            "Epoch [1/3], Batch [7191/21340], Train Loss: 3.1605, Train Perplexity: 23.5834\n",
            "Epoch [1/3], Batch [7192/21340], Train Loss: 3.2941, Train Perplexity: 26.9530\n",
            "Epoch [1/3], Batch [7193/21340], Train Loss: 3.2637, Train Perplexity: 26.1460\n",
            "Epoch [1/3], Batch [7194/21340], Train Loss: 3.3072, Train Perplexity: 27.3091\n",
            "Epoch [1/3], Batch [7195/21340], Train Loss: 3.3209, Train Perplexity: 27.6846\n",
            "Epoch [1/3], Batch [7196/21340], Train Loss: 3.3063, Train Perplexity: 27.2840\n",
            "Epoch [1/3], Batch [7197/21340], Train Loss: 3.2930, Train Perplexity: 26.9233\n",
            "Epoch [1/3], Batch [7198/21340], Train Loss: 3.3115, Train Perplexity: 27.4266\n",
            "Epoch [1/3], Batch [7199/21340], Train Loss: 3.2682, Train Perplexity: 26.2634\n",
            "Epoch [1/3], Batch [7200/21340], Train Loss: 3.2397, Train Perplexity: 25.5271\n",
            "Epoch [1/3], Batch [7201/21340], Train Loss: 3.3561, Train Perplexity: 28.6765\n",
            "Epoch [1/3], Batch [7202/21340], Train Loss: 3.4331, Train Perplexity: 30.9715\n",
            "Epoch [1/3], Batch [7203/21340], Train Loss: 3.2873, Train Perplexity: 26.7693\n",
            "Epoch [1/3], Batch [7204/21340], Train Loss: 3.3533, Train Perplexity: 28.5962\n",
            "Epoch [1/3], Batch [7205/21340], Train Loss: 3.1813, Train Perplexity: 24.0771\n",
            "Epoch [1/3], Batch [7206/21340], Train Loss: 3.2143, Train Perplexity: 24.8869\n",
            "Epoch [1/3], Batch [7207/21340], Train Loss: 3.4218, Train Perplexity: 30.6230\n",
            "Epoch [1/3], Batch [7208/21340], Train Loss: 3.3493, Train Perplexity: 28.4823\n",
            "Epoch [1/3], Batch [7209/21340], Train Loss: 3.2700, Train Perplexity: 26.3101\n",
            "Epoch [1/3], Batch [7210/21340], Train Loss: 3.2332, Train Perplexity: 25.3606\n",
            "Epoch [1/3], Batch [7211/21340], Train Loss: 3.1935, Train Perplexity: 24.3745\n",
            "Epoch [1/3], Batch [7212/21340], Train Loss: 3.2617, Train Perplexity: 26.0939\n",
            "Epoch [1/3], Batch [7213/21340], Train Loss: 3.3139, Train Perplexity: 27.4919\n",
            "Epoch [1/3], Batch [7214/21340], Train Loss: 3.3433, Train Perplexity: 28.3127\n",
            "Epoch [1/3], Batch [7215/21340], Train Loss: 3.2561, Train Perplexity: 25.9478\n",
            "Epoch [1/3], Batch [7216/21340], Train Loss: 3.2204, Train Perplexity: 25.0377\n",
            "Epoch [1/3], Batch [7217/21340], Train Loss: 3.2541, Train Perplexity: 25.8964\n",
            "Epoch [1/3], Batch [7218/21340], Train Loss: 3.2255, Train Perplexity: 25.1670\n",
            "Epoch [1/3], Batch [7219/21340], Train Loss: 3.3894, Train Perplexity: 29.6478\n",
            "Epoch [1/3], Batch [7220/21340], Train Loss: 3.3168, Train Perplexity: 27.5717\n",
            "Epoch [1/3], Batch [7221/21340], Train Loss: 3.2229, Train Perplexity: 25.0999\n",
            "Epoch [1/3], Batch [7222/21340], Train Loss: 3.3294, Train Perplexity: 27.9211\n",
            "Epoch [1/3], Batch [7223/21340], Train Loss: 3.1730, Train Perplexity: 23.8785\n",
            "Epoch [1/3], Batch [7224/21340], Train Loss: 3.1608, Train Perplexity: 23.5900\n",
            "Epoch [1/3], Batch [7225/21340], Train Loss: 3.2596, Train Perplexity: 26.0388\n",
            "Epoch [1/3], Batch [7226/21340], Train Loss: 3.2259, Train Perplexity: 25.1752\n",
            "Epoch [1/3], Batch [7227/21340], Train Loss: 3.2638, Train Perplexity: 26.1479\n",
            "Epoch [1/3], Batch [7228/21340], Train Loss: 3.2772, Train Perplexity: 26.5020\n",
            "Epoch [1/3], Batch [7229/21340], Train Loss: 3.2345, Train Perplexity: 25.3937\n",
            "Epoch [1/3], Batch [7230/21340], Train Loss: 3.2226, Train Perplexity: 25.0937\n",
            "Epoch [1/3], Batch [7231/21340], Train Loss: 3.2135, Train Perplexity: 24.8655\n",
            "Epoch [1/3], Batch [7232/21340], Train Loss: 3.2537, Train Perplexity: 25.8862\n",
            "Epoch [1/3], Batch [7233/21340], Train Loss: 3.2517, Train Perplexity: 25.8335\n",
            "Epoch [1/3], Batch [7234/21340], Train Loss: 3.3083, Train Perplexity: 27.3381\n",
            "Epoch [1/3], Batch [7235/21340], Train Loss: 3.2674, Train Perplexity: 26.2435\n",
            "Epoch [1/3], Batch [7236/21340], Train Loss: 3.2066, Train Perplexity: 24.6956\n",
            "Epoch [1/3], Batch [7237/21340], Train Loss: 3.3491, Train Perplexity: 28.4758\n",
            "Epoch [1/3], Batch [7238/21340], Train Loss: 3.2679, Train Perplexity: 26.2560\n",
            "Epoch [1/3], Batch [7239/21340], Train Loss: 3.2492, Train Perplexity: 25.7695\n",
            "Epoch [1/3], Batch [7240/21340], Train Loss: 3.3684, Train Perplexity: 29.0312\n",
            "Epoch [1/3], Batch [7241/21340], Train Loss: 3.1957, Train Perplexity: 24.4272\n",
            "Epoch [1/3], Batch [7242/21340], Train Loss: 3.2717, Train Perplexity: 26.3554\n",
            "Epoch [1/3], Batch [7243/21340], Train Loss: 3.1698, Train Perplexity: 23.8030\n",
            "Epoch [1/3], Batch [7244/21340], Train Loss: 3.1623, Train Perplexity: 23.6242\n",
            "Epoch [1/3], Batch [7245/21340], Train Loss: 3.2057, Train Perplexity: 24.6735\n",
            "Epoch [1/3], Batch [7246/21340], Train Loss: 3.4028, Train Perplexity: 30.0481\n",
            "Epoch [1/3], Batch [7247/21340], Train Loss: 3.2643, Train Perplexity: 26.1617\n",
            "Epoch [1/3], Batch [7248/21340], Train Loss: 3.3731, Train Perplexity: 29.1687\n",
            "Epoch [1/3], Batch [7249/21340], Train Loss: 3.3257, Train Perplexity: 27.8195\n",
            "Epoch [1/3], Batch [7250/21340], Train Loss: 3.2636, Train Perplexity: 26.1429\n",
            "Epoch [1/3], Batch [7251/21340], Train Loss: 3.0936, Train Perplexity: 22.0562\n",
            "Epoch [1/3], Batch [7252/21340], Train Loss: 3.2973, Train Perplexity: 27.0386\n",
            "Epoch [1/3], Batch [7253/21340], Train Loss: 3.2292, Train Perplexity: 25.2600\n",
            "Epoch [1/3], Batch [7254/21340], Train Loss: 3.2949, Train Perplexity: 26.9755\n",
            "Epoch [1/3], Batch [7255/21340], Train Loss: 3.2486, Train Perplexity: 25.7544\n",
            "Epoch [1/3], Batch [7256/21340], Train Loss: 3.2914, Train Perplexity: 26.8797\n",
            "Epoch [1/3], Batch [7257/21340], Train Loss: 3.1618, Train Perplexity: 23.6138\n",
            "Epoch [1/3], Batch [7258/21340], Train Loss: 3.1777, Train Perplexity: 23.9908\n",
            "Epoch [1/3], Batch [7259/21340], Train Loss: 3.3543, Train Perplexity: 28.6270\n",
            "Epoch [1/3], Batch [7260/21340], Train Loss: 3.3087, Train Perplexity: 27.3484\n",
            "Epoch [1/3], Batch [7261/21340], Train Loss: 3.1950, Train Perplexity: 24.4112\n",
            "Epoch [1/3], Batch [7262/21340], Train Loss: 3.4945, Train Perplexity: 32.9329\n",
            "Epoch [1/3], Batch [7263/21340], Train Loss: 3.2577, Train Perplexity: 25.9895\n",
            "Epoch [1/3], Batch [7264/21340], Train Loss: 3.2884, Train Perplexity: 26.7993\n",
            "Epoch [1/3], Batch [7265/21340], Train Loss: 3.2613, Train Perplexity: 26.0835\n",
            "Epoch [1/3], Batch [7266/21340], Train Loss: 3.1694, Train Perplexity: 23.7939\n",
            "Epoch [1/3], Batch [7267/21340], Train Loss: 3.1739, Train Perplexity: 23.9005\n",
            "Epoch [1/3], Batch [7268/21340], Train Loss: 3.1956, Train Perplexity: 24.4259\n",
            "Epoch [1/3], Batch [7269/21340], Train Loss: 3.1757, Train Perplexity: 23.9436\n",
            "Epoch [1/3], Batch [7270/21340], Train Loss: 3.2854, Train Perplexity: 26.7192\n",
            "Epoch [1/3], Batch [7271/21340], Train Loss: 3.1948, Train Perplexity: 24.4041\n",
            "Epoch [1/3], Batch [7272/21340], Train Loss: 3.2381, Train Perplexity: 25.4852\n",
            "Epoch [1/3], Batch [7273/21340], Train Loss: 3.3278, Train Perplexity: 27.8758\n",
            "Epoch [1/3], Batch [7274/21340], Train Loss: 3.3816, Train Perplexity: 29.4187\n",
            "Epoch [1/3], Batch [7275/21340], Train Loss: 3.3006, Train Perplexity: 27.1281\n",
            "Epoch [1/3], Batch [7276/21340], Train Loss: 3.3090, Train Perplexity: 27.3575\n",
            "Epoch [1/3], Batch [7277/21340], Train Loss: 3.3221, Train Perplexity: 27.7181\n",
            "Epoch [1/3], Batch [7278/21340], Train Loss: 3.2215, Train Perplexity: 25.0664\n",
            "Epoch [1/3], Batch [7279/21340], Train Loss: 3.1808, Train Perplexity: 24.0655\n",
            "Epoch [1/3], Batch [7280/21340], Train Loss: 3.1317, Train Perplexity: 22.9120\n",
            "Epoch [1/3], Batch [7281/21340], Train Loss: 3.2135, Train Perplexity: 24.8657\n",
            "Epoch [1/3], Batch [7282/21340], Train Loss: 3.2067, Train Perplexity: 24.6967\n",
            "Epoch [1/3], Batch [7283/21340], Train Loss: 3.3208, Train Perplexity: 27.6826\n",
            "Epoch [1/3], Batch [7284/21340], Train Loss: 3.2774, Train Perplexity: 26.5074\n",
            "Epoch [1/3], Batch [7285/21340], Train Loss: 3.1615, Train Perplexity: 23.6052\n",
            "Epoch [1/3], Batch [7286/21340], Train Loss: 3.2413, Train Perplexity: 25.5679\n",
            "Epoch [1/3], Batch [7287/21340], Train Loss: 3.2732, Train Perplexity: 26.3956\n",
            "Epoch [1/3], Batch [7288/21340], Train Loss: 3.2895, Train Perplexity: 26.8282\n",
            "Epoch [1/3], Batch [7289/21340], Train Loss: 3.2682, Train Perplexity: 26.2636\n",
            "Epoch [1/3], Batch [7290/21340], Train Loss: 3.3244, Train Perplexity: 27.7822\n",
            "Epoch [1/3], Batch [7291/21340], Train Loss: 3.2428, Train Perplexity: 25.6047\n",
            "Epoch [1/3], Batch [7292/21340], Train Loss: 3.2478, Train Perplexity: 25.7328\n",
            "Epoch [1/3], Batch [7293/21340], Train Loss: 3.2107, Train Perplexity: 24.7964\n",
            "Epoch [1/3], Batch [7294/21340], Train Loss: 3.3181, Train Perplexity: 27.6088\n",
            "Epoch [1/3], Batch [7295/21340], Train Loss: 3.2388, Train Perplexity: 25.5022\n",
            "Epoch [1/3], Batch [7296/21340], Train Loss: 3.1599, Train Perplexity: 23.5676\n",
            "Epoch [1/3], Batch [7297/21340], Train Loss: 3.2722, Train Perplexity: 26.3683\n",
            "Epoch [1/3], Batch [7298/21340], Train Loss: 3.2109, Train Perplexity: 24.8011\n",
            "Epoch [1/3], Batch [7299/21340], Train Loss: 3.2815, Train Perplexity: 26.6160\n",
            "Epoch [1/3], Batch [7300/21340], Train Loss: 3.2195, Train Perplexity: 25.0164\n",
            "Epoch [1/3], Batch [7301/21340], Train Loss: 3.2876, Train Perplexity: 26.7796\n",
            "Epoch [1/3], Batch [7302/21340], Train Loss: 3.1700, Train Perplexity: 23.8081\n",
            "Epoch [1/3], Batch [7303/21340], Train Loss: 3.2642, Train Perplexity: 26.1588\n",
            "Epoch [1/3], Batch [7304/21340], Train Loss: 3.1718, Train Perplexity: 23.8512\n",
            "Epoch [1/3], Batch [7305/21340], Train Loss: 3.1645, Train Perplexity: 23.6768\n",
            "Epoch [1/3], Batch [7306/21340], Train Loss: 3.3466, Train Perplexity: 28.4055\n",
            "Epoch [1/3], Batch [7307/21340], Train Loss: 3.2854, Train Perplexity: 26.7200\n",
            "Epoch [1/3], Batch [7308/21340], Train Loss: 3.2842, Train Perplexity: 26.6881\n",
            "Epoch [1/3], Batch [7309/21340], Train Loss: 3.2259, Train Perplexity: 25.1773\n",
            "Epoch [1/3], Batch [7310/21340], Train Loss: 3.2380, Train Perplexity: 25.4831\n",
            "Epoch [1/3], Batch [7311/21340], Train Loss: 3.2619, Train Perplexity: 26.0992\n",
            "Epoch [1/3], Batch [7312/21340], Train Loss: 3.2330, Train Perplexity: 25.3567\n",
            "Epoch [1/3], Batch [7313/21340], Train Loss: 3.2853, Train Perplexity: 26.7172\n",
            "Epoch [1/3], Batch [7314/21340], Train Loss: 3.1798, Train Perplexity: 24.0418\n",
            "Epoch [1/3], Batch [7315/21340], Train Loss: 3.3346, Train Perplexity: 28.0659\n",
            "Epoch [1/3], Batch [7316/21340], Train Loss: 3.2721, Train Perplexity: 26.3673\n",
            "Epoch [1/3], Batch [7317/21340], Train Loss: 3.2829, Train Perplexity: 26.6520\n",
            "Epoch [1/3], Batch [7318/21340], Train Loss: 3.2243, Train Perplexity: 25.1356\n",
            "Epoch [1/3], Batch [7319/21340], Train Loss: 3.1689, Train Perplexity: 23.7817\n",
            "Epoch [1/3], Batch [7320/21340], Train Loss: 3.2216, Train Perplexity: 25.0675\n",
            "Epoch [1/3], Batch [7321/21340], Train Loss: 3.3535, Train Perplexity: 28.6032\n",
            "Epoch [1/3], Batch [7322/21340], Train Loss: 3.2608, Train Perplexity: 26.0704\n",
            "Epoch [1/3], Batch [7323/21340], Train Loss: 3.2168, Train Perplexity: 24.9482\n",
            "Epoch [1/3], Batch [7324/21340], Train Loss: 3.3952, Train Perplexity: 29.8194\n",
            "Epoch [1/3], Batch [7325/21340], Train Loss: 3.3190, Train Perplexity: 27.6317\n",
            "Epoch [1/3], Batch [7326/21340], Train Loss: 3.2777, Train Perplexity: 26.5151\n",
            "Epoch [1/3], Batch [7327/21340], Train Loss: 3.2183, Train Perplexity: 24.9864\n",
            "Epoch [1/3], Batch [7328/21340], Train Loss: 3.2500, Train Perplexity: 25.7898\n",
            "Epoch [1/3], Batch [7329/21340], Train Loss: 3.2219, Train Perplexity: 25.0767\n",
            "Epoch [1/3], Batch [7330/21340], Train Loss: 3.1675, Train Perplexity: 23.7469\n",
            "Epoch [1/3], Batch [7331/21340], Train Loss: 3.2811, Train Perplexity: 26.6063\n",
            "Epoch [1/3], Batch [7332/21340], Train Loss: 3.2743, Train Perplexity: 26.4259\n",
            "Epoch [1/3], Batch [7333/21340], Train Loss: 3.2330, Train Perplexity: 25.3566\n",
            "Epoch [1/3], Batch [7334/21340], Train Loss: 3.2096, Train Perplexity: 24.7691\n",
            "Epoch [1/3], Batch [7335/21340], Train Loss: 3.2805, Train Perplexity: 26.5878\n",
            "Epoch [1/3], Batch [7336/21340], Train Loss: 3.1791, Train Perplexity: 24.0258\n",
            "Epoch [1/3], Batch [7337/21340], Train Loss: 3.2577, Train Perplexity: 25.9887\n",
            "Epoch [1/3], Batch [7338/21340], Train Loss: 3.3441, Train Perplexity: 28.3337\n",
            "Epoch [1/3], Batch [7339/21340], Train Loss: 3.1421, Train Perplexity: 23.1521\n",
            "Epoch [1/3], Batch [7340/21340], Train Loss: 3.2192, Train Perplexity: 25.0088\n",
            "Epoch [1/3], Batch [7341/21340], Train Loss: 3.2109, Train Perplexity: 24.8010\n",
            "Epoch [1/3], Batch [7342/21340], Train Loss: 3.2934, Train Perplexity: 26.9331\n",
            "Epoch [1/3], Batch [7343/21340], Train Loss: 3.2487, Train Perplexity: 25.7557\n",
            "Epoch [1/3], Batch [7344/21340], Train Loss: 3.2288, Train Perplexity: 25.2498\n",
            "Epoch [1/3], Batch [7345/21340], Train Loss: 3.2552, Train Perplexity: 25.9256\n",
            "Epoch [1/3], Batch [7346/21340], Train Loss: 3.2270, Train Perplexity: 25.2036\n",
            "Epoch [1/3], Batch [7347/21340], Train Loss: 3.3170, Train Perplexity: 27.5764\n",
            "Epoch [1/3], Batch [7348/21340], Train Loss: 3.2712, Train Perplexity: 26.3417\n",
            "Epoch [1/3], Batch [7349/21340], Train Loss: 3.2456, Train Perplexity: 25.6764\n",
            "Epoch [1/3], Batch [7350/21340], Train Loss: 3.2797, Train Perplexity: 26.5677\n",
            "Epoch [1/3], Batch [7351/21340], Train Loss: 3.2090, Train Perplexity: 24.7534\n",
            "Epoch [1/3], Batch [7352/21340], Train Loss: 3.2480, Train Perplexity: 25.7376\n",
            "Epoch [1/3], Batch [7353/21340], Train Loss: 3.3200, Train Perplexity: 27.6594\n",
            "Epoch [1/3], Batch [7354/21340], Train Loss: 3.7482, Train Perplexity: 42.4440\n",
            "Epoch [1/3], Batch [7355/21340], Train Loss: 3.2608, Train Perplexity: 26.0697\n",
            "Epoch [1/3], Batch [7356/21340], Train Loss: 3.1932, Train Perplexity: 24.3656\n",
            "Epoch [1/3], Batch [7357/21340], Train Loss: 3.2511, Train Perplexity: 25.8188\n",
            "Epoch [1/3], Batch [7358/21340], Train Loss: 3.5267, Train Perplexity: 34.0108\n",
            "Epoch [1/3], Batch [7359/21340], Train Loss: 3.2541, Train Perplexity: 25.8951\n",
            "Epoch [1/3], Batch [7360/21340], Train Loss: 3.3337, Train Perplexity: 28.0432\n",
            "Epoch [1/3], Batch [7361/21340], Train Loss: 3.1520, Train Perplexity: 23.3816\n",
            "Epoch [1/3], Batch [7362/21340], Train Loss: 3.2732, Train Perplexity: 26.3969\n",
            "Epoch [1/3], Batch [7363/21340], Train Loss: 3.3488, Train Perplexity: 28.4673\n",
            "Epoch [1/3], Batch [7364/21340], Train Loss: 3.3326, Train Perplexity: 28.0113\n",
            "Epoch [1/3], Batch [7365/21340], Train Loss: 3.2796, Train Perplexity: 26.5660\n",
            "Epoch [1/3], Batch [7366/21340], Train Loss: 3.2468, Train Perplexity: 25.7092\n",
            "Epoch [1/3], Batch [7367/21340], Train Loss: 3.2363, Train Perplexity: 25.4384\n",
            "Epoch [1/3], Batch [7368/21340], Train Loss: 3.1908, Train Perplexity: 24.3077\n",
            "Epoch [1/3], Batch [7369/21340], Train Loss: 3.2318, Train Perplexity: 25.3255\n",
            "Epoch [1/3], Batch [7370/21340], Train Loss: 3.2692, Train Perplexity: 26.2899\n",
            "Epoch [1/3], Batch [7371/21340], Train Loss: 3.1676, Train Perplexity: 23.7511\n",
            "Epoch [1/3], Batch [7372/21340], Train Loss: 3.1770, Train Perplexity: 23.9740\n",
            "Epoch [1/3], Batch [7373/21340], Train Loss: 3.3013, Train Perplexity: 27.1476\n",
            "Epoch [1/3], Batch [7374/21340], Train Loss: 3.2201, Train Perplexity: 25.0319\n",
            "Epoch [1/3], Batch [7375/21340], Train Loss: 3.2268, Train Perplexity: 25.1978\n",
            "Epoch [1/3], Batch [7376/21340], Train Loss: 3.2925, Train Perplexity: 26.9109\n",
            "Epoch [1/3], Batch [7377/21340], Train Loss: 3.1964, Train Perplexity: 24.4456\n",
            "Epoch [1/3], Batch [7378/21340], Train Loss: 3.1412, Train Perplexity: 23.1324\n",
            "Epoch [1/3], Batch [7379/21340], Train Loss: 3.1514, Train Perplexity: 23.3687\n",
            "Epoch [1/3], Batch [7380/21340], Train Loss: 3.3456, Train Perplexity: 28.3783\n",
            "Epoch [1/3], Batch [7381/21340], Train Loss: 3.2882, Train Perplexity: 26.7943\n",
            "Epoch [1/3], Batch [7382/21340], Train Loss: 3.2448, Train Perplexity: 25.6564\n",
            "Epoch [1/3], Batch [7383/21340], Train Loss: 3.1393, Train Perplexity: 23.0886\n",
            "Epoch [1/3], Batch [7384/21340], Train Loss: 3.1616, Train Perplexity: 23.6082\n",
            "Epoch [1/3], Batch [7385/21340], Train Loss: 3.2274, Train Perplexity: 25.2131\n",
            "Epoch [1/3], Batch [7386/21340], Train Loss: 3.2640, Train Perplexity: 26.1548\n",
            "Epoch [1/3], Batch [7387/21340], Train Loss: 3.4312, Train Perplexity: 30.9134\n",
            "Epoch [1/3], Batch [7388/21340], Train Loss: 3.1773, Train Perplexity: 23.9810\n",
            "Epoch [1/3], Batch [7389/21340], Train Loss: 3.2646, Train Perplexity: 26.1705\n",
            "Epoch [1/3], Batch [7390/21340], Train Loss: 3.3425, Train Perplexity: 28.2909\n",
            "Epoch [1/3], Batch [7391/21340], Train Loss: 3.3443, Train Perplexity: 28.3402\n",
            "Epoch [1/3], Batch [7392/21340], Train Loss: 3.3432, Train Perplexity: 28.3106\n",
            "Epoch [1/3], Batch [7393/21340], Train Loss: 3.3435, Train Perplexity: 28.3169\n",
            "Epoch [1/3], Batch [7394/21340], Train Loss: 3.2672, Train Perplexity: 26.2365\n",
            "Epoch [1/3], Batch [7395/21340], Train Loss: 3.2109, Train Perplexity: 24.8012\n",
            "Epoch [1/3], Batch [7396/21340], Train Loss: 3.2146, Train Perplexity: 24.8932\n",
            "Epoch [1/3], Batch [7397/21340], Train Loss: 3.3206, Train Perplexity: 27.6764\n",
            "Epoch [1/3], Batch [7398/21340], Train Loss: 3.2516, Train Perplexity: 25.8314\n",
            "Epoch [1/3], Batch [7399/21340], Train Loss: 3.2375, Train Perplexity: 25.4688\n",
            "Epoch [1/3], Batch [7400/21340], Train Loss: 3.1353, Train Perplexity: 22.9965\n",
            "Epoch [1/3], Batch [7401/21340], Train Loss: 3.2125, Train Perplexity: 24.8406\n",
            "Epoch [1/3], Batch [7402/21340], Train Loss: 3.3308, Train Perplexity: 27.9602\n",
            "Epoch [1/3], Batch [7403/21340], Train Loss: 3.3763, Train Perplexity: 29.2630\n",
            "Epoch [1/3], Batch [7404/21340], Train Loss: 3.2205, Train Perplexity: 25.0394\n",
            "Epoch [1/3], Batch [7405/21340], Train Loss: 3.1537, Train Perplexity: 23.4215\n",
            "Epoch [1/3], Batch [7406/21340], Train Loss: 3.1494, Train Perplexity: 23.3213\n",
            "Epoch [1/3], Batch [7407/21340], Train Loss: 3.3098, Train Perplexity: 27.3809\n",
            "Epoch [1/3], Batch [7408/21340], Train Loss: 3.2164, Train Perplexity: 24.9383\n",
            "Epoch [1/3], Batch [7409/21340], Train Loss: 3.3516, Train Perplexity: 28.5493\n",
            "Epoch [1/3], Batch [7410/21340], Train Loss: 3.2668, Train Perplexity: 26.2277\n",
            "Epoch [1/3], Batch [7411/21340], Train Loss: 3.2243, Train Perplexity: 25.1355\n",
            "Epoch [1/3], Batch [7412/21340], Train Loss: 3.3430, Train Perplexity: 28.3040\n",
            "Epoch [1/3], Batch [7413/21340], Train Loss: 3.4555, Train Perplexity: 31.6749\n",
            "Epoch [1/3], Batch [7414/21340], Train Loss: 3.2151, Train Perplexity: 24.9056\n",
            "Epoch [1/3], Batch [7415/21340], Train Loss: 3.2816, Train Perplexity: 26.6172\n",
            "Epoch [1/3], Batch [7416/21340], Train Loss: 3.3351, Train Perplexity: 28.0823\n",
            "Epoch [1/3], Batch [7417/21340], Train Loss: 3.2088, Train Perplexity: 24.7495\n",
            "Epoch [1/3], Batch [7418/21340], Train Loss: 3.2420, Train Perplexity: 25.5840\n",
            "Epoch [1/3], Batch [7419/21340], Train Loss: 3.3374, Train Perplexity: 28.1466\n",
            "Epoch [1/3], Batch [7420/21340], Train Loss: 3.2110, Train Perplexity: 24.8040\n",
            "Epoch [1/3], Batch [7421/21340], Train Loss: 3.2771, Train Perplexity: 26.4977\n",
            "Epoch [1/3], Batch [7422/21340], Train Loss: 3.1818, Train Perplexity: 24.0898\n",
            "Epoch [1/3], Batch [7423/21340], Train Loss: 3.2041, Train Perplexity: 24.6332\n",
            "Epoch [1/3], Batch [7424/21340], Train Loss: 3.3320, Train Perplexity: 27.9948\n",
            "Epoch [1/3], Batch [7425/21340], Train Loss: 3.1872, Train Perplexity: 24.2208\n",
            "Epoch [1/3], Batch [7426/21340], Train Loss: 3.2361, Train Perplexity: 25.4337\n",
            "Epoch [1/3], Batch [7427/21340], Train Loss: 3.2306, Train Perplexity: 25.2960\n",
            "Epoch [1/3], Batch [7428/21340], Train Loss: 3.1259, Train Perplexity: 22.7805\n",
            "Epoch [1/3], Batch [7429/21340], Train Loss: 3.1872, Train Perplexity: 24.2216\n",
            "Epoch [1/3], Batch [7430/21340], Train Loss: 3.3899, Train Perplexity: 29.6639\n",
            "Epoch [1/3], Batch [7431/21340], Train Loss: 3.2887, Train Perplexity: 26.8076\n",
            "Epoch [1/3], Batch [7432/21340], Train Loss: 3.3416, Train Perplexity: 28.2654\n",
            "Epoch [1/3], Batch [7433/21340], Train Loss: 3.2574, Train Perplexity: 25.9829\n",
            "Epoch [1/3], Batch [7434/21340], Train Loss: 3.2642, Train Perplexity: 26.1592\n",
            "Epoch [1/3], Batch [7435/21340], Train Loss: 3.1740, Train Perplexity: 23.9037\n",
            "Epoch [1/3], Batch [7436/21340], Train Loss: 3.2895, Train Perplexity: 26.8292\n",
            "Epoch [1/3], Batch [7437/21340], Train Loss: 3.2352, Train Perplexity: 25.4119\n",
            "Epoch [1/3], Batch [7438/21340], Train Loss: 3.1958, Train Perplexity: 24.4298\n",
            "Epoch [1/3], Batch [7439/21340], Train Loss: 3.3538, Train Perplexity: 28.6113\n",
            "Epoch [1/3], Batch [7440/21340], Train Loss: 3.2715, Train Perplexity: 26.3516\n",
            "Epoch [1/3], Batch [7441/21340], Train Loss: 3.2544, Train Perplexity: 25.9034\n",
            "Epoch [1/3], Batch [7442/21340], Train Loss: 3.4394, Train Perplexity: 31.1698\n",
            "Epoch [1/3], Batch [7443/21340], Train Loss: 3.2995, Train Perplexity: 27.0999\n",
            "Epoch [1/3], Batch [7444/21340], Train Loss: 3.2080, Train Perplexity: 24.7291\n",
            "Epoch [1/3], Batch [7445/21340], Train Loss: 3.1932, Train Perplexity: 24.3655\n",
            "Epoch [1/3], Batch [7446/21340], Train Loss: 3.2137, Train Perplexity: 24.8715\n",
            "Epoch [1/3], Batch [7447/21340], Train Loss: 3.2140, Train Perplexity: 24.8789\n",
            "Epoch [1/3], Batch [7448/21340], Train Loss: 3.2527, Train Perplexity: 25.8599\n",
            "Epoch [1/3], Batch [7449/21340], Train Loss: 3.2091, Train Perplexity: 24.7562\n",
            "Epoch [1/3], Batch [7450/21340], Train Loss: 3.1748, Train Perplexity: 23.9209\n",
            "Epoch [1/3], Batch [7451/21340], Train Loss: 3.2764, Train Perplexity: 26.4810\n",
            "Epoch [1/3], Batch [7452/21340], Train Loss: 3.2403, Train Perplexity: 25.5416\n",
            "Epoch [1/3], Batch [7453/21340], Train Loss: 3.2750, Train Perplexity: 26.4430\n",
            "Epoch [1/3], Batch [7454/21340], Train Loss: 3.3273, Train Perplexity: 27.8617\n",
            "Epoch [1/3], Batch [7455/21340], Train Loss: 3.2890, Train Perplexity: 26.8173\n",
            "Epoch [1/3], Batch [7456/21340], Train Loss: 3.2990, Train Perplexity: 27.0862\n",
            "Epoch [1/3], Batch [7457/21340], Train Loss: 3.2064, Train Perplexity: 24.6906\n",
            "Epoch [1/3], Batch [7458/21340], Train Loss: 3.2229, Train Perplexity: 25.1001\n",
            "Epoch [1/3], Batch [7459/21340], Train Loss: 3.2966, Train Perplexity: 27.0211\n",
            "Epoch [1/3], Batch [7460/21340], Train Loss: 3.2608, Train Perplexity: 26.0711\n",
            "Epoch [1/3], Batch [7461/21340], Train Loss: 3.2926, Train Perplexity: 26.9137\n",
            "Epoch [1/3], Batch [7462/21340], Train Loss: 3.2675, Train Perplexity: 26.2451\n",
            "Epoch [1/3], Batch [7463/21340], Train Loss: 3.2895, Train Perplexity: 26.8305\n",
            "Epoch [1/3], Batch [7464/21340], Train Loss: 3.2566, Train Perplexity: 25.9603\n",
            "Epoch [1/3], Batch [7465/21340], Train Loss: 3.2689, Train Perplexity: 26.2819\n",
            "Epoch [1/3], Batch [7466/21340], Train Loss: 3.2071, Train Perplexity: 24.7065\n",
            "Epoch [1/3], Batch [7467/21340], Train Loss: 3.3097, Train Perplexity: 27.3768\n",
            "Epoch [1/3], Batch [7468/21340], Train Loss: 3.2919, Train Perplexity: 26.8928\n",
            "Epoch [1/3], Batch [7469/21340], Train Loss: 3.2300, Train Perplexity: 25.2809\n",
            "Epoch [1/3], Batch [7470/21340], Train Loss: 3.2487, Train Perplexity: 25.7572\n",
            "Epoch [1/3], Batch [7471/21340], Train Loss: 3.2700, Train Perplexity: 26.3108\n",
            "Epoch [1/3], Batch [7472/21340], Train Loss: 3.2550, Train Perplexity: 25.9186\n",
            "Epoch [1/3], Batch [7473/21340], Train Loss: 3.2922, Train Perplexity: 26.9016\n",
            "Epoch [1/3], Batch [7474/21340], Train Loss: 3.2907, Train Perplexity: 26.8628\n",
            "Epoch [1/3], Batch [7475/21340], Train Loss: 3.2630, Train Perplexity: 26.1283\n",
            "Epoch [1/3], Batch [7476/21340], Train Loss: 3.2936, Train Perplexity: 26.9384\n",
            "Epoch [1/3], Batch [7477/21340], Train Loss: 3.3004, Train Perplexity: 27.1222\n",
            "Epoch [1/3], Batch [7478/21340], Train Loss: 3.2108, Train Perplexity: 24.7980\n",
            "Epoch [1/3], Batch [7479/21340], Train Loss: 3.2158, Train Perplexity: 24.9237\n",
            "Epoch [1/3], Batch [7480/21340], Train Loss: 3.2186, Train Perplexity: 24.9932\n",
            "Epoch [1/3], Batch [7481/21340], Train Loss: 3.3015, Train Perplexity: 27.1530\n",
            "Epoch [1/3], Batch [7482/21340], Train Loss: 3.2451, Train Perplexity: 25.6634\n",
            "Epoch [1/3], Batch [7483/21340], Train Loss: 3.2682, Train Perplexity: 26.2649\n",
            "Epoch [1/3], Batch [7484/21340], Train Loss: 3.3666, Train Perplexity: 28.9812\n",
            "Epoch [1/3], Batch [7485/21340], Train Loss: 3.2650, Train Perplexity: 26.1796\n",
            "Epoch [1/3], Batch [7486/21340], Train Loss: 3.2961, Train Perplexity: 27.0058\n",
            "Epoch [1/3], Batch [7487/21340], Train Loss: 3.1375, Train Perplexity: 23.0462\n",
            "Epoch [1/3], Batch [7488/21340], Train Loss: 3.1863, Train Perplexity: 24.1988\n",
            "Epoch [1/3], Batch [7489/21340], Train Loss: 3.4469, Train Perplexity: 31.4043\n",
            "Epoch [1/3], Batch [7490/21340], Train Loss: 3.1763, Train Perplexity: 23.9568\n",
            "Epoch [1/3], Batch [7491/21340], Train Loss: 3.2682, Train Perplexity: 26.2642\n",
            "Epoch [1/3], Batch [7492/21340], Train Loss: 3.3468, Train Perplexity: 28.4123\n",
            "Epoch [1/3], Batch [7493/21340], Train Loss: 3.4006, Train Perplexity: 29.9831\n",
            "Epoch [1/3], Batch [7494/21340], Train Loss: 3.4199, Train Perplexity: 30.5659\n",
            "Epoch [1/3], Batch [7495/21340], Train Loss: 3.2854, Train Perplexity: 26.7183\n",
            "Epoch [1/3], Batch [7496/21340], Train Loss: 3.2258, Train Perplexity: 25.1726\n",
            "Epoch [1/3], Batch [7497/21340], Train Loss: 3.2064, Train Perplexity: 24.6896\n",
            "Epoch [1/3], Batch [7498/21340], Train Loss: 3.1765, Train Perplexity: 23.9624\n",
            "Epoch [1/3], Batch [7499/21340], Train Loss: 3.1981, Train Perplexity: 24.4849\n",
            "Epoch [1/3], Batch [7500/21340], Train Loss: 3.5296, Train Perplexity: 34.1114\n",
            "Epoch [1/3], Batch [7501/21340], Train Loss: 3.3542, Train Perplexity: 28.6230\n",
            "Epoch [1/3], Batch [7502/21340], Train Loss: 3.1836, Train Perplexity: 24.1332\n",
            "Epoch [1/3], Batch [7503/21340], Train Loss: 3.3201, Train Perplexity: 27.6645\n",
            "Epoch [1/3], Batch [7504/21340], Train Loss: 3.3047, Train Perplexity: 27.2393\n",
            "Epoch [1/3], Batch [7505/21340], Train Loss: 3.3483, Train Perplexity: 28.4533\n",
            "Epoch [1/3], Batch [7506/21340], Train Loss: 3.1399, Train Perplexity: 23.1014\n",
            "Epoch [1/3], Batch [7507/21340], Train Loss: 3.3069, Train Perplexity: 27.2997\n",
            "Epoch [1/3], Batch [7508/21340], Train Loss: 3.1980, Train Perplexity: 24.4842\n",
            "Epoch [1/3], Batch [7509/21340], Train Loss: 3.1639, Train Perplexity: 23.6621\n",
            "Epoch [1/3], Batch [7510/21340], Train Loss: 3.3017, Train Perplexity: 27.1601\n",
            "Epoch [1/3], Batch [7511/21340], Train Loss: 3.1519, Train Perplexity: 23.3808\n",
            "Epoch [1/3], Batch [7512/21340], Train Loss: 3.2472, Train Perplexity: 25.7169\n",
            "Epoch [1/3], Batch [7513/21340], Train Loss: 3.3047, Train Perplexity: 27.2415\n",
            "Epoch [1/3], Batch [7514/21340], Train Loss: 3.3766, Train Perplexity: 29.2718\n",
            "Epoch [1/3], Batch [7515/21340], Train Loss: 3.1587, Train Perplexity: 23.5401\n",
            "Epoch [1/3], Batch [7516/21340], Train Loss: 3.2972, Train Perplexity: 27.0355\n",
            "Epoch [1/3], Batch [7517/21340], Train Loss: 3.3927, Train Perplexity: 29.7452\n",
            "Epoch [1/3], Batch [7518/21340], Train Loss: 3.2493, Train Perplexity: 25.7713\n",
            "Epoch [1/3], Batch [7519/21340], Train Loss: 3.2633, Train Perplexity: 26.1362\n",
            "Epoch [1/3], Batch [7520/21340], Train Loss: 3.3977, Train Perplexity: 29.8939\n",
            "Epoch [1/3], Batch [7521/21340], Train Loss: 3.1873, Train Perplexity: 24.2236\n",
            "Epoch [1/3], Batch [7522/21340], Train Loss: 3.3092, Train Perplexity: 27.3624\n",
            "Epoch [1/3], Batch [7523/21340], Train Loss: 3.1740, Train Perplexity: 23.9033\n",
            "Epoch [1/3], Batch [7524/21340], Train Loss: 3.2751, Train Perplexity: 26.4446\n",
            "Epoch [1/3], Batch [7525/21340], Train Loss: 3.1932, Train Perplexity: 24.3661\n",
            "Epoch [1/3], Batch [7526/21340], Train Loss: 3.2535, Train Perplexity: 25.8803\n",
            "Epoch [1/3], Batch [7527/21340], Train Loss: 3.2048, Train Perplexity: 24.6502\n",
            "Epoch [1/3], Batch [7528/21340], Train Loss: 3.2315, Train Perplexity: 25.3187\n",
            "Epoch [1/3], Batch [7529/21340], Train Loss: 3.1939, Train Perplexity: 24.3825\n",
            "Epoch [1/3], Batch [7530/21340], Train Loss: 3.3021, Train Perplexity: 27.1704\n",
            "Epoch [1/3], Batch [7531/21340], Train Loss: 3.2479, Train Perplexity: 25.7350\n",
            "Epoch [1/3], Batch [7532/21340], Train Loss: 3.3489, Train Perplexity: 28.4712\n",
            "Epoch [1/3], Batch [7533/21340], Train Loss: 3.2777, Train Perplexity: 26.5152\n",
            "Epoch [1/3], Batch [7534/21340], Train Loss: 3.2237, Train Perplexity: 25.1198\n",
            "Epoch [1/3], Batch [7535/21340], Train Loss: 3.2341, Train Perplexity: 25.3829\n",
            "Epoch [1/3], Batch [7536/21340], Train Loss: 3.2388, Train Perplexity: 25.5034\n",
            "Epoch [1/3], Batch [7537/21340], Train Loss: 3.2376, Train Perplexity: 25.4719\n",
            "Epoch [1/3], Batch [7538/21340], Train Loss: 3.2680, Train Perplexity: 26.2596\n",
            "Epoch [1/3], Batch [7539/21340], Train Loss: 3.1991, Train Perplexity: 24.5105\n",
            "Epoch [1/3], Batch [7540/21340], Train Loss: 3.3903, Train Perplexity: 29.6749\n",
            "Epoch [1/3], Batch [7541/21340], Train Loss: 3.2398, Train Perplexity: 25.5288\n",
            "Epoch [1/3], Batch [7542/21340], Train Loss: 3.2405, Train Perplexity: 25.5468\n",
            "Epoch [1/3], Batch [7543/21340], Train Loss: 3.2534, Train Perplexity: 25.8776\n",
            "Epoch [1/3], Batch [7544/21340], Train Loss: 3.2776, Train Perplexity: 26.5122\n",
            "Epoch [1/3], Batch [7545/21340], Train Loss: 3.2269, Train Perplexity: 25.2022\n",
            "Epoch [1/3], Batch [7546/21340], Train Loss: 3.3462, Train Perplexity: 28.3934\n",
            "Epoch [1/3], Batch [7547/21340], Train Loss: 3.4481, Train Perplexity: 31.4418\n",
            "Epoch [1/3], Batch [7548/21340], Train Loss: 3.2495, Train Perplexity: 25.7762\n",
            "Epoch [1/3], Batch [7549/21340], Train Loss: 3.2443, Train Perplexity: 25.6431\n",
            "Epoch [1/3], Batch [7550/21340], Train Loss: 3.2058, Train Perplexity: 24.6764\n",
            "Epoch [1/3], Batch [7551/21340], Train Loss: 3.3249, Train Perplexity: 27.7971\n",
            "Epoch [1/3], Batch [7552/21340], Train Loss: 3.2234, Train Perplexity: 25.1122\n",
            "Epoch [1/3], Batch [7553/21340], Train Loss: 3.1784, Train Perplexity: 24.0073\n",
            "Epoch [1/3], Batch [7554/21340], Train Loss: 3.3451, Train Perplexity: 28.3647\n",
            "Epoch [1/3], Batch [7555/21340], Train Loss: 3.4634, Train Perplexity: 31.9253\n",
            "Epoch [1/3], Batch [7556/21340], Train Loss: 3.2154, Train Perplexity: 24.9141\n",
            "Epoch [1/3], Batch [7557/21340], Train Loss: 3.2798, Train Perplexity: 26.5703\n",
            "Epoch [1/3], Batch [7558/21340], Train Loss: 3.1683, Train Perplexity: 23.7674\n",
            "Epoch [1/3], Batch [7559/21340], Train Loss: 3.2647, Train Perplexity: 26.1726\n",
            "Epoch [1/3], Batch [7560/21340], Train Loss: 3.2619, Train Perplexity: 26.1004\n",
            "Epoch [1/3], Batch [7561/21340], Train Loss: 3.2517, Train Perplexity: 25.8344\n",
            "Epoch [1/3], Batch [7562/21340], Train Loss: 3.1645, Train Perplexity: 23.6763\n",
            "Epoch [1/3], Batch [7563/21340], Train Loss: 3.2545, Train Perplexity: 25.9071\n",
            "Epoch [1/3], Batch [7564/21340], Train Loss: 3.2773, Train Perplexity: 26.5049\n",
            "Epoch [1/3], Batch [7565/21340], Train Loss: 3.2591, Train Perplexity: 26.0257\n",
            "Epoch [1/3], Batch [7566/21340], Train Loss: 3.2352, Train Perplexity: 25.4118\n",
            "Epoch [1/3], Batch [7567/21340], Train Loss: 3.1798, Train Perplexity: 24.0422\n",
            "Epoch [1/3], Batch [7568/21340], Train Loss: 3.2177, Train Perplexity: 24.9704\n",
            "Epoch [1/3], Batch [7569/21340], Train Loss: 3.3120, Train Perplexity: 27.4391\n",
            "Epoch [1/3], Batch [7570/21340], Train Loss: 3.4001, Train Perplexity: 29.9685\n",
            "Epoch [1/3], Batch [7571/21340], Train Loss: 3.2370, Train Perplexity: 25.4579\n",
            "Epoch [1/3], Batch [7572/21340], Train Loss: 3.2117, Train Perplexity: 24.8223\n",
            "Epoch [1/3], Batch [7573/21340], Train Loss: 3.2152, Train Perplexity: 24.9074\n",
            "Epoch [1/3], Batch [7574/21340], Train Loss: 3.2002, Train Perplexity: 24.5382\n",
            "Epoch [1/3], Batch [7575/21340], Train Loss: 3.3251, Train Perplexity: 27.8022\n",
            "Epoch [1/3], Batch [7576/21340], Train Loss: 3.2595, Train Perplexity: 26.0362\n",
            "Epoch [1/3], Batch [7577/21340], Train Loss: 3.3014, Train Perplexity: 27.1516\n",
            "Epoch [1/3], Batch [7578/21340], Train Loss: 3.1719, Train Perplexity: 23.8535\n",
            "Epoch [1/3], Batch [7579/21340], Train Loss: 3.2780, Train Perplexity: 26.5214\n",
            "Epoch [1/3], Batch [7580/21340], Train Loss: 3.2198, Train Perplexity: 25.0244\n",
            "Epoch [1/3], Batch [7581/21340], Train Loss: 3.1983, Train Perplexity: 24.4904\n",
            "Epoch [1/3], Batch [7582/21340], Train Loss: 3.2324, Train Perplexity: 25.3407\n",
            "Epoch [1/3], Batch [7583/21340], Train Loss: 3.2947, Train Perplexity: 26.9688\n",
            "Epoch [1/3], Batch [7584/21340], Train Loss: 3.2061, Train Perplexity: 24.6830\n",
            "Epoch [1/3], Batch [7585/21340], Train Loss: 3.2346, Train Perplexity: 25.3951\n",
            "Epoch [1/3], Batch [7586/21340], Train Loss: 3.2483, Train Perplexity: 25.7454\n",
            "Epoch [1/3], Batch [7587/21340], Train Loss: 3.2246, Train Perplexity: 25.1424\n",
            "Epoch [1/3], Batch [7588/21340], Train Loss: 3.4062, Train Perplexity: 30.1513\n",
            "Epoch [1/3], Batch [7589/21340], Train Loss: 3.3378, Train Perplexity: 28.1570\n",
            "Epoch [1/3], Batch [7590/21340], Train Loss: 3.2840, Train Perplexity: 26.6825\n",
            "Epoch [1/3], Batch [7591/21340], Train Loss: 3.2207, Train Perplexity: 25.0451\n",
            "Epoch [1/3], Batch [7592/21340], Train Loss: 3.2672, Train Perplexity: 26.2383\n",
            "Epoch [1/3], Batch [7593/21340], Train Loss: 3.3062, Train Perplexity: 27.2824\n",
            "Epoch [1/3], Batch [7594/21340], Train Loss: 3.2083, Train Perplexity: 24.7376\n",
            "Epoch [1/3], Batch [7595/21340], Train Loss: 3.4331, Train Perplexity: 30.9725\n",
            "Epoch [1/3], Batch [7596/21340], Train Loss: 3.2804, Train Perplexity: 26.5854\n",
            "Epoch [1/3], Batch [7597/21340], Train Loss: 3.3013, Train Perplexity: 27.1467\n",
            "Epoch [1/3], Batch [7598/21340], Train Loss: 3.2080, Train Perplexity: 24.7304\n",
            "Epoch [1/3], Batch [7599/21340], Train Loss: 3.2621, Train Perplexity: 26.1039\n",
            "Epoch [1/3], Batch [7600/21340], Train Loss: 3.2957, Train Perplexity: 26.9962\n",
            "Epoch [1/3], Batch [7601/21340], Train Loss: 3.2973, Train Perplexity: 27.0384\n",
            "Epoch [1/3], Batch [7602/21340], Train Loss: 3.3550, Train Perplexity: 28.6461\n",
            "Epoch [1/3], Batch [7603/21340], Train Loss: 3.3256, Train Perplexity: 27.8160\n",
            "Epoch [1/3], Batch [7604/21340], Train Loss: 3.2245, Train Perplexity: 25.1409\n",
            "Epoch [1/3], Batch [7605/21340], Train Loss: 3.2126, Train Perplexity: 24.8426\n",
            "Epoch [1/3], Batch [7606/21340], Train Loss: 3.3156, Train Perplexity: 27.5396\n",
            "Epoch [1/3], Batch [7607/21340], Train Loss: 3.2395, Train Perplexity: 25.5198\n",
            "Epoch [1/3], Batch [7608/21340], Train Loss: 3.3184, Train Perplexity: 27.6174\n",
            "Epoch [1/3], Batch [7609/21340], Train Loss: 3.2418, Train Perplexity: 25.5809\n",
            "Epoch [1/3], Batch [7610/21340], Train Loss: 3.3476, Train Perplexity: 28.4349\n",
            "Epoch [1/3], Batch [7611/21340], Train Loss: 3.2311, Train Perplexity: 25.3067\n",
            "Epoch [1/3], Batch [7612/21340], Train Loss: 3.2531, Train Perplexity: 25.8714\n",
            "Epoch [1/3], Batch [7613/21340], Train Loss: 3.2965, Train Perplexity: 27.0169\n",
            "Epoch [1/3], Batch [7614/21340], Train Loss: 3.2823, Train Perplexity: 26.6368\n",
            "Epoch [1/3], Batch [7615/21340], Train Loss: 3.3187, Train Perplexity: 27.6253\n",
            "Epoch [1/3], Batch [7616/21340], Train Loss: 3.2771, Train Perplexity: 26.4977\n",
            "Epoch [1/3], Batch [7617/21340], Train Loss: 3.2258, Train Perplexity: 25.1740\n",
            "Epoch [1/3], Batch [7618/21340], Train Loss: 3.3057, Train Perplexity: 27.2678\n",
            "Epoch [1/3], Batch [7619/21340], Train Loss: 3.3804, Train Perplexity: 29.3812\n",
            "Epoch [1/3], Batch [7620/21340], Train Loss: 3.2873, Train Perplexity: 26.7693\n",
            "Epoch [1/3], Batch [7621/21340], Train Loss: 3.3101, Train Perplexity: 27.3888\n",
            "Epoch [1/3], Batch [7622/21340], Train Loss: 3.2242, Train Perplexity: 25.1337\n",
            "Epoch [1/3], Batch [7623/21340], Train Loss: 3.2002, Train Perplexity: 24.5364\n",
            "Epoch [1/3], Batch [7624/21340], Train Loss: 3.1721, Train Perplexity: 23.8566\n",
            "Epoch [1/3], Batch [7625/21340], Train Loss: 3.1905, Train Perplexity: 24.3003\n",
            "Epoch [1/3], Batch [7626/21340], Train Loss: 3.2417, Train Perplexity: 25.5769\n",
            "Epoch [1/3], Batch [7627/21340], Train Loss: 3.2873, Train Perplexity: 26.7708\n",
            "Epoch [1/3], Batch [7628/21340], Train Loss: 3.3303, Train Perplexity: 27.9475\n",
            "Epoch [1/3], Batch [7629/21340], Train Loss: 3.2377, Train Perplexity: 25.4757\n",
            "Epoch [1/3], Batch [7630/21340], Train Loss: 3.1624, Train Perplexity: 23.6278\n",
            "Epoch [1/3], Batch [7631/21340], Train Loss: 3.3549, Train Perplexity: 28.6421\n",
            "Epoch [1/3], Batch [7632/21340], Train Loss: 3.1423, Train Perplexity: 23.1568\n",
            "Epoch [1/3], Batch [7633/21340], Train Loss: 3.3190, Train Perplexity: 27.6334\n",
            "Epoch [1/3], Batch [7634/21340], Train Loss: 3.2858, Train Perplexity: 26.7299\n",
            "Epoch [1/3], Batch [7635/21340], Train Loss: 3.2414, Train Perplexity: 25.5694\n",
            "Epoch [1/3], Batch [7636/21340], Train Loss: 3.2460, Train Perplexity: 25.6868\n",
            "Epoch [1/3], Batch [7637/21340], Train Loss: 3.2614, Train Perplexity: 26.0865\n",
            "Epoch [1/3], Batch [7638/21340], Train Loss: 3.2871, Train Perplexity: 26.7651\n",
            "Epoch [1/3], Batch [7639/21340], Train Loss: 3.2832, Train Perplexity: 26.6618\n",
            "Epoch [1/3], Batch [7640/21340], Train Loss: 3.4498, Train Perplexity: 31.4945\n",
            "Epoch [1/3], Batch [7641/21340], Train Loss: 3.1554, Train Perplexity: 23.4624\n",
            "Epoch [1/3], Batch [7642/21340], Train Loss: 3.2359, Train Perplexity: 25.4303\n",
            "Epoch [1/3], Batch [7643/21340], Train Loss: 3.2877, Train Perplexity: 26.7807\n",
            "Epoch [1/3], Batch [7644/21340], Train Loss: 3.1990, Train Perplexity: 24.5078\n",
            "Epoch [1/3], Batch [7645/21340], Train Loss: 3.2080, Train Perplexity: 24.7291\n",
            "Epoch [1/3], Batch [7646/21340], Train Loss: 3.2544, Train Perplexity: 25.9037\n",
            "Epoch [1/3], Batch [7647/21340], Train Loss: 3.1447, Train Perplexity: 23.2124\n",
            "Epoch [1/3], Batch [7648/21340], Train Loss: 3.2683, Train Perplexity: 26.2677\n",
            "Epoch [1/3], Batch [7649/21340], Train Loss: 3.2785, Train Perplexity: 26.5359\n",
            "Epoch [1/3], Batch [7650/21340], Train Loss: 3.3146, Train Perplexity: 27.5120\n",
            "Epoch [1/3], Batch [7651/21340], Train Loss: 3.2429, Train Perplexity: 25.6069\n",
            "Epoch [1/3], Batch [7652/21340], Train Loss: 3.3700, Train Perplexity: 29.0793\n",
            "Epoch [1/3], Batch [7653/21340], Train Loss: 3.2528, Train Perplexity: 25.8629\n",
            "Epoch [1/3], Batch [7654/21340], Train Loss: 3.3275, Train Perplexity: 27.8677\n",
            "Epoch [1/3], Batch [7655/21340], Train Loss: 3.2246, Train Perplexity: 25.1430\n",
            "Epoch [1/3], Batch [7656/21340], Train Loss: 3.2926, Train Perplexity: 26.9133\n",
            "Epoch [1/3], Batch [7657/21340], Train Loss: 3.2613, Train Perplexity: 26.0828\n",
            "Epoch [1/3], Batch [7658/21340], Train Loss: 3.2866, Train Perplexity: 26.7517\n",
            "Epoch [1/3], Batch [7659/21340], Train Loss: 3.1495, Train Perplexity: 23.3236\n",
            "Epoch [1/3], Batch [7660/21340], Train Loss: 3.3268, Train Perplexity: 27.8503\n",
            "Epoch [1/3], Batch [7661/21340], Train Loss: 3.2892, Train Perplexity: 26.8222\n",
            "Epoch [1/3], Batch [7662/21340], Train Loss: 3.3017, Train Perplexity: 27.1586\n",
            "Epoch [1/3], Batch [7663/21340], Train Loss: 3.2426, Train Perplexity: 25.5999\n",
            "Epoch [1/3], Batch [7664/21340], Train Loss: 3.1978, Train Perplexity: 24.4788\n",
            "Epoch [1/3], Batch [7665/21340], Train Loss: 3.3084, Train Perplexity: 27.3407\n",
            "Epoch [1/3], Batch [7666/21340], Train Loss: 3.2318, Train Perplexity: 25.3251\n",
            "Epoch [1/3], Batch [7667/21340], Train Loss: 3.2785, Train Perplexity: 26.5363\n",
            "Epoch [1/3], Batch [7668/21340], Train Loss: 3.2771, Train Perplexity: 26.4993\n",
            "Epoch [1/3], Batch [7669/21340], Train Loss: 3.2988, Train Perplexity: 27.0788\n",
            "Epoch [1/3], Batch [7670/21340], Train Loss: 3.3619, Train Perplexity: 28.8427\n",
            "Epoch [1/3], Batch [7671/21340], Train Loss: 3.2500, Train Perplexity: 25.7900\n",
            "Epoch [1/3], Batch [7672/21340], Train Loss: 3.3590, Train Perplexity: 28.7598\n",
            "Epoch [1/3], Batch [7673/21340], Train Loss: 3.1362, Train Perplexity: 23.0171\n",
            "Epoch [1/3], Batch [7674/21340], Train Loss: 3.3019, Train Perplexity: 27.1648\n",
            "Epoch [1/3], Batch [7675/21340], Train Loss: 3.4261, Train Perplexity: 30.7579\n",
            "Epoch [1/3], Batch [7676/21340], Train Loss: 3.2086, Train Perplexity: 24.7455\n",
            "Epoch [1/3], Batch [7677/21340], Train Loss: 3.2544, Train Perplexity: 25.9038\n",
            "Epoch [1/3], Batch [7678/21340], Train Loss: 3.3801, Train Perplexity: 29.3725\n",
            "Epoch [1/3], Batch [7679/21340], Train Loss: 3.3248, Train Perplexity: 27.7933\n",
            "Epoch [1/3], Batch [7680/21340], Train Loss: 3.2137, Train Perplexity: 24.8711\n",
            "Epoch [1/3], Batch [7681/21340], Train Loss: 3.5291, Train Perplexity: 34.0925\n",
            "Epoch [1/3], Batch [7682/21340], Train Loss: 3.2678, Train Perplexity: 26.2540\n",
            "Epoch [1/3], Batch [7683/21340], Train Loss: 3.3017, Train Perplexity: 27.1595\n",
            "Epoch [1/3], Batch [7684/21340], Train Loss: 3.2707, Train Perplexity: 26.3300\n",
            "Epoch [1/3], Batch [7685/21340], Train Loss: 3.2379, Train Perplexity: 25.4808\n",
            "Epoch [1/3], Batch [7686/21340], Train Loss: 3.2219, Train Perplexity: 25.0748\n",
            "Epoch [1/3], Batch [7687/21340], Train Loss: 3.2318, Train Perplexity: 25.3256\n",
            "Epoch [1/3], Batch [7688/21340], Train Loss: 3.2121, Train Perplexity: 24.8308\n",
            "Epoch [1/3], Batch [7689/21340], Train Loss: 3.3289, Train Perplexity: 27.9077\n",
            "Epoch [1/3], Batch [7690/21340], Train Loss: 3.5354, Train Perplexity: 34.3098\n",
            "Epoch [1/3], Batch [7691/21340], Train Loss: 3.3624, Train Perplexity: 28.8579\n",
            "Epoch [1/3], Batch [7692/21340], Train Loss: 3.2788, Train Perplexity: 26.5439\n",
            "Epoch [1/3], Batch [7693/21340], Train Loss: 3.2046, Train Perplexity: 24.6461\n",
            "Epoch [1/3], Batch [7694/21340], Train Loss: 3.1771, Train Perplexity: 23.9770\n",
            "Epoch [1/3], Batch [7695/21340], Train Loss: 3.2136, Train Perplexity: 24.8692\n",
            "Epoch [1/3], Batch [7696/21340], Train Loss: 3.2409, Train Perplexity: 25.5566\n",
            "Epoch [1/3], Batch [7697/21340], Train Loss: 3.2783, Train Perplexity: 26.5312\n",
            "Epoch [1/3], Batch [7698/21340], Train Loss: 3.2429, Train Perplexity: 25.6082\n",
            "Epoch [1/3], Batch [7699/21340], Train Loss: 3.1090, Train Perplexity: 22.3982\n",
            "Epoch [1/3], Batch [7700/21340], Train Loss: 3.3837, Train Perplexity: 29.4788\n",
            "Epoch [1/3], Batch [7701/21340], Train Loss: 3.1652, Train Perplexity: 23.6946\n",
            "Epoch [1/3], Batch [7702/21340], Train Loss: 3.3778, Train Perplexity: 29.3076\n",
            "Epoch [1/3], Batch [7703/21340], Train Loss: 3.2242, Train Perplexity: 25.1340\n",
            "Epoch [1/3], Batch [7704/21340], Train Loss: 3.1897, Train Perplexity: 24.2813\n",
            "Epoch [1/3], Batch [7705/21340], Train Loss: 3.2302, Train Perplexity: 25.2858\n",
            "Epoch [1/3], Batch [7706/21340], Train Loss: 3.1561, Train Perplexity: 23.4783\n",
            "Epoch [1/3], Batch [7707/21340], Train Loss: 3.4194, Train Perplexity: 30.5519\n",
            "Epoch [1/3], Batch [7708/21340], Train Loss: 3.2404, Train Perplexity: 25.5427\n",
            "Epoch [1/3], Batch [7709/21340], Train Loss: 3.3028, Train Perplexity: 27.1888\n",
            "Epoch [1/3], Batch [7710/21340], Train Loss: 3.2817, Train Perplexity: 26.6219\n",
            "Epoch [1/3], Batch [7711/21340], Train Loss: 3.2158, Train Perplexity: 24.9243\n",
            "Epoch [1/3], Batch [7712/21340], Train Loss: 3.2481, Train Perplexity: 25.7405\n",
            "Epoch [1/3], Batch [7713/21340], Train Loss: 3.1882, Train Perplexity: 24.2435\n",
            "Epoch [1/3], Batch [7714/21340], Train Loss: 3.2312, Train Perplexity: 25.3108\n",
            "Epoch [1/3], Batch [7715/21340], Train Loss: 3.3259, Train Perplexity: 27.8251\n",
            "Epoch [1/3], Batch [7716/21340], Train Loss: 3.3315, Train Perplexity: 27.9799\n",
            "Epoch [1/3], Batch [7717/21340], Train Loss: 3.1961, Train Perplexity: 24.4376\n",
            "Epoch [1/3], Batch [7718/21340], Train Loss: 3.2821, Train Perplexity: 26.6322\n",
            "Epoch [1/3], Batch [7719/21340], Train Loss: 3.2560, Train Perplexity: 25.9457\n",
            "Epoch [1/3], Batch [7720/21340], Train Loss: 3.3976, Train Perplexity: 29.8919\n",
            "Epoch [1/3], Batch [7721/21340], Train Loss: 3.2398, Train Perplexity: 25.5278\n",
            "Epoch [1/3], Batch [7722/21340], Train Loss: 3.2348, Train Perplexity: 25.4007\n",
            "Epoch [1/3], Batch [7723/21340], Train Loss: 3.2716, Train Perplexity: 26.3536\n",
            "Epoch [1/3], Batch [7724/21340], Train Loss: 3.4037, Train Perplexity: 30.0750\n",
            "Epoch [1/3], Batch [7725/21340], Train Loss: 3.2608, Train Perplexity: 26.0696\n",
            "Epoch [1/3], Batch [7726/21340], Train Loss: 3.2387, Train Perplexity: 25.4995\n",
            "Epoch [1/3], Batch [7727/21340], Train Loss: 3.1950, Train Perplexity: 24.4105\n",
            "Epoch [1/3], Batch [7728/21340], Train Loss: 3.2979, Train Perplexity: 27.0566\n",
            "Epoch [1/3], Batch [7729/21340], Train Loss: 3.4030, Train Perplexity: 30.0536\n",
            "Epoch [1/3], Batch [7730/21340], Train Loss: 3.2231, Train Perplexity: 25.1065\n",
            "Epoch [1/3], Batch [7731/21340], Train Loss: 3.2124, Train Perplexity: 24.8379\n",
            "Epoch [1/3], Batch [7732/21340], Train Loss: 3.2458, Train Perplexity: 25.6824\n",
            "Epoch [1/3], Batch [7733/21340], Train Loss: 3.2166, Train Perplexity: 24.9427\n",
            "Epoch [1/3], Batch [7734/21340], Train Loss: 3.1929, Train Perplexity: 24.3590\n",
            "Epoch [1/3], Batch [7735/21340], Train Loss: 3.2350, Train Perplexity: 25.4064\n",
            "Epoch [1/3], Batch [7736/21340], Train Loss: 3.2365, Train Perplexity: 25.4440\n",
            "Epoch [1/3], Batch [7737/21340], Train Loss: 3.1555, Train Perplexity: 23.4637\n",
            "Epoch [1/3], Batch [7738/21340], Train Loss: 3.2003, Train Perplexity: 24.5390\n",
            "Epoch [1/3], Batch [7739/21340], Train Loss: 3.2985, Train Perplexity: 27.0728\n",
            "Epoch [1/3], Batch [7740/21340], Train Loss: 3.2262, Train Perplexity: 25.1833\n",
            "Epoch [1/3], Batch [7741/21340], Train Loss: 3.2464, Train Perplexity: 25.6983\n",
            "Epoch [1/3], Batch [7742/21340], Train Loss: 3.2321, Train Perplexity: 25.3338\n",
            "Epoch [1/3], Batch [7743/21340], Train Loss: 3.3434, Train Perplexity: 28.3156\n",
            "Epoch [1/3], Batch [7744/21340], Train Loss: 3.1561, Train Perplexity: 23.4793\n",
            "Epoch [1/3], Batch [7745/21340], Train Loss: 3.2900, Train Perplexity: 26.8428\n",
            "Epoch [1/3], Batch [7746/21340], Train Loss: 3.3064, Train Perplexity: 27.2862\n",
            "Epoch [1/3], Batch [7747/21340], Train Loss: 3.3733, Train Perplexity: 29.1733\n",
            "Epoch [1/3], Batch [7748/21340], Train Loss: 3.2599, Train Perplexity: 26.0464\n",
            "Epoch [1/3], Batch [7749/21340], Train Loss: 3.2400, Train Perplexity: 25.5325\n",
            "Epoch [1/3], Batch [7750/21340], Train Loss: 3.2515, Train Perplexity: 25.8280\n",
            "Epoch [1/3], Batch [7751/21340], Train Loss: 3.3330, Train Perplexity: 28.0212\n",
            "Epoch [1/3], Batch [7752/21340], Train Loss: 3.2227, Train Perplexity: 25.0948\n",
            "Epoch [1/3], Batch [7753/21340], Train Loss: 3.3496, Train Perplexity: 28.4912\n",
            "Epoch [1/3], Batch [7754/21340], Train Loss: 3.4703, Train Perplexity: 32.1461\n",
            "Epoch [1/3], Batch [7755/21340], Train Loss: 3.2269, Train Perplexity: 25.2018\n",
            "Epoch [1/3], Batch [7756/21340], Train Loss: 3.2672, Train Perplexity: 26.2378\n",
            "Epoch [1/3], Batch [7757/21340], Train Loss: 3.3720, Train Perplexity: 29.1375\n",
            "Epoch [1/3], Batch [7758/21340], Train Loss: 3.4153, Train Perplexity: 30.4255\n",
            "Epoch [1/3], Batch [7759/21340], Train Loss: 3.4774, Train Perplexity: 32.3741\n",
            "Epoch [1/3], Batch [7760/21340], Train Loss: 3.1345, Train Perplexity: 22.9776\n",
            "Epoch [1/3], Batch [7761/21340], Train Loss: 3.2293, Train Perplexity: 25.2630\n",
            "Epoch [1/3], Batch [7762/21340], Train Loss: 3.2234, Train Perplexity: 25.1139\n",
            "Epoch [1/3], Batch [7763/21340], Train Loss: 3.2048, Train Perplexity: 24.6504\n",
            "Epoch [1/3], Batch [7764/21340], Train Loss: 3.2464, Train Perplexity: 25.6964\n",
            "Epoch [1/3], Batch [7765/21340], Train Loss: 3.1957, Train Perplexity: 24.4272\n",
            "Epoch [1/3], Batch [7766/21340], Train Loss: 3.2393, Train Perplexity: 25.5170\n",
            "Epoch [1/3], Batch [7767/21340], Train Loss: 3.3046, Train Perplexity: 27.2369\n",
            "Epoch [1/3], Batch [7768/21340], Train Loss: 3.3270, Train Perplexity: 27.8548\n",
            "Epoch [1/3], Batch [7769/21340], Train Loss: 3.3171, Train Perplexity: 27.5796\n",
            "Epoch [1/3], Batch [7770/21340], Train Loss: 3.2593, Train Perplexity: 26.0320\n",
            "Epoch [1/3], Batch [7771/21340], Train Loss: 3.2709, Train Perplexity: 26.3345\n",
            "Epoch [1/3], Batch [7772/21340], Train Loss: 3.2180, Train Perplexity: 24.9781\n",
            "Epoch [1/3], Batch [7773/21340], Train Loss: 3.2334, Train Perplexity: 25.3660\n",
            "Epoch [1/3], Batch [7774/21340], Train Loss: 3.1733, Train Perplexity: 23.8861\n",
            "Epoch [1/3], Batch [7775/21340], Train Loss: 3.2590, Train Perplexity: 26.0241\n",
            "Epoch [1/3], Batch [7776/21340], Train Loss: 3.2149, Train Perplexity: 24.9015\n",
            "Epoch [1/3], Batch [7777/21340], Train Loss: 3.4091, Train Perplexity: 30.2386\n",
            "Epoch [1/3], Batch [7778/21340], Train Loss: 3.2314, Train Perplexity: 25.3138\n",
            "Epoch [1/3], Batch [7779/21340], Train Loss: 3.4279, Train Perplexity: 30.8130\n",
            "Epoch [1/3], Batch [7780/21340], Train Loss: 3.3418, Train Perplexity: 28.2694\n",
            "Epoch [1/3], Batch [7781/21340], Train Loss: 3.2939, Train Perplexity: 26.9467\n",
            "Epoch [1/3], Batch [7782/21340], Train Loss: 3.3041, Train Perplexity: 27.2236\n",
            "Epoch [1/3], Batch [7783/21340], Train Loss: 3.2578, Train Perplexity: 25.9918\n",
            "Epoch [1/3], Batch [7784/21340], Train Loss: 3.3052, Train Perplexity: 27.2543\n",
            "Epoch [1/3], Batch [7785/21340], Train Loss: 3.1891, Train Perplexity: 24.2665\n",
            "Epoch [1/3], Batch [7786/21340], Train Loss: 3.3260, Train Perplexity: 27.8271\n",
            "Epoch [1/3], Batch [7787/21340], Train Loss: 3.3896, Train Perplexity: 29.6527\n",
            "Epoch [1/3], Batch [7788/21340], Train Loss: 3.5140, Train Perplexity: 33.5811\n",
            "Epoch [1/3], Batch [7789/21340], Train Loss: 3.2999, Train Perplexity: 27.1092\n",
            "Epoch [1/3], Batch [7790/21340], Train Loss: 3.2166, Train Perplexity: 24.9437\n",
            "Epoch [1/3], Batch [7791/21340], Train Loss: 3.2772, Train Perplexity: 26.5020\n",
            "Epoch [1/3], Batch [7792/21340], Train Loss: 3.1555, Train Perplexity: 23.4656\n",
            "Epoch [1/3], Batch [7793/21340], Train Loss: 3.2160, Train Perplexity: 24.9289\n",
            "Epoch [1/3], Batch [7794/21340], Train Loss: 3.2861, Train Perplexity: 26.7372\n",
            "Epoch [1/3], Batch [7795/21340], Train Loss: 3.2363, Train Perplexity: 25.4389\n",
            "Epoch [1/3], Batch [7796/21340], Train Loss: 3.2671, Train Perplexity: 26.2361\n",
            "Epoch [1/3], Batch [7797/21340], Train Loss: 3.2379, Train Perplexity: 25.4803\n",
            "Epoch [1/3], Batch [7798/21340], Train Loss: 3.3079, Train Perplexity: 27.3281\n",
            "Epoch [1/3], Batch [7799/21340], Train Loss: 3.2164, Train Perplexity: 24.9392\n",
            "Epoch [1/3], Batch [7800/21340], Train Loss: 3.3050, Train Perplexity: 27.2480\n",
            "Epoch [1/3], Batch [7801/21340], Train Loss: 3.1608, Train Perplexity: 23.5895\n",
            "Epoch [1/3], Batch [7802/21340], Train Loss: 3.3346, Train Perplexity: 28.0681\n",
            "Epoch [1/3], Batch [7803/21340], Train Loss: 3.2789, Train Perplexity: 26.5471\n",
            "Epoch [1/3], Batch [7804/21340], Train Loss: 3.1677, Train Perplexity: 23.7534\n",
            "Epoch [1/3], Batch [7805/21340], Train Loss: 3.2831, Train Perplexity: 26.6576\n",
            "Epoch [1/3], Batch [7806/21340], Train Loss: 3.2551, Train Perplexity: 25.9219\n",
            "Epoch [1/3], Batch [7807/21340], Train Loss: 3.2132, Train Perplexity: 24.8586\n",
            "Epoch [1/3], Batch [7808/21340], Train Loss: 3.1851, Train Perplexity: 24.1690\n",
            "Epoch [1/3], Batch [7809/21340], Train Loss: 3.1531, Train Perplexity: 23.4077\n",
            "Epoch [1/3], Batch [7810/21340], Train Loss: 3.1779, Train Perplexity: 23.9967\n",
            "Epoch [1/3], Batch [7811/21340], Train Loss: 3.1588, Train Perplexity: 23.5427\n",
            "Epoch [1/3], Batch [7812/21340], Train Loss: 3.2167, Train Perplexity: 24.9462\n",
            "Epoch [1/3], Batch [7813/21340], Train Loss: 3.3272, Train Perplexity: 27.8595\n",
            "Epoch [1/3], Batch [7814/21340], Train Loss: 3.3113, Train Perplexity: 27.4200\n",
            "Epoch [1/3], Batch [7815/21340], Train Loss: 3.3316, Train Perplexity: 27.9817\n",
            "Epoch [1/3], Batch [7816/21340], Train Loss: 3.1844, Train Perplexity: 24.1527\n",
            "Epoch [1/3], Batch [7817/21340], Train Loss: 3.2989, Train Perplexity: 27.0817\n",
            "Epoch [1/3], Batch [7818/21340], Train Loss: 3.3073, Train Perplexity: 27.3124\n",
            "Epoch [1/3], Batch [7819/21340], Train Loss: 3.2635, Train Perplexity: 26.1420\n",
            "Epoch [1/3], Batch [7820/21340], Train Loss: 3.2486, Train Perplexity: 25.7536\n",
            "Epoch [1/3], Batch [7821/21340], Train Loss: 3.3554, Train Perplexity: 28.6574\n",
            "Epoch [1/3], Batch [7822/21340], Train Loss: 3.3025, Train Perplexity: 27.1814\n",
            "Epoch [1/3], Batch [7823/21340], Train Loss: 3.2641, Train Perplexity: 26.1564\n",
            "Epoch [1/3], Batch [7824/21340], Train Loss: 3.1626, Train Perplexity: 23.6316\n",
            "Epoch [1/3], Batch [7825/21340], Train Loss: 3.2171, Train Perplexity: 24.9562\n",
            "Epoch [1/3], Batch [7826/21340], Train Loss: 3.3033, Train Perplexity: 27.2016\n",
            "Epoch [1/3], Batch [7827/21340], Train Loss: 3.2198, Train Perplexity: 25.0225\n",
            "Epoch [1/3], Batch [7828/21340], Train Loss: 3.3397, Train Perplexity: 28.2115\n",
            "Epoch [1/3], Batch [7829/21340], Train Loss: 3.3735, Train Perplexity: 29.1819\n",
            "Epoch [1/3], Batch [7830/21340], Train Loss: 3.2923, Train Perplexity: 26.9055\n",
            "Epoch [1/3], Batch [7831/21340], Train Loss: 3.1845, Train Perplexity: 24.1549\n",
            "Epoch [1/3], Batch [7832/21340], Train Loss: 3.1882, Train Perplexity: 24.2443\n",
            "Epoch [1/3], Batch [7833/21340], Train Loss: 3.2643, Train Perplexity: 26.1625\n",
            "Epoch [1/3], Batch [7834/21340], Train Loss: 3.5433, Train Perplexity: 34.5796\n",
            "Epoch [1/3], Batch [7835/21340], Train Loss: 3.2175, Train Perplexity: 24.9650\n",
            "Epoch [1/3], Batch [7836/21340], Train Loss: 3.2352, Train Perplexity: 25.4115\n",
            "Epoch [1/3], Batch [7837/21340], Train Loss: 3.3339, Train Perplexity: 28.0474\n",
            "Epoch [1/3], Batch [7838/21340], Train Loss: 3.1922, Train Perplexity: 24.3420\n",
            "Epoch [1/3], Batch [7839/21340], Train Loss: 3.2653, Train Perplexity: 26.1889\n",
            "Epoch [1/3], Batch [7840/21340], Train Loss: 3.2530, Train Perplexity: 25.8671\n",
            "Epoch [1/3], Batch [7841/21340], Train Loss: 3.2794, Train Perplexity: 26.5588\n",
            "Epoch [1/3], Batch [7842/21340], Train Loss: 3.2262, Train Perplexity: 25.1844\n",
            "Epoch [1/3], Batch [7843/21340], Train Loss: 3.2316, Train Perplexity: 25.3201\n",
            "Epoch [1/3], Batch [7844/21340], Train Loss: 3.1696, Train Perplexity: 23.7972\n",
            "Epoch [1/3], Batch [7845/21340], Train Loss: 3.1774, Train Perplexity: 23.9845\n",
            "Epoch [1/3], Batch [7846/21340], Train Loss: 3.1872, Train Perplexity: 24.2202\n",
            "Epoch [1/3], Batch [7847/21340], Train Loss: 3.2069, Train Perplexity: 24.7015\n",
            "Epoch [1/3], Batch [7848/21340], Train Loss: 3.2614, Train Perplexity: 26.0867\n",
            "Epoch [1/3], Batch [7849/21340], Train Loss: 3.2135, Train Perplexity: 24.8649\n",
            "Epoch [1/3], Batch [7850/21340], Train Loss: 3.4740, Train Perplexity: 32.2655\n",
            "Epoch [1/3], Batch [7851/21340], Train Loss: 3.1814, Train Perplexity: 24.0803\n",
            "Epoch [1/3], Batch [7852/21340], Train Loss: 3.2770, Train Perplexity: 26.4968\n",
            "Epoch [1/3], Batch [7853/21340], Train Loss: 3.3026, Train Perplexity: 27.1830\n",
            "Epoch [1/3], Batch [7854/21340], Train Loss: 3.1988, Train Perplexity: 24.5039\n",
            "Epoch [1/3], Batch [7855/21340], Train Loss: 3.2820, Train Perplexity: 26.6296\n",
            "Epoch [1/3], Batch [7856/21340], Train Loss: 3.2421, Train Perplexity: 25.5885\n",
            "Epoch [1/3], Batch [7857/21340], Train Loss: 3.2963, Train Perplexity: 27.0131\n",
            "Epoch [1/3], Batch [7858/21340], Train Loss: 3.2244, Train Perplexity: 25.1390\n",
            "Epoch [1/3], Batch [7859/21340], Train Loss: 3.2343, Train Perplexity: 25.3893\n",
            "Epoch [1/3], Batch [7860/21340], Train Loss: 3.2927, Train Perplexity: 26.9146\n",
            "Epoch [1/3], Batch [7861/21340], Train Loss: 3.2794, Train Perplexity: 26.5593\n",
            "Epoch [1/3], Batch [7862/21340], Train Loss: 3.3717, Train Perplexity: 29.1279\n",
            "Epoch [1/3], Batch [7863/21340], Train Loss: 3.2180, Train Perplexity: 24.9772\n",
            "Epoch [1/3], Batch [7864/21340], Train Loss: 3.3046, Train Perplexity: 27.2381\n",
            "Epoch [1/3], Batch [7865/21340], Train Loss: 3.2489, Train Perplexity: 25.7631\n",
            "Epoch [1/3], Batch [7866/21340], Train Loss: 3.3220, Train Perplexity: 27.7166\n",
            "Epoch [1/3], Batch [7867/21340], Train Loss: 3.3626, Train Perplexity: 28.8640\n",
            "Epoch [1/3], Batch [7868/21340], Train Loss: 3.2536, Train Perplexity: 25.8833\n",
            "Epoch [1/3], Batch [7869/21340], Train Loss: 3.2561, Train Perplexity: 25.9471\n",
            "Epoch [1/3], Batch [7870/21340], Train Loss: 3.2377, Train Perplexity: 25.4748\n",
            "Epoch [1/3], Batch [7871/21340], Train Loss: 3.1748, Train Perplexity: 23.9228\n",
            "Epoch [1/3], Batch [7872/21340], Train Loss: 3.2586, Train Perplexity: 26.0133\n",
            "Epoch [1/3], Batch [7873/21340], Train Loss: 3.2288, Train Perplexity: 25.2494\n",
            "Epoch [1/3], Batch [7874/21340], Train Loss: 3.2663, Train Perplexity: 26.2141\n",
            "Epoch [1/3], Batch [7875/21340], Train Loss: 3.1316, Train Perplexity: 22.9115\n",
            "Epoch [1/3], Batch [7876/21340], Train Loss: 3.2874, Train Perplexity: 26.7733\n",
            "Epoch [1/3], Batch [7877/21340], Train Loss: 3.3054, Train Perplexity: 27.2585\n",
            "Epoch [1/3], Batch [7878/21340], Train Loss: 3.3277, Train Perplexity: 27.8752\n",
            "Epoch [1/3], Batch [7879/21340], Train Loss: 3.3395, Train Perplexity: 28.2051\n",
            "Epoch [1/3], Batch [7880/21340], Train Loss: 3.3246, Train Perplexity: 27.7887\n",
            "Epoch [1/3], Batch [7881/21340], Train Loss: 3.2163, Train Perplexity: 24.9357\n",
            "Epoch [1/3], Batch [7882/21340], Train Loss: 3.3218, Train Perplexity: 27.7099\n",
            "Epoch [1/3], Batch [7883/21340], Train Loss: 3.1754, Train Perplexity: 23.9355\n",
            "Epoch [1/3], Batch [7884/21340], Train Loss: 3.1882, Train Perplexity: 24.2447\n",
            "Epoch [1/3], Batch [7885/21340], Train Loss: 3.2386, Train Perplexity: 25.4976\n",
            "Epoch [1/3], Batch [7886/21340], Train Loss: 3.2978, Train Perplexity: 27.0518\n",
            "Epoch [1/3], Batch [7887/21340], Train Loss: 3.2289, Train Perplexity: 25.2508\n",
            "Epoch [1/3], Batch [7888/21340], Train Loss: 3.4622, Train Perplexity: 31.8883\n",
            "Epoch [1/3], Batch [7889/21340], Train Loss: 3.3170, Train Perplexity: 27.5766\n",
            "Epoch [1/3], Batch [7890/21340], Train Loss: 3.2057, Train Perplexity: 24.6732\n",
            "Epoch [1/3], Batch [7891/21340], Train Loss: 3.3218, Train Perplexity: 27.7101\n",
            "Epoch [1/3], Batch [7892/21340], Train Loss: 3.2120, Train Perplexity: 24.8288\n",
            "Epoch [1/3], Batch [7893/21340], Train Loss: 3.1668, Train Perplexity: 23.7311\n",
            "Epoch [1/3], Batch [7894/21340], Train Loss: 3.2238, Train Perplexity: 25.1223\n",
            "Epoch [1/3], Batch [7895/21340], Train Loss: 3.1947, Train Perplexity: 24.4030\n",
            "Epoch [1/3], Batch [7896/21340], Train Loss: 3.3223, Train Perplexity: 27.7232\n",
            "Epoch [1/3], Batch [7897/21340], Train Loss: 3.2287, Train Perplexity: 25.2460\n",
            "Epoch [1/3], Batch [7898/21340], Train Loss: 3.1108, Train Perplexity: 22.4392\n",
            "Epoch [1/3], Batch [7899/21340], Train Loss: 3.3051, Train Perplexity: 27.2510\n",
            "Epoch [1/3], Batch [7900/21340], Train Loss: 3.2978, Train Perplexity: 27.0538\n",
            "Epoch [1/3], Batch [7901/21340], Train Loss: 3.2602, Train Perplexity: 26.0555\n",
            "Epoch [1/3], Batch [7902/21340], Train Loss: 3.3323, Train Perplexity: 28.0014\n",
            "Epoch [1/3], Batch [7903/21340], Train Loss: 3.2818, Train Perplexity: 26.6234\n",
            "Epoch [1/3], Batch [7904/21340], Train Loss: 3.1960, Train Perplexity: 24.4340\n",
            "Epoch [1/3], Batch [7905/21340], Train Loss: 3.3296, Train Perplexity: 27.9260\n",
            "Epoch [1/3], Batch [7906/21340], Train Loss: 3.2398, Train Perplexity: 25.5284\n",
            "Epoch [1/3], Batch [7907/21340], Train Loss: 3.3137, Train Perplexity: 27.4855\n",
            "Epoch [1/3], Batch [7908/21340], Train Loss: 3.1463, Train Perplexity: 23.2503\n",
            "Epoch [1/3], Batch [7909/21340], Train Loss: 3.2577, Train Perplexity: 25.9908\n",
            "Epoch [1/3], Batch [7910/21340], Train Loss: 3.2919, Train Perplexity: 26.8948\n",
            "Epoch [1/3], Batch [7911/21340], Train Loss: 3.1621, Train Perplexity: 23.6193\n",
            "Epoch [1/3], Batch [7912/21340], Train Loss: 3.3523, Train Perplexity: 28.5697\n",
            "Epoch [1/3], Batch [7913/21340], Train Loss: 3.1860, Train Perplexity: 24.1912\n",
            "Epoch [1/3], Batch [7914/21340], Train Loss: 3.3127, Train Perplexity: 27.4596\n",
            "Epoch [1/3], Batch [7915/21340], Train Loss: 3.2526, Train Perplexity: 25.8584\n",
            "Epoch [1/3], Batch [7916/21340], Train Loss: 3.1733, Train Perplexity: 23.8856\n",
            "Epoch [1/3], Batch [7917/21340], Train Loss: 3.2096, Train Perplexity: 24.7690\n",
            "Epoch [1/3], Batch [7918/21340], Train Loss: 3.2012, Train Perplexity: 24.5629\n",
            "Epoch [1/3], Batch [7919/21340], Train Loss: 3.2118, Train Perplexity: 24.8234\n",
            "Epoch [1/3], Batch [7920/21340], Train Loss: 3.3059, Train Perplexity: 27.2717\n",
            "Epoch [1/3], Batch [7921/21340], Train Loss: 3.2127, Train Perplexity: 24.8467\n",
            "Epoch [1/3], Batch [7922/21340], Train Loss: 3.2130, Train Perplexity: 24.8544\n",
            "Epoch [1/3], Batch [7923/21340], Train Loss: 3.3284, Train Perplexity: 27.8950\n",
            "Epoch [1/3], Batch [7924/21340], Train Loss: 3.1854, Train Perplexity: 24.1769\n",
            "Epoch [1/3], Batch [7925/21340], Train Loss: 3.2409, Train Perplexity: 25.5565\n",
            "Epoch [1/3], Batch [7926/21340], Train Loss: 3.2157, Train Perplexity: 24.9213\n",
            "Epoch [1/3], Batch [7927/21340], Train Loss: 3.2010, Train Perplexity: 24.5565\n",
            "Epoch [1/3], Batch [7928/21340], Train Loss: 3.3646, Train Perplexity: 28.9207\n",
            "Epoch [1/3], Batch [7929/21340], Train Loss: 3.2671, Train Perplexity: 26.2345\n",
            "Epoch [1/3], Batch [7930/21340], Train Loss: 3.2562, Train Perplexity: 25.9510\n",
            "Epoch [1/3], Batch [7931/21340], Train Loss: 3.1900, Train Perplexity: 24.2887\n",
            "Epoch [1/3], Batch [7932/21340], Train Loss: 3.1845, Train Perplexity: 24.1562\n",
            "Epoch [1/3], Batch [7933/21340], Train Loss: 3.2908, Train Perplexity: 26.8631\n",
            "Epoch [1/3], Batch [7934/21340], Train Loss: 3.2543, Train Perplexity: 25.9019\n",
            "Epoch [1/3], Batch [7935/21340], Train Loss: 3.1883, Train Perplexity: 24.2476\n",
            "Epoch [1/3], Batch [7936/21340], Train Loss: 3.3311, Train Perplexity: 27.9697\n",
            "Epoch [1/3], Batch [7937/21340], Train Loss: 3.3101, Train Perplexity: 27.3869\n",
            "Epoch [1/3], Batch [7938/21340], Train Loss: 3.2887, Train Perplexity: 26.8077\n",
            "Epoch [1/3], Batch [7939/21340], Train Loss: 3.3724, Train Perplexity: 29.1494\n",
            "Epoch [1/3], Batch [7940/21340], Train Loss: 3.2860, Train Perplexity: 26.7355\n",
            "Epoch [1/3], Batch [7941/21340], Train Loss: 3.2991, Train Perplexity: 27.0885\n",
            "Epoch [1/3], Batch [7942/21340], Train Loss: 3.2228, Train Perplexity: 25.0976\n",
            "Epoch [1/3], Batch [7943/21340], Train Loss: 3.2284, Train Perplexity: 25.2399\n",
            "Epoch [1/3], Batch [7944/21340], Train Loss: 3.4048, Train Perplexity: 30.1081\n",
            "Epoch [1/3], Batch [7945/21340], Train Loss: 3.2533, Train Perplexity: 25.8747\n",
            "Epoch [1/3], Batch [7946/21340], Train Loss: 3.2144, Train Perplexity: 24.8875\n",
            "Epoch [1/3], Batch [7947/21340], Train Loss: 3.2823, Train Perplexity: 26.6380\n",
            "Epoch [1/3], Batch [7948/21340], Train Loss: 3.3130, Train Perplexity: 27.4665\n",
            "Epoch [1/3], Batch [7949/21340], Train Loss: 3.2774, Train Perplexity: 26.5071\n",
            "Epoch [1/3], Batch [7950/21340], Train Loss: 3.1916, Train Perplexity: 24.3266\n",
            "Epoch [1/3], Batch [7951/21340], Train Loss: 3.3863, Train Perplexity: 29.5567\n",
            "Epoch [1/3], Batch [7952/21340], Train Loss: 3.2022, Train Perplexity: 24.5859\n",
            "Epoch [1/3], Batch [7953/21340], Train Loss: 3.4558, Train Perplexity: 31.6827\n",
            "Epoch [1/3], Batch [7954/21340], Train Loss: 3.2191, Train Perplexity: 25.0058\n",
            "Epoch [1/3], Batch [7955/21340], Train Loss: 3.2941, Train Perplexity: 26.9528\n",
            "Epoch [1/3], Batch [7956/21340], Train Loss: 3.2942, Train Perplexity: 26.9552\n",
            "Epoch [1/3], Batch [7957/21340], Train Loss: 3.2262, Train Perplexity: 25.1827\n",
            "Epoch [1/3], Batch [7958/21340], Train Loss: 3.3357, Train Perplexity: 28.0986\n",
            "Epoch [1/3], Batch [7959/21340], Train Loss: 3.2877, Train Perplexity: 26.7817\n",
            "Epoch [1/3], Batch [7960/21340], Train Loss: 3.2811, Train Perplexity: 26.6055\n",
            "Epoch [1/3], Batch [7961/21340], Train Loss: 3.1860, Train Perplexity: 24.1921\n",
            "Epoch [1/3], Batch [7962/21340], Train Loss: 3.1824, Train Perplexity: 24.1049\n",
            "Epoch [1/3], Batch [7963/21340], Train Loss: 3.2278, Train Perplexity: 25.2244\n",
            "Epoch [1/3], Batch [7964/21340], Train Loss: 3.2416, Train Perplexity: 25.5745\n",
            "Epoch [1/3], Batch [7965/21340], Train Loss: 3.3455, Train Perplexity: 28.3733\n",
            "Epoch [1/3], Batch [7966/21340], Train Loss: 3.2784, Train Perplexity: 26.5332\n",
            "Epoch [1/3], Batch [7967/21340], Train Loss: 3.4044, Train Perplexity: 30.0974\n",
            "Epoch [1/3], Batch [7968/21340], Train Loss: 3.3471, Train Perplexity: 28.4200\n",
            "Epoch [1/3], Batch [7969/21340], Train Loss: 3.1707, Train Perplexity: 23.8250\n",
            "Epoch [1/3], Batch [7970/21340], Train Loss: 3.2234, Train Perplexity: 25.1122\n",
            "Epoch [1/3], Batch [7971/21340], Train Loss: 3.2436, Train Perplexity: 25.6256\n",
            "Epoch [1/3], Batch [7972/21340], Train Loss: 3.2399, Train Perplexity: 25.5317\n",
            "Epoch [1/3], Batch [7973/21340], Train Loss: 3.2515, Train Perplexity: 25.8284\n",
            "Epoch [1/3], Batch [7974/21340], Train Loss: 3.1887, Train Perplexity: 24.2581\n",
            "Epoch [1/3], Batch [7975/21340], Train Loss: 3.3982, Train Perplexity: 29.9090\n",
            "Epoch [1/3], Batch [7976/21340], Train Loss: 3.3721, Train Perplexity: 29.1403\n",
            "Epoch [1/3], Batch [7977/21340], Train Loss: 3.2351, Train Perplexity: 25.4095\n",
            "Epoch [1/3], Batch [7978/21340], Train Loss: 3.2106, Train Perplexity: 24.7933\n",
            "Epoch [1/3], Batch [7979/21340], Train Loss: 3.2431, Train Perplexity: 25.6130\n",
            "Epoch [1/3], Batch [7980/21340], Train Loss: 3.2508, Train Perplexity: 25.8112\n",
            "Epoch [1/3], Batch [7981/21340], Train Loss: 3.2489, Train Perplexity: 25.7610\n",
            "Epoch [1/3], Batch [7982/21340], Train Loss: 3.2341, Train Perplexity: 25.3825\n",
            "Epoch [1/3], Batch [7983/21340], Train Loss: 3.2352, Train Perplexity: 25.4124\n",
            "Epoch [1/3], Batch [7984/21340], Train Loss: 3.2348, Train Perplexity: 25.4010\n",
            "Epoch [1/3], Batch [7985/21340], Train Loss: 3.2413, Train Perplexity: 25.5677\n",
            "Epoch [1/3], Batch [7986/21340], Train Loss: 3.2858, Train Perplexity: 26.7292\n",
            "Epoch [1/3], Batch [7987/21340], Train Loss: 3.3047, Train Perplexity: 27.2415\n",
            "Epoch [1/3], Batch [7988/21340], Train Loss: 3.3518, Train Perplexity: 28.5533\n",
            "Epoch [1/3], Batch [7989/21340], Train Loss: 3.3402, Train Perplexity: 28.2248\n",
            "Epoch [1/3], Batch [7990/21340], Train Loss: 3.3081, Train Perplexity: 27.3331\n",
            "Epoch [1/3], Batch [7991/21340], Train Loss: 3.4081, Train Perplexity: 30.2090\n",
            "Epoch [1/3], Batch [7992/21340], Train Loss: 3.2807, Train Perplexity: 26.5935\n",
            "Epoch [1/3], Batch [7993/21340], Train Loss: 3.2556, Train Perplexity: 25.9341\n",
            "Epoch [1/3], Batch [7994/21340], Train Loss: 3.2175, Train Perplexity: 24.9646\n",
            "Epoch [1/3], Batch [7995/21340], Train Loss: 3.2927, Train Perplexity: 26.9145\n",
            "Epoch [1/3], Batch [7996/21340], Train Loss: 3.2680, Train Perplexity: 26.2586\n",
            "Epoch [1/3], Batch [7997/21340], Train Loss: 3.3818, Train Perplexity: 29.4238\n",
            "Epoch [1/3], Batch [7998/21340], Train Loss: 3.2637, Train Perplexity: 26.1463\n",
            "Epoch [1/3], Batch [7999/21340], Train Loss: 3.2344, Train Perplexity: 25.3913\n",
            "Epoch [1/3], Batch [8000/21340], Train Loss: 3.2950, Train Perplexity: 26.9779\n",
            "Epoch [1/3], Batch [8001/21340], Train Loss: 3.1997, Train Perplexity: 24.5254\n",
            "Epoch [1/3], Batch [8002/21340], Train Loss: 3.2060, Train Perplexity: 24.6809\n",
            "Epoch [1/3], Batch [8003/21340], Train Loss: 3.2346, Train Perplexity: 25.3966\n",
            "Epoch [1/3], Batch [8004/21340], Train Loss: 3.3003, Train Perplexity: 27.1197\n",
            "Epoch [1/3], Batch [8005/21340], Train Loss: 3.3046, Train Perplexity: 27.2374\n",
            "Epoch [1/3], Batch [8006/21340], Train Loss: 3.3386, Train Perplexity: 28.1789\n",
            "Epoch [1/3], Batch [8007/21340], Train Loss: 3.1713, Train Perplexity: 23.8379\n",
            "Epoch [1/3], Batch [8008/21340], Train Loss: 3.4988, Train Perplexity: 33.0754\n",
            "Epoch [1/3], Batch [8009/21340], Train Loss: 3.2596, Train Perplexity: 26.0384\n",
            "Epoch [1/3], Batch [8010/21340], Train Loss: 3.4219, Train Perplexity: 30.6279\n",
            "Epoch [1/3], Batch [8011/21340], Train Loss: 3.4234, Train Perplexity: 30.6744\n",
            "Epoch [1/3], Batch [8012/21340], Train Loss: 3.1608, Train Perplexity: 23.5891\n",
            "Epoch [1/3], Batch [8013/21340], Train Loss: 3.2160, Train Perplexity: 24.9275\n",
            "Epoch [1/3], Batch [8014/21340], Train Loss: 3.3507, Train Perplexity: 28.5230\n",
            "Epoch [1/3], Batch [8015/21340], Train Loss: 3.2951, Train Perplexity: 26.9790\n",
            "Epoch [1/3], Batch [8016/21340], Train Loss: 3.2647, Train Perplexity: 26.1718\n",
            "Epoch [1/3], Batch [8017/21340], Train Loss: 3.2177, Train Perplexity: 24.9699\n",
            "Epoch [1/3], Batch [8018/21340], Train Loss: 3.2322, Train Perplexity: 25.3361\n",
            "Epoch [1/3], Batch [8019/21340], Train Loss: 3.2278, Train Perplexity: 25.2240\n",
            "Epoch [1/3], Batch [8020/21340], Train Loss: 3.2914, Train Perplexity: 26.8804\n",
            "Epoch [1/3], Batch [8021/21340], Train Loss: 3.1683, Train Perplexity: 23.7676\n",
            "Epoch [1/3], Batch [8022/21340], Train Loss: 3.2627, Train Perplexity: 26.1195\n",
            "Epoch [1/3], Batch [8023/21340], Train Loss: 3.2228, Train Perplexity: 25.0977\n",
            "Epoch [1/3], Batch [8024/21340], Train Loss: 3.2021, Train Perplexity: 24.5852\n",
            "Epoch [1/3], Batch [8025/21340], Train Loss: 3.3042, Train Perplexity: 27.2277\n",
            "Epoch [1/3], Batch [8026/21340], Train Loss: 3.1924, Train Perplexity: 24.3476\n",
            "Epoch [1/3], Batch [8027/21340], Train Loss: 3.2874, Train Perplexity: 26.7720\n",
            "Epoch [1/3], Batch [8028/21340], Train Loss: 3.2470, Train Perplexity: 25.7142\n",
            "Epoch [1/3], Batch [8029/21340], Train Loss: 3.2238, Train Perplexity: 25.1246\n",
            "Epoch [1/3], Batch [8030/21340], Train Loss: 3.2742, Train Perplexity: 26.4222\n",
            "Epoch [1/3], Batch [8031/21340], Train Loss: 3.3292, Train Perplexity: 27.9146\n",
            "Epoch [1/3], Batch [8032/21340], Train Loss: 3.3747, Train Perplexity: 29.2143\n",
            "Epoch [1/3], Batch [8033/21340], Train Loss: 3.2362, Train Perplexity: 25.4365\n",
            "Epoch [1/3], Batch [8034/21340], Train Loss: 3.2034, Train Perplexity: 24.6173\n",
            "Epoch [1/3], Batch [8035/21340], Train Loss: 3.2356, Train Perplexity: 25.4228\n",
            "Epoch [1/3], Batch [8036/21340], Train Loss: 3.1687, Train Perplexity: 23.7774\n",
            "Epoch [1/3], Batch [8037/21340], Train Loss: 3.2715, Train Perplexity: 26.3513\n",
            "Epoch [1/3], Batch [8038/21340], Train Loss: 3.2459, Train Perplexity: 25.6854\n",
            "Epoch [1/3], Batch [8039/21340], Train Loss: 3.2957, Train Perplexity: 26.9973\n",
            "Epoch [1/3], Batch [8040/21340], Train Loss: 3.2799, Train Perplexity: 26.5738\n",
            "Epoch [1/3], Batch [8041/21340], Train Loss: 3.2248, Train Perplexity: 25.1482\n",
            "Epoch [1/3], Batch [8042/21340], Train Loss: 3.2827, Train Perplexity: 26.6482\n",
            "Epoch [1/3], Batch [8043/21340], Train Loss: 3.2489, Train Perplexity: 25.7614\n",
            "Epoch [1/3], Batch [8044/21340], Train Loss: 3.2836, Train Perplexity: 26.6709\n",
            "Epoch [1/3], Batch [8045/21340], Train Loss: 3.3167, Train Perplexity: 27.5689\n",
            "Epoch [1/3], Batch [8046/21340], Train Loss: 3.2469, Train Perplexity: 25.7114\n",
            "Epoch [1/3], Batch [8047/21340], Train Loss: 3.2782, Train Perplexity: 26.5284\n",
            "Epoch [1/3], Batch [8048/21340], Train Loss: 3.2442, Train Perplexity: 25.6409\n",
            "Epoch [1/3], Batch [8049/21340], Train Loss: 3.3325, Train Perplexity: 28.0094\n",
            "Epoch [1/3], Batch [8050/21340], Train Loss: 3.2273, Train Perplexity: 25.2107\n",
            "Epoch [1/3], Batch [8051/21340], Train Loss: 3.2609, Train Perplexity: 26.0732\n",
            "Epoch [1/3], Batch [8052/21340], Train Loss: 3.1780, Train Perplexity: 23.9992\n",
            "Epoch [1/3], Batch [8053/21340], Train Loss: 3.2640, Train Perplexity: 26.1544\n",
            "Epoch [1/3], Batch [8054/21340], Train Loss: 3.1510, Train Perplexity: 23.3585\n",
            "Epoch [1/3], Batch [8055/21340], Train Loss: 3.1540, Train Perplexity: 23.4286\n",
            "Epoch [1/3], Batch [8056/21340], Train Loss: 3.2521, Train Perplexity: 25.8451\n",
            "Epoch [1/3], Batch [8057/21340], Train Loss: 3.3304, Train Perplexity: 27.9484\n",
            "Epoch [1/3], Batch [8058/21340], Train Loss: 3.1862, Train Perplexity: 24.1958\n",
            "Epoch [1/3], Batch [8059/21340], Train Loss: 3.3168, Train Perplexity: 27.5709\n",
            "Epoch [1/3], Batch [8060/21340], Train Loss: 3.2565, Train Perplexity: 25.9597\n",
            "Epoch [1/3], Batch [8061/21340], Train Loss: 3.1893, Train Perplexity: 24.2717\n",
            "Epoch [1/3], Batch [8062/21340], Train Loss: 3.1248, Train Perplexity: 22.7555\n",
            "Epoch [1/3], Batch [8063/21340], Train Loss: 3.3219, Train Perplexity: 27.7127\n",
            "Epoch [1/3], Batch [8064/21340], Train Loss: 3.2234, Train Perplexity: 25.1132\n",
            "Epoch [1/3], Batch [8065/21340], Train Loss: 3.2830, Train Perplexity: 26.6566\n",
            "Epoch [1/3], Batch [8066/21340], Train Loss: 3.3341, Train Perplexity: 28.0535\n",
            "Epoch [1/3], Batch [8067/21340], Train Loss: 3.2208, Train Perplexity: 25.0472\n",
            "Epoch [1/3], Batch [8068/21340], Train Loss: 3.2894, Train Perplexity: 26.8268\n",
            "Epoch [1/3], Batch [8069/21340], Train Loss: 3.2865, Train Perplexity: 26.7479\n",
            "Epoch [1/3], Batch [8070/21340], Train Loss: 3.3082, Train Perplexity: 27.3364\n",
            "Epoch [1/3], Batch [8071/21340], Train Loss: 3.3681, Train Perplexity: 29.0234\n",
            "Epoch [1/3], Batch [8072/21340], Train Loss: 3.2725, Train Perplexity: 26.3769\n",
            "Epoch [1/3], Batch [8073/21340], Train Loss: 3.1788, Train Perplexity: 24.0169\n",
            "Epoch [1/3], Batch [8074/21340], Train Loss: 3.1784, Train Perplexity: 24.0083\n",
            "Epoch [1/3], Batch [8075/21340], Train Loss: 3.1780, Train Perplexity: 23.9976\n",
            "Epoch [1/3], Batch [8076/21340], Train Loss: 3.2100, Train Perplexity: 24.7787\n",
            "Epoch [1/3], Batch [8077/21340], Train Loss: 3.2610, Train Perplexity: 26.0758\n",
            "Epoch [1/3], Batch [8078/21340], Train Loss: 3.2451, Train Perplexity: 25.6643\n",
            "Epoch [1/3], Batch [8079/21340], Train Loss: 3.2530, Train Perplexity: 25.8666\n",
            "Epoch [1/3], Batch [8080/21340], Train Loss: 3.3308, Train Perplexity: 27.9596\n",
            "Epoch [1/3], Batch [8081/21340], Train Loss: 3.2398, Train Perplexity: 25.5296\n",
            "Epoch [1/3], Batch [8082/21340], Train Loss: 3.2608, Train Perplexity: 26.0700\n",
            "Epoch [1/3], Batch [8083/21340], Train Loss: 3.2599, Train Perplexity: 26.0465\n",
            "Epoch [1/3], Batch [8084/21340], Train Loss: 3.2591, Train Perplexity: 26.0270\n",
            "Epoch [1/3], Batch [8085/21340], Train Loss: 3.2476, Train Perplexity: 25.7289\n",
            "Epoch [1/3], Batch [8086/21340], Train Loss: 3.2423, Train Perplexity: 25.5923\n",
            "Epoch [1/3], Batch [8087/21340], Train Loss: 3.3405, Train Perplexity: 28.2337\n",
            "Epoch [1/3], Batch [8088/21340], Train Loss: 3.2504, Train Perplexity: 25.8009\n",
            "Epoch [1/3], Batch [8089/21340], Train Loss: 3.2724, Train Perplexity: 26.3739\n",
            "Epoch [1/3], Batch [8090/21340], Train Loss: 3.1638, Train Perplexity: 23.6592\n",
            "Epoch [1/3], Batch [8091/21340], Train Loss: 3.3009, Train Perplexity: 27.1381\n",
            "Epoch [1/3], Batch [8092/21340], Train Loss: 3.2942, Train Perplexity: 26.9549\n",
            "Epoch [1/3], Batch [8093/21340], Train Loss: 3.2081, Train Perplexity: 24.7320\n",
            "Epoch [1/3], Batch [8094/21340], Train Loss: 3.2100, Train Perplexity: 24.7794\n",
            "Epoch [1/3], Batch [8095/21340], Train Loss: 3.2314, Train Perplexity: 25.3154\n",
            "Epoch [1/3], Batch [8096/21340], Train Loss: 3.2286, Train Perplexity: 25.2455\n",
            "Epoch [1/3], Batch [8097/21340], Train Loss: 3.1428, Train Perplexity: 23.1679\n",
            "Epoch [1/3], Batch [8098/21340], Train Loss: 3.1974, Train Perplexity: 24.4689\n",
            "Epoch [1/3], Batch [8099/21340], Train Loss: 3.1652, Train Perplexity: 23.6935\n",
            "Epoch [1/3], Batch [8100/21340], Train Loss: 3.2015, Train Perplexity: 24.5705\n",
            "Epoch [1/3], Batch [8101/21340], Train Loss: 3.3703, Train Perplexity: 29.0877\n",
            "Epoch [1/3], Batch [8102/21340], Train Loss: 3.1928, Train Perplexity: 24.3560\n",
            "Epoch [1/3], Batch [8103/21340], Train Loss: 3.4064, Train Perplexity: 30.1560\n",
            "Epoch [1/3], Batch [8104/21340], Train Loss: 3.1815, Train Perplexity: 24.0840\n",
            "Epoch [1/3], Batch [8105/21340], Train Loss: 3.2750, Train Perplexity: 26.4420\n",
            "Epoch [1/3], Batch [8106/21340], Train Loss: 3.2252, Train Perplexity: 25.1578\n",
            "Epoch [1/3], Batch [8107/21340], Train Loss: 3.1694, Train Perplexity: 23.7931\n",
            "Epoch [1/3], Batch [8108/21340], Train Loss: 3.4345, Train Perplexity: 31.0161\n",
            "Epoch [1/3], Batch [8109/21340], Train Loss: 3.1783, Train Perplexity: 24.0064\n",
            "Epoch [1/3], Batch [8110/21340], Train Loss: 3.3307, Train Perplexity: 27.9581\n",
            "Epoch [1/3], Batch [8111/21340], Train Loss: 3.1909, Train Perplexity: 24.3112\n",
            "Epoch [1/3], Batch [8112/21340], Train Loss: 3.6012, Train Perplexity: 36.6419\n",
            "Epoch [1/3], Batch [8113/21340], Train Loss: 3.2379, Train Perplexity: 25.4791\n",
            "Epoch [1/3], Batch [8114/21340], Train Loss: 3.3080, Train Perplexity: 27.3293\n",
            "Epoch [1/3], Batch [8115/21340], Train Loss: 3.2770, Train Perplexity: 26.4966\n",
            "Epoch [1/3], Batch [8116/21340], Train Loss: 3.4276, Train Perplexity: 30.8030\n",
            "Epoch [1/3], Batch [8117/21340], Train Loss: 3.2578, Train Perplexity: 25.9924\n",
            "Epoch [1/3], Batch [8118/21340], Train Loss: 3.3702, Train Perplexity: 29.0832\n",
            "Epoch [1/3], Batch [8119/21340], Train Loss: 3.3153, Train Perplexity: 27.5308\n",
            "Epoch [1/3], Batch [8120/21340], Train Loss: 3.1955, Train Perplexity: 24.4221\n",
            "Epoch [1/3], Batch [8121/21340], Train Loss: 3.2657, Train Perplexity: 26.1984\n",
            "Epoch [1/3], Batch [8122/21340], Train Loss: 3.1676, Train Perplexity: 23.7509\n",
            "Epoch [1/3], Batch [8123/21340], Train Loss: 3.1939, Train Perplexity: 24.3832\n",
            "Epoch [1/3], Batch [8124/21340], Train Loss: 3.2425, Train Perplexity: 25.5970\n",
            "Epoch [1/3], Batch [8125/21340], Train Loss: 3.2392, Train Perplexity: 25.5145\n",
            "Epoch [1/3], Batch [8126/21340], Train Loss: 3.2390, Train Perplexity: 25.5071\n",
            "Epoch [1/3], Batch [8127/21340], Train Loss: 3.1840, Train Perplexity: 24.1425\n",
            "Epoch [1/3], Batch [8128/21340], Train Loss: 3.2842, Train Perplexity: 26.6874\n",
            "Epoch [1/3], Batch [8129/21340], Train Loss: 3.3568, Train Perplexity: 28.6969\n",
            "Epoch [1/3], Batch [8130/21340], Train Loss: 3.2310, Train Perplexity: 25.3051\n",
            "Epoch [1/3], Batch [8131/21340], Train Loss: 3.3108, Train Perplexity: 27.4073\n",
            "Epoch [1/3], Batch [8132/21340], Train Loss: 3.2561, Train Perplexity: 25.9476\n",
            "Epoch [1/3], Batch [8133/21340], Train Loss: 3.2750, Train Perplexity: 26.4443\n",
            "Epoch [1/3], Batch [8134/21340], Train Loss: 3.2808, Train Perplexity: 26.5968\n",
            "Epoch [1/3], Batch [8135/21340], Train Loss: 3.2909, Train Perplexity: 26.8669\n",
            "Epoch [1/3], Batch [8136/21340], Train Loss: 3.3254, Train Perplexity: 27.8105\n",
            "Epoch [1/3], Batch [8137/21340], Train Loss: 3.3833, Train Perplexity: 29.4666\n",
            "Epoch [1/3], Batch [8138/21340], Train Loss: 3.2805, Train Perplexity: 26.5897\n",
            "Epoch [1/3], Batch [8139/21340], Train Loss: 3.2511, Train Perplexity: 25.8180\n",
            "Epoch [1/3], Batch [8140/21340], Train Loss: 3.2702, Train Perplexity: 26.3172\n",
            "Epoch [1/3], Batch [8141/21340], Train Loss: 3.3010, Train Perplexity: 27.1410\n",
            "Epoch [1/3], Batch [8142/21340], Train Loss: 3.4128, Train Perplexity: 30.3501\n",
            "Epoch [1/3], Batch [8143/21340], Train Loss: 3.1705, Train Perplexity: 23.8193\n",
            "Epoch [1/3], Batch [8144/21340], Train Loss: 3.2969, Train Perplexity: 27.0296\n",
            "Epoch [1/3], Batch [8145/21340], Train Loss: 3.2007, Train Perplexity: 24.5505\n",
            "Epoch [1/3], Batch [8146/21340], Train Loss: 3.3771, Train Perplexity: 29.2845\n",
            "Epoch [1/3], Batch [8147/21340], Train Loss: 3.2546, Train Perplexity: 25.9080\n",
            "Epoch [1/3], Batch [8148/21340], Train Loss: 3.2734, Train Perplexity: 26.4005\n",
            "Epoch [1/3], Batch [8149/21340], Train Loss: 3.2732, Train Perplexity: 26.3963\n",
            "Epoch [1/3], Batch [8150/21340], Train Loss: 3.3519, Train Perplexity: 28.5575\n",
            "Epoch [1/3], Batch [8151/21340], Train Loss: 3.4087, Train Perplexity: 30.2247\n",
            "Epoch [1/3], Batch [8152/21340], Train Loss: 3.2231, Train Perplexity: 25.1069\n",
            "Epoch [1/3], Batch [8153/21340], Train Loss: 3.3330, Train Perplexity: 28.0233\n",
            "Epoch [1/3], Batch [8154/21340], Train Loss: 3.2162, Train Perplexity: 24.9327\n",
            "Epoch [1/3], Batch [8155/21340], Train Loss: 3.2558, Train Perplexity: 25.9416\n",
            "Epoch [1/3], Batch [8156/21340], Train Loss: 3.3980, Train Perplexity: 29.9040\n",
            "Epoch [1/3], Batch [8157/21340], Train Loss: 3.3139, Train Perplexity: 27.4925\n",
            "Epoch [1/3], Batch [8158/21340], Train Loss: 3.1637, Train Perplexity: 23.6573\n",
            "Epoch [1/3], Batch [8159/21340], Train Loss: 3.1773, Train Perplexity: 23.9822\n",
            "Epoch [1/3], Batch [8160/21340], Train Loss: 3.2340, Train Perplexity: 25.3818\n",
            "Epoch [1/3], Batch [8161/21340], Train Loss: 3.4237, Train Perplexity: 30.6815\n",
            "Epoch [1/3], Batch [8162/21340], Train Loss: 3.2170, Train Perplexity: 24.9528\n",
            "Epoch [1/3], Batch [8163/21340], Train Loss: 3.3709, Train Perplexity: 29.1055\n",
            "Epoch [1/3], Batch [8164/21340], Train Loss: 3.2624, Train Perplexity: 26.1122\n",
            "Epoch [1/3], Batch [8165/21340], Train Loss: 3.2848, Train Perplexity: 26.7035\n",
            "Epoch [1/3], Batch [8166/21340], Train Loss: 3.2946, Train Perplexity: 26.9679\n",
            "Epoch [1/3], Batch [8167/21340], Train Loss: 3.2923, Train Perplexity: 26.9055\n",
            "Epoch [1/3], Batch [8168/21340], Train Loss: 3.3048, Train Perplexity: 27.2425\n",
            "Epoch [1/3], Batch [8169/21340], Train Loss: 3.3065, Train Perplexity: 27.2899\n",
            "Epoch [1/3], Batch [8170/21340], Train Loss: 3.3305, Train Perplexity: 27.9533\n",
            "Epoch [1/3], Batch [8171/21340], Train Loss: 3.3192, Train Perplexity: 27.6382\n",
            "Epoch [1/3], Batch [8172/21340], Train Loss: 3.2242, Train Perplexity: 25.1343\n",
            "Epoch [1/3], Batch [8173/21340], Train Loss: 3.2206, Train Perplexity: 25.0443\n",
            "Epoch [1/3], Batch [8174/21340], Train Loss: 3.1894, Train Perplexity: 24.2733\n",
            "Epoch [1/3], Batch [8175/21340], Train Loss: 3.1950, Train Perplexity: 24.4109\n",
            "Epoch [1/3], Batch [8176/21340], Train Loss: 3.1549, Train Perplexity: 23.4505\n",
            "Epoch [1/3], Batch [8177/21340], Train Loss: 3.2892, Train Perplexity: 26.8208\n",
            "Epoch [1/3], Batch [8178/21340], Train Loss: 3.1762, Train Perplexity: 23.9548\n",
            "Epoch [1/3], Batch [8179/21340], Train Loss: 3.3115, Train Perplexity: 27.4268\n",
            "Epoch [1/3], Batch [8180/21340], Train Loss: 3.3258, Train Perplexity: 27.8222\n",
            "Epoch [1/3], Batch [8181/21340], Train Loss: 3.3299, Train Perplexity: 27.9360\n",
            "Epoch [1/3], Batch [8182/21340], Train Loss: 3.3292, Train Perplexity: 27.9147\n",
            "Epoch [1/3], Batch [8183/21340], Train Loss: 3.2950, Train Perplexity: 26.9782\n",
            "Epoch [1/3], Batch [8184/21340], Train Loss: 3.3078, Train Perplexity: 27.3241\n",
            "Epoch [1/3], Batch [8185/21340], Train Loss: 3.2227, Train Perplexity: 25.0958\n",
            "Epoch [1/3], Batch [8186/21340], Train Loss: 3.3086, Train Perplexity: 27.3459\n",
            "Epoch [1/3], Batch [8187/21340], Train Loss: 3.3377, Train Perplexity: 28.1542\n",
            "Epoch [1/3], Batch [8188/21340], Train Loss: 3.3881, Train Perplexity: 29.6088\n",
            "Epoch [1/3], Batch [8189/21340], Train Loss: 3.3712, Train Perplexity: 29.1128\n",
            "Epoch [1/3], Batch [8190/21340], Train Loss: 3.1952, Train Perplexity: 24.4154\n",
            "Epoch [1/3], Batch [8191/21340], Train Loss: 3.2658, Train Perplexity: 26.2021\n",
            "Epoch [1/3], Batch [8192/21340], Train Loss: 3.2431, Train Perplexity: 25.6134\n",
            "Epoch [1/3], Batch [8193/21340], Train Loss: 3.1704, Train Perplexity: 23.8179\n",
            "Epoch [1/3], Batch [8194/21340], Train Loss: 3.2822, Train Perplexity: 26.6352\n",
            "Epoch [1/3], Batch [8195/21340], Train Loss: 3.2726, Train Perplexity: 26.3802\n",
            "Epoch [1/3], Batch [8196/21340], Train Loss: 3.3376, Train Perplexity: 28.1520\n",
            "Epoch [1/3], Batch [8197/21340], Train Loss: 3.3554, Train Perplexity: 28.6558\n",
            "Epoch [1/3], Batch [8198/21340], Train Loss: 3.2760, Train Perplexity: 26.4706\n",
            "Epoch [1/3], Batch [8199/21340], Train Loss: 3.1592, Train Perplexity: 23.5522\n",
            "Epoch [1/3], Batch [8200/21340], Train Loss: 3.3127, Train Perplexity: 27.4602\n",
            "Epoch [1/3], Batch [8201/21340], Train Loss: 3.3335, Train Perplexity: 28.0368\n",
            "Epoch [1/3], Batch [8202/21340], Train Loss: 3.2233, Train Perplexity: 25.1108\n",
            "Epoch [1/3], Batch [8203/21340], Train Loss: 3.2510, Train Perplexity: 25.8154\n",
            "Epoch [1/3], Batch [8204/21340], Train Loss: 3.2495, Train Perplexity: 25.7766\n",
            "Epoch [1/3], Batch [8205/21340], Train Loss: 3.1989, Train Perplexity: 24.5068\n",
            "Epoch [1/3], Batch [8206/21340], Train Loss: 3.2885, Train Perplexity: 26.8025\n",
            "Epoch [1/3], Batch [8207/21340], Train Loss: 3.3501, Train Perplexity: 28.5066\n",
            "Epoch [1/3], Batch [8208/21340], Train Loss: 3.2459, Train Perplexity: 25.6849\n",
            "Epoch [1/3], Batch [8209/21340], Train Loss: 3.3531, Train Perplexity: 28.5915\n",
            "Epoch [1/3], Batch [8210/21340], Train Loss: 3.2553, Train Perplexity: 25.9271\n",
            "Epoch [1/3], Batch [8211/21340], Train Loss: 3.2446, Train Perplexity: 25.6521\n",
            "Epoch [1/3], Batch [8212/21340], Train Loss: 3.2295, Train Perplexity: 25.2678\n",
            "Epoch [1/3], Batch [8213/21340], Train Loss: 3.3390, Train Perplexity: 28.1908\n",
            "Epoch [1/3], Batch [8214/21340], Train Loss: 3.3221, Train Perplexity: 27.7196\n",
            "Epoch [1/3], Batch [8215/21340], Train Loss: 3.2757, Train Perplexity: 26.4615\n",
            "Epoch [1/3], Batch [8216/21340], Train Loss: 3.2863, Train Perplexity: 26.7434\n",
            "Epoch [1/3], Batch [8217/21340], Train Loss: 3.1938, Train Perplexity: 24.3799\n",
            "Epoch [1/3], Batch [8218/21340], Train Loss: 3.4102, Train Perplexity: 30.2708\n",
            "Epoch [1/3], Batch [8219/21340], Train Loss: 3.3583, Train Perplexity: 28.7408\n",
            "Epoch [1/3], Batch [8220/21340], Train Loss: 3.1944, Train Perplexity: 24.3960\n",
            "Epoch [1/3], Batch [8221/21340], Train Loss: 3.3164, Train Perplexity: 27.5603\n",
            "Epoch [1/3], Batch [8222/21340], Train Loss: 3.3920, Train Perplexity: 29.7243\n",
            "Epoch [1/3], Batch [8223/21340], Train Loss: 3.3290, Train Perplexity: 27.9117\n",
            "Epoch [1/3], Batch [8224/21340], Train Loss: 3.3023, Train Perplexity: 27.1739\n",
            "Epoch [1/3], Batch [8225/21340], Train Loss: 3.2790, Train Perplexity: 26.5503\n",
            "Epoch [1/3], Batch [8226/21340], Train Loss: 3.2486, Train Perplexity: 25.7534\n",
            "Epoch [1/3], Batch [8227/21340], Train Loss: 3.2443, Train Perplexity: 25.6446\n",
            "Epoch [1/3], Batch [8228/21340], Train Loss: 3.2446, Train Perplexity: 25.6503\n",
            "Epoch [1/3], Batch [8229/21340], Train Loss: 3.3236, Train Perplexity: 27.7602\n",
            "Epoch [1/3], Batch [8230/21340], Train Loss: 3.3158, Train Perplexity: 27.5455\n",
            "Epoch [1/3], Batch [8231/21340], Train Loss: 3.1758, Train Perplexity: 23.9449\n",
            "Epoch [1/3], Batch [8232/21340], Train Loss: 3.2203, Train Perplexity: 25.0358\n",
            "Epoch [1/3], Batch [8233/21340], Train Loss: 3.2894, Train Perplexity: 26.8262\n",
            "Epoch [1/3], Batch [8234/21340], Train Loss: 3.2508, Train Perplexity: 25.8121\n",
            "Epoch [1/3], Batch [8235/21340], Train Loss: 3.3374, Train Perplexity: 28.1445\n",
            "Epoch [1/3], Batch [8236/21340], Train Loss: 3.2614, Train Perplexity: 26.0864\n",
            "Epoch [1/3], Batch [8237/21340], Train Loss: 3.2481, Train Perplexity: 25.7407\n",
            "Epoch [1/3], Batch [8238/21340], Train Loss: 3.1944, Train Perplexity: 24.3954\n",
            "Epoch [1/3], Batch [8239/21340], Train Loss: 3.3227, Train Perplexity: 27.7355\n",
            "Epoch [1/3], Batch [8240/21340], Train Loss: 3.3188, Train Perplexity: 27.6280\n",
            "Epoch [1/3], Batch [8241/21340], Train Loss: 3.2001, Train Perplexity: 24.5355\n",
            "Epoch [1/3], Batch [8242/21340], Train Loss: 3.2570, Train Perplexity: 25.9710\n",
            "Epoch [1/3], Batch [8243/21340], Train Loss: 3.3997, Train Perplexity: 29.9564\n",
            "Epoch [1/3], Batch [8244/21340], Train Loss: 3.2968, Train Perplexity: 27.0268\n",
            "Epoch [1/3], Batch [8245/21340], Train Loss: 3.2406, Train Perplexity: 25.5481\n",
            "Epoch [1/3], Batch [8246/21340], Train Loss: 3.1828, Train Perplexity: 24.1144\n",
            "Epoch [1/3], Batch [8247/21340], Train Loss: 3.2998, Train Perplexity: 27.1082\n",
            "Epoch [1/3], Batch [8248/21340], Train Loss: 3.4290, Train Perplexity: 30.8472\n",
            "Epoch [1/3], Batch [8249/21340], Train Loss: 3.2358, Train Perplexity: 25.4256\n",
            "Epoch [1/3], Batch [8250/21340], Train Loss: 3.2044, Train Perplexity: 24.6417\n",
            "Epoch [1/3], Batch [8251/21340], Train Loss: 3.3480, Train Perplexity: 28.4452\n",
            "Epoch [1/3], Batch [8252/21340], Train Loss: 3.1531, Train Perplexity: 23.4090\n",
            "Epoch [1/3], Batch [8253/21340], Train Loss: 3.2043, Train Perplexity: 24.6385\n",
            "Epoch [1/3], Batch [8254/21340], Train Loss: 3.1751, Train Perplexity: 23.9282\n",
            "Epoch [1/3], Batch [8255/21340], Train Loss: 3.3167, Train Perplexity: 27.5704\n",
            "Epoch [1/3], Batch [8256/21340], Train Loss: 3.3078, Train Perplexity: 27.3261\n",
            "Epoch [1/3], Batch [8257/21340], Train Loss: 3.3106, Train Perplexity: 27.4026\n",
            "Epoch [1/3], Batch [8258/21340], Train Loss: 3.2717, Train Perplexity: 26.3569\n",
            "Epoch [1/3], Batch [8259/21340], Train Loss: 3.2282, Train Perplexity: 25.2341\n",
            "Epoch [1/3], Batch [8260/21340], Train Loss: 3.2325, Train Perplexity: 25.3433\n",
            "Epoch [1/3], Batch [8261/21340], Train Loss: 3.4274, Train Perplexity: 30.7978\n",
            "Epoch [1/3], Batch [8262/21340], Train Loss: 3.2096, Train Perplexity: 24.7682\n",
            "Epoch [1/3], Batch [8263/21340], Train Loss: 3.1989, Train Perplexity: 24.5066\n",
            "Epoch [1/3], Batch [8264/21340], Train Loss: 3.2328, Train Perplexity: 25.3513\n",
            "Epoch [1/3], Batch [8265/21340], Train Loss: 3.3938, Train Perplexity: 29.7785\n",
            "Epoch [1/3], Batch [8266/21340], Train Loss: 3.1861, Train Perplexity: 24.1940\n",
            "Epoch [1/3], Batch [8267/21340], Train Loss: 3.2453, Train Perplexity: 25.6692\n",
            "Epoch [1/3], Batch [8268/21340], Train Loss: 3.2194, Train Perplexity: 25.0140\n",
            "Epoch [1/3], Batch [8269/21340], Train Loss: 3.3075, Train Perplexity: 27.3164\n",
            "Epoch [1/3], Batch [8270/21340], Train Loss: 3.3684, Train Perplexity: 29.0310\n",
            "Epoch [1/3], Batch [8271/21340], Train Loss: 3.2432, Train Perplexity: 25.6163\n",
            "Epoch [1/3], Batch [8272/21340], Train Loss: 3.2850, Train Perplexity: 26.7097\n",
            "Epoch [1/3], Batch [8273/21340], Train Loss: 3.3911, Train Perplexity: 29.6980\n",
            "Epoch [1/3], Batch [8274/21340], Train Loss: 3.2690, Train Perplexity: 26.2846\n",
            "Epoch [1/3], Batch [8275/21340], Train Loss: 3.2339, Train Perplexity: 25.3776\n",
            "Epoch [1/3], Batch [8276/21340], Train Loss: 3.2352, Train Perplexity: 25.4110\n",
            "Epoch [1/3], Batch [8277/21340], Train Loss: 3.2715, Train Perplexity: 26.3519\n",
            "Epoch [1/3], Batch [8278/21340], Train Loss: 3.4431, Train Perplexity: 31.2832\n",
            "Epoch [1/3], Batch [8279/21340], Train Loss: 3.2544, Train Perplexity: 25.9048\n",
            "Epoch [1/3], Batch [8280/21340], Train Loss: 3.3643, Train Perplexity: 28.9141\n",
            "Epoch [1/3], Batch [8281/21340], Train Loss: 3.3350, Train Perplexity: 28.0789\n",
            "Epoch [1/3], Batch [8282/21340], Train Loss: 3.2991, Train Perplexity: 27.0888\n",
            "Epoch [1/3], Batch [8283/21340], Train Loss: 3.2232, Train Perplexity: 25.1094\n",
            "Epoch [1/3], Batch [8284/21340], Train Loss: 3.1696, Train Perplexity: 23.7986\n",
            "Epoch [1/3], Batch [8285/21340], Train Loss: 3.2327, Train Perplexity: 25.3483\n",
            "Epoch [1/3], Batch [8286/21340], Train Loss: 3.2955, Train Perplexity: 26.9910\n",
            "Epoch [1/3], Batch [8287/21340], Train Loss: 3.1649, Train Perplexity: 23.6862\n",
            "Epoch [1/3], Batch [8288/21340], Train Loss: 3.6054, Train Perplexity: 36.7967\n",
            "Epoch [1/3], Batch [8289/21340], Train Loss: 3.1404, Train Perplexity: 23.1139\n",
            "Epoch [1/3], Batch [8290/21340], Train Loss: 3.1758, Train Perplexity: 23.9455\n",
            "Epoch [1/3], Batch [8291/21340], Train Loss: 3.3096, Train Perplexity: 27.3751\n",
            "Epoch [1/3], Batch [8292/21340], Train Loss: 3.1941, Train Perplexity: 24.3875\n",
            "Epoch [1/3], Batch [8293/21340], Train Loss: 3.3110, Train Perplexity: 27.4129\n",
            "Epoch [1/3], Batch [8294/21340], Train Loss: 3.2005, Train Perplexity: 24.5436\n",
            "Epoch [1/3], Batch [8295/21340], Train Loss: 3.1427, Train Perplexity: 23.1675\n",
            "Epoch [1/3], Batch [8296/21340], Train Loss: 3.2299, Train Perplexity: 25.2773\n",
            "Epoch [1/3], Batch [8297/21340], Train Loss: 3.2049, Train Perplexity: 24.6541\n",
            "Epoch [1/3], Batch [8298/21340], Train Loss: 3.2949, Train Perplexity: 26.9750\n",
            "Epoch [1/3], Batch [8299/21340], Train Loss: 3.2256, Train Perplexity: 25.1677\n",
            "Epoch [1/3], Batch [8300/21340], Train Loss: 3.1546, Train Perplexity: 23.4438\n",
            "Epoch [1/3], Batch [8301/21340], Train Loss: 3.2108, Train Perplexity: 24.7986\n",
            "Epoch [1/3], Batch [8302/21340], Train Loss: 3.4200, Train Perplexity: 30.5694\n",
            "Epoch [1/3], Batch [8303/21340], Train Loss: 3.1900, Train Perplexity: 24.2873\n",
            "Epoch [1/3], Batch [8304/21340], Train Loss: 3.3432, Train Perplexity: 28.3109\n",
            "Epoch [1/3], Batch [8305/21340], Train Loss: 3.2620, Train Perplexity: 26.1007\n",
            "Epoch [1/3], Batch [8306/21340], Train Loss: 3.2724, Train Perplexity: 26.3748\n",
            "Epoch [1/3], Batch [8307/21340], Train Loss: 3.2780, Train Perplexity: 26.5235\n",
            "Epoch [1/3], Batch [8308/21340], Train Loss: 3.1794, Train Perplexity: 24.0333\n",
            "Epoch [1/3], Batch [8309/21340], Train Loss: 3.1935, Train Perplexity: 24.3730\n",
            "Epoch [1/3], Batch [8310/21340], Train Loss: 3.1669, Train Perplexity: 23.7338\n",
            "Epoch [1/3], Batch [8311/21340], Train Loss: 3.3413, Train Perplexity: 28.2552\n",
            "Epoch [1/3], Batch [8312/21340], Train Loss: 3.2310, Train Perplexity: 25.3040\n",
            "Epoch [1/3], Batch [8313/21340], Train Loss: 3.1776, Train Perplexity: 23.9889\n",
            "Epoch [1/3], Batch [8314/21340], Train Loss: 3.3322, Train Perplexity: 27.9996\n",
            "Epoch [1/3], Batch [8315/21340], Train Loss: 3.1808, Train Perplexity: 24.0665\n",
            "Epoch [1/3], Batch [8316/21340], Train Loss: 3.2680, Train Perplexity: 26.2580\n",
            "Epoch [1/3], Batch [8317/21340], Train Loss: 3.1982, Train Perplexity: 24.4877\n",
            "Epoch [1/3], Batch [8318/21340], Train Loss: 3.2599, Train Perplexity: 26.0474\n",
            "Epoch [1/3], Batch [8319/21340], Train Loss: 3.4105, Train Perplexity: 30.2805\n",
            "Epoch [1/3], Batch [8320/21340], Train Loss: 3.1651, Train Perplexity: 23.6908\n",
            "Epoch [1/3], Batch [8321/21340], Train Loss: 3.2634, Train Perplexity: 26.1383\n",
            "Epoch [1/3], Batch [8322/21340], Train Loss: 3.3225, Train Perplexity: 27.7309\n",
            "Epoch [1/3], Batch [8323/21340], Train Loss: 3.3230, Train Perplexity: 27.7436\n",
            "Epoch [1/3], Batch [8324/21340], Train Loss: 3.1863, Train Perplexity: 24.1979\n",
            "Epoch [1/3], Batch [8325/21340], Train Loss: 3.3233, Train Perplexity: 27.7531\n",
            "Epoch [1/3], Batch [8326/21340], Train Loss: 3.2160, Train Perplexity: 24.9280\n",
            "Epoch [1/3], Batch [8327/21340], Train Loss: 3.3529, Train Perplexity: 28.5864\n",
            "Epoch [1/3], Batch [8328/21340], Train Loss: 3.3486, Train Perplexity: 28.4636\n",
            "Epoch [1/3], Batch [8329/21340], Train Loss: 3.3049, Train Perplexity: 27.2462\n",
            "Epoch [1/3], Batch [8330/21340], Train Loss: 3.2114, Train Perplexity: 24.8138\n",
            "Epoch [1/3], Batch [8331/21340], Train Loss: 3.3155, Train Perplexity: 27.5359\n",
            "Epoch [1/3], Batch [8332/21340], Train Loss: 3.2158, Train Perplexity: 24.9232\n",
            "Epoch [1/3], Batch [8333/21340], Train Loss: 3.2649, Train Perplexity: 26.1771\n",
            "Epoch [1/3], Batch [8334/21340], Train Loss: 3.2564, Train Perplexity: 25.9564\n",
            "Epoch [1/3], Batch [8335/21340], Train Loss: 3.2329, Train Perplexity: 25.3525\n",
            "Epoch [1/3], Batch [8336/21340], Train Loss: 3.3203, Train Perplexity: 27.6685\n",
            "Epoch [1/3], Batch [8337/21340], Train Loss: 3.1889, Train Perplexity: 24.2619\n",
            "Epoch [1/3], Batch [8338/21340], Train Loss: 3.2716, Train Perplexity: 26.3548\n",
            "Epoch [1/3], Batch [8339/21340], Train Loss: 3.3588, Train Perplexity: 28.7546\n",
            "Epoch [1/3], Batch [8340/21340], Train Loss: 3.2514, Train Perplexity: 25.8253\n",
            "Epoch [1/3], Batch [8341/21340], Train Loss: 3.2128, Train Perplexity: 24.8491\n",
            "Epoch [1/3], Batch [8342/21340], Train Loss: 3.2229, Train Perplexity: 25.1011\n",
            "Epoch [1/3], Batch [8343/21340], Train Loss: 3.2720, Train Perplexity: 26.3636\n",
            "Epoch [1/3], Batch [8344/21340], Train Loss: 3.2231, Train Perplexity: 25.1054\n",
            "Epoch [1/3], Batch [8345/21340], Train Loss: 3.2552, Train Perplexity: 25.9248\n",
            "Epoch [1/3], Batch [8346/21340], Train Loss: 3.3851, Train Perplexity: 29.5223\n",
            "Epoch [1/3], Batch [8347/21340], Train Loss: 3.3423, Train Perplexity: 28.2842\n",
            "Epoch [1/3], Batch [8348/21340], Train Loss: 3.1495, Train Perplexity: 23.3241\n",
            "Epoch [1/3], Batch [8349/21340], Train Loss: 3.3177, Train Perplexity: 27.5980\n",
            "Epoch [1/3], Batch [8350/21340], Train Loss: 3.2312, Train Perplexity: 25.3110\n",
            "Epoch [1/3], Batch [8351/21340], Train Loss: 3.2223, Train Perplexity: 25.0847\n",
            "Epoch [1/3], Batch [8352/21340], Train Loss: 3.3061, Train Perplexity: 27.2772\n",
            "Epoch [1/3], Batch [8353/21340], Train Loss: 3.3523, Train Perplexity: 28.5674\n",
            "Epoch [1/3], Batch [8354/21340], Train Loss: 3.2474, Train Perplexity: 25.7236\n",
            "Epoch [1/3], Batch [8355/21340], Train Loss: 3.1934, Train Perplexity: 24.3704\n",
            "Epoch [1/3], Batch [8356/21340], Train Loss: 3.2733, Train Perplexity: 26.3991\n",
            "Epoch [1/3], Batch [8357/21340], Train Loss: 3.2020, Train Perplexity: 24.5816\n",
            "Epoch [1/3], Batch [8358/21340], Train Loss: 3.2947, Train Perplexity: 26.9681\n",
            "Epoch [1/3], Batch [8359/21340], Train Loss: 3.1775, Train Perplexity: 23.9868\n",
            "Epoch [1/3], Batch [8360/21340], Train Loss: 3.2161, Train Perplexity: 24.9309\n",
            "Epoch [1/3], Batch [8361/21340], Train Loss: 3.1348, Train Perplexity: 22.9836\n",
            "Epoch [1/3], Batch [8362/21340], Train Loss: 3.1390, Train Perplexity: 23.0797\n",
            "Epoch [1/3], Batch [8363/21340], Train Loss: 3.1219, Train Perplexity: 22.6897\n",
            "Epoch [1/3], Batch [8364/21340], Train Loss: 3.2253, Train Perplexity: 25.1616\n",
            "Epoch [1/3], Batch [8365/21340], Train Loss: 3.1740, Train Perplexity: 23.9034\n",
            "Epoch [1/3], Batch [8366/21340], Train Loss: 3.2882, Train Perplexity: 26.7936\n",
            "Epoch [1/3], Batch [8367/21340], Train Loss: 3.2913, Train Perplexity: 26.8773\n",
            "Epoch [1/3], Batch [8368/21340], Train Loss: 3.3025, Train Perplexity: 27.1802\n",
            "Epoch [1/3], Batch [8369/21340], Train Loss: 3.2999, Train Perplexity: 27.1097\n",
            "Epoch [1/3], Batch [8370/21340], Train Loss: 3.2771, Train Perplexity: 26.4984\n",
            "Epoch [1/3], Batch [8371/21340], Train Loss: 3.3390, Train Perplexity: 28.1917\n",
            "Epoch [1/3], Batch [8372/21340], Train Loss: 3.2602, Train Perplexity: 26.0550\n",
            "Epoch [1/3], Batch [8373/21340], Train Loss: 3.2854, Train Perplexity: 26.7186\n",
            "Epoch [1/3], Batch [8374/21340], Train Loss: 3.2269, Train Perplexity: 25.2023\n",
            "Epoch [1/3], Batch [8375/21340], Train Loss: 3.1753, Train Perplexity: 23.9336\n",
            "Epoch [1/3], Batch [8376/21340], Train Loss: 3.2809, Train Perplexity: 26.6003\n",
            "Epoch [1/3], Batch [8377/21340], Train Loss: 3.2094, Train Perplexity: 24.7640\n",
            "Epoch [1/3], Batch [8378/21340], Train Loss: 3.3034, Train Perplexity: 27.2046\n",
            "Epoch [1/3], Batch [8379/21340], Train Loss: 3.2162, Train Perplexity: 24.9340\n",
            "Epoch [1/3], Batch [8380/21340], Train Loss: 3.2567, Train Perplexity: 25.9631\n",
            "Epoch [1/3], Batch [8381/21340], Train Loss: 3.4457, Train Perplexity: 31.3647\n",
            "Epoch [1/3], Batch [8382/21340], Train Loss: 3.4100, Train Perplexity: 30.2649\n",
            "Epoch [1/3], Batch [8383/21340], Train Loss: 3.2835, Train Perplexity: 26.6683\n",
            "Epoch [1/3], Batch [8384/21340], Train Loss: 3.2801, Train Perplexity: 26.5778\n",
            "Epoch [1/3], Batch [8385/21340], Train Loss: 3.2729, Train Perplexity: 26.3883\n",
            "Epoch [1/3], Batch [8386/21340], Train Loss: 3.2247, Train Perplexity: 25.1458\n",
            "Epoch [1/3], Batch [8387/21340], Train Loss: 3.2814, Train Perplexity: 26.6117\n",
            "Epoch [1/3], Batch [8388/21340], Train Loss: 3.2932, Train Perplexity: 26.9278\n",
            "Epoch [1/3], Batch [8389/21340], Train Loss: 3.4538, Train Perplexity: 31.6206\n",
            "Epoch [1/3], Batch [8390/21340], Train Loss: 3.2356, Train Perplexity: 25.4215\n",
            "Epoch [1/3], Batch [8391/21340], Train Loss: 3.3134, Train Perplexity: 27.4787\n",
            "Epoch [1/3], Batch [8392/21340], Train Loss: 3.3038, Train Perplexity: 27.2147\n",
            "Epoch [1/3], Batch [8393/21340], Train Loss: 3.3154, Train Perplexity: 27.5328\n",
            "Epoch [1/3], Batch [8394/21340], Train Loss: 3.1979, Train Perplexity: 24.4803\n",
            "Epoch [1/3], Batch [8395/21340], Train Loss: 3.1689, Train Perplexity: 23.7819\n",
            "Epoch [1/3], Batch [8396/21340], Train Loss: 3.2635, Train Perplexity: 26.1415\n",
            "Epoch [1/3], Batch [8397/21340], Train Loss: 3.2418, Train Perplexity: 25.5798\n",
            "Epoch [1/3], Batch [8398/21340], Train Loss: 3.2282, Train Perplexity: 25.2351\n",
            "Epoch [1/3], Batch [8399/21340], Train Loss: 3.1851, Train Perplexity: 24.1693\n",
            "Epoch [1/3], Batch [8400/21340], Train Loss: 3.3484, Train Perplexity: 28.4567\n",
            "Epoch [1/3], Batch [8401/21340], Train Loss: 3.2723, Train Perplexity: 26.3714\n",
            "Epoch [1/3], Batch [8402/21340], Train Loss: 3.1936, Train Perplexity: 24.3751\n",
            "Epoch [1/3], Batch [8403/21340], Train Loss: 3.3723, Train Perplexity: 29.1468\n",
            "Epoch [1/3], Batch [8404/21340], Train Loss: 3.3090, Train Perplexity: 27.3585\n",
            "Epoch [1/3], Batch [8405/21340], Train Loss: 3.1929, Train Perplexity: 24.3579\n",
            "Epoch [1/3], Batch [8406/21340], Train Loss: 3.1772, Train Perplexity: 23.9794\n",
            "Epoch [1/3], Batch [8407/21340], Train Loss: 3.4203, Train Perplexity: 30.5771\n",
            "Epoch [1/3], Batch [8408/21340], Train Loss: 3.3126, Train Perplexity: 27.4553\n",
            "Epoch [1/3], Batch [8409/21340], Train Loss: 3.2310, Train Perplexity: 25.3039\n",
            "Epoch [1/3], Batch [8410/21340], Train Loss: 3.1311, Train Perplexity: 22.8981\n",
            "Epoch [1/3], Batch [8411/21340], Train Loss: 3.2179, Train Perplexity: 24.9766\n",
            "Epoch [1/3], Batch [8412/21340], Train Loss: 3.3453, Train Perplexity: 28.3689\n",
            "Epoch [1/3], Batch [8413/21340], Train Loss: 3.1643, Train Perplexity: 23.6714\n",
            "Epoch [1/3], Batch [8414/21340], Train Loss: 3.2414, Train Perplexity: 25.5693\n",
            "Epoch [1/3], Batch [8415/21340], Train Loss: 3.1612, Train Perplexity: 23.5984\n",
            "Epoch [1/3], Batch [8416/21340], Train Loss: 3.1963, Train Perplexity: 24.4424\n",
            "Epoch [1/3], Batch [8417/21340], Train Loss: 3.4000, Train Perplexity: 29.9643\n",
            "Epoch [1/3], Batch [8418/21340], Train Loss: 3.2854, Train Perplexity: 26.7210\n",
            "Epoch [1/3], Batch [8419/21340], Train Loss: 3.2222, Train Perplexity: 25.0842\n",
            "Epoch [1/3], Batch [8420/21340], Train Loss: 3.2881, Train Perplexity: 26.7912\n",
            "Epoch [1/3], Batch [8421/21340], Train Loss: 3.1748, Train Perplexity: 23.9218\n",
            "Epoch [1/3], Batch [8422/21340], Train Loss: 3.2859, Train Perplexity: 26.7339\n",
            "Epoch [1/3], Batch [8423/21340], Train Loss: 3.2954, Train Perplexity: 26.9875\n",
            "Epoch [1/3], Batch [8424/21340], Train Loss: 3.2567, Train Perplexity: 25.9647\n",
            "Epoch [1/3], Batch [8425/21340], Train Loss: 3.2026, Train Perplexity: 24.5959\n",
            "Epoch [1/3], Batch [8426/21340], Train Loss: 3.1372, Train Perplexity: 23.0390\n",
            "Epoch [1/3], Batch [8427/21340], Train Loss: 3.2064, Train Perplexity: 24.6907\n",
            "Epoch [1/3], Batch [8428/21340], Train Loss: 3.2005, Train Perplexity: 24.5453\n",
            "Epoch [1/3], Batch [8429/21340], Train Loss: 3.4358, Train Perplexity: 31.0572\n",
            "Epoch [1/3], Batch [8430/21340], Train Loss: 3.2196, Train Perplexity: 25.0169\n",
            "Epoch [1/3], Batch [8431/21340], Train Loss: 3.5454, Train Perplexity: 34.6553\n",
            "Epoch [1/3], Batch [8432/21340], Train Loss: 3.3077, Train Perplexity: 27.3226\n",
            "Epoch [1/3], Batch [8433/21340], Train Loss: 3.4863, Train Perplexity: 32.6639\n",
            "Epoch [1/3], Batch [8434/21340], Train Loss: 3.2246, Train Perplexity: 25.1443\n",
            "Epoch [1/3], Batch [8435/21340], Train Loss: 3.1812, Train Perplexity: 24.0752\n",
            "Epoch [1/3], Batch [8436/21340], Train Loss: 3.2690, Train Perplexity: 26.2857\n",
            "Epoch [1/3], Batch [8437/21340], Train Loss: 3.1230, Train Perplexity: 22.7146\n",
            "Epoch [1/3], Batch [8438/21340], Train Loss: 3.2115, Train Perplexity: 24.8171\n",
            "Epoch [1/3], Batch [8439/21340], Train Loss: 3.1720, Train Perplexity: 23.8562\n",
            "Epoch [1/3], Batch [8440/21340], Train Loss: 3.3919, Train Perplexity: 29.7223\n",
            "Epoch [1/3], Batch [8441/21340], Train Loss: 3.1749, Train Perplexity: 23.9255\n",
            "Epoch [1/3], Batch [8442/21340], Train Loss: 3.2918, Train Perplexity: 26.8908\n",
            "Epoch [1/3], Batch [8443/21340], Train Loss: 3.4008, Train Perplexity: 29.9885\n",
            "Epoch [1/3], Batch [8444/21340], Train Loss: 3.2288, Train Perplexity: 25.2482\n",
            "Epoch [1/3], Batch [8445/21340], Train Loss: 3.2284, Train Perplexity: 25.2404\n",
            "Epoch [1/3], Batch [8446/21340], Train Loss: 3.1878, Train Perplexity: 24.2352\n",
            "Epoch [1/3], Batch [8447/21340], Train Loss: 3.4129, Train Perplexity: 30.3523\n",
            "Epoch [1/3], Batch [8448/21340], Train Loss: 3.3341, Train Perplexity: 28.0527\n",
            "Epoch [1/3], Batch [8449/21340], Train Loss: 3.2073, Train Perplexity: 24.7133\n",
            "Epoch [1/3], Batch [8450/21340], Train Loss: 3.0958, Train Perplexity: 22.1047\n",
            "Epoch [1/3], Batch [8451/21340], Train Loss: 3.3015, Train Perplexity: 27.1544\n",
            "Epoch [1/3], Batch [8452/21340], Train Loss: 3.2215, Train Perplexity: 25.0657\n",
            "Epoch [1/3], Batch [8453/21340], Train Loss: 3.1854, Train Perplexity: 24.1775\n",
            "Epoch [1/3], Batch [8454/21340], Train Loss: 3.2870, Train Perplexity: 26.7630\n",
            "Epoch [1/3], Batch [8455/21340], Train Loss: 3.2345, Train Perplexity: 25.3935\n",
            "Epoch [1/3], Batch [8456/21340], Train Loss: 3.2533, Train Perplexity: 25.8759\n",
            "Epoch [1/3], Batch [8457/21340], Train Loss: 3.2704, Train Perplexity: 26.3229\n",
            "Epoch [1/3], Batch [8458/21340], Train Loss: 3.4791, Train Perplexity: 32.4309\n",
            "Epoch [1/3], Batch [8459/21340], Train Loss: 3.2951, Train Perplexity: 26.9797\n",
            "Epoch [1/3], Batch [8460/21340], Train Loss: 3.2601, Train Perplexity: 26.0522\n",
            "Epoch [1/3], Batch [8461/21340], Train Loss: 3.2888, Train Perplexity: 26.8105\n",
            "Epoch [1/3], Batch [8462/21340], Train Loss: 3.2429, Train Perplexity: 25.6083\n",
            "Epoch [1/3], Batch [8463/21340], Train Loss: 3.2225, Train Perplexity: 25.0899\n",
            "Epoch [1/3], Batch [8464/21340], Train Loss: 3.2404, Train Perplexity: 25.5433\n",
            "Epoch [1/3], Batch [8465/21340], Train Loss: 3.2810, Train Perplexity: 26.6029\n",
            "Epoch [1/3], Batch [8466/21340], Train Loss: 3.2929, Train Perplexity: 26.9196\n",
            "Epoch [1/3], Batch [8467/21340], Train Loss: 3.2499, Train Perplexity: 25.7876\n",
            "Epoch [1/3], Batch [8468/21340], Train Loss: 3.1628, Train Perplexity: 23.6376\n",
            "Epoch [1/3], Batch [8469/21340], Train Loss: 3.1744, Train Perplexity: 23.9130\n",
            "Epoch [1/3], Batch [8470/21340], Train Loss: 3.2677, Train Perplexity: 26.2520\n",
            "Epoch [1/3], Batch [8471/21340], Train Loss: 3.2563, Train Perplexity: 25.9545\n",
            "Epoch [1/3], Batch [8472/21340], Train Loss: 3.1884, Train Perplexity: 24.2485\n",
            "Epoch [1/3], Batch [8473/21340], Train Loss: 3.2635, Train Perplexity: 26.1412\n",
            "Epoch [1/3], Batch [8474/21340], Train Loss: 3.2338, Train Perplexity: 25.3750\n",
            "Epoch [1/3], Batch [8475/21340], Train Loss: 3.2556, Train Perplexity: 25.9357\n",
            "Epoch [1/3], Batch [8476/21340], Train Loss: 3.1963, Train Perplexity: 24.4417\n",
            "Epoch [1/3], Batch [8477/21340], Train Loss: 3.2290, Train Perplexity: 25.2549\n",
            "Epoch [1/3], Batch [8478/21340], Train Loss: 3.2275, Train Perplexity: 25.2166\n",
            "Epoch [1/3], Batch [8479/21340], Train Loss: 3.2063, Train Perplexity: 24.6878\n",
            "Epoch [1/3], Batch [8480/21340], Train Loss: 3.1805, Train Perplexity: 24.0584\n",
            "Epoch [1/3], Batch [8481/21340], Train Loss: 3.2492, Train Perplexity: 25.7702\n",
            "Epoch [1/3], Batch [8482/21340], Train Loss: 3.1951, Train Perplexity: 24.4114\n",
            "Epoch [1/3], Batch [8483/21340], Train Loss: 3.2169, Train Perplexity: 24.9497\n",
            "Epoch [1/3], Batch [8484/21340], Train Loss: 3.2920, Train Perplexity: 26.8973\n",
            "Epoch [1/3], Batch [8485/21340], Train Loss: 3.2711, Train Perplexity: 26.3390\n",
            "Epoch [1/3], Batch [8486/21340], Train Loss: 3.2513, Train Perplexity: 25.8241\n",
            "Epoch [1/3], Batch [8487/21340], Train Loss: 3.1978, Train Perplexity: 24.4780\n",
            "Epoch [1/3], Batch [8488/21340], Train Loss: 3.2870, Train Perplexity: 26.7615\n",
            "Epoch [1/3], Batch [8489/21340], Train Loss: 3.1968, Train Perplexity: 24.4553\n",
            "Epoch [1/3], Batch [8490/21340], Train Loss: 3.2388, Train Perplexity: 25.5027\n",
            "Epoch [1/3], Batch [8491/21340], Train Loss: 3.2724, Train Perplexity: 26.3734\n",
            "Epoch [1/3], Batch [8492/21340], Train Loss: 3.2470, Train Perplexity: 25.7140\n",
            "Epoch [1/3], Batch [8493/21340], Train Loss: 3.4310, Train Perplexity: 30.9086\n",
            "Epoch [1/3], Batch [8494/21340], Train Loss: 3.2176, Train Perplexity: 24.9675\n",
            "Epoch [1/3], Batch [8495/21340], Train Loss: 3.2694, Train Perplexity: 26.2954\n",
            "Epoch [1/3], Batch [8496/21340], Train Loss: 3.1816, Train Perplexity: 24.0860\n",
            "Epoch [1/3], Batch [8497/21340], Train Loss: 3.1708, Train Perplexity: 23.8260\n",
            "Epoch [1/3], Batch [8498/21340], Train Loss: 3.1867, Train Perplexity: 24.2083\n",
            "Epoch [1/3], Batch [8499/21340], Train Loss: 3.3096, Train Perplexity: 27.3733\n",
            "Epoch [1/3], Batch [8500/21340], Train Loss: 3.2434, Train Perplexity: 25.6213\n",
            "Epoch [1/3], Batch [8501/21340], Train Loss: 3.3284, Train Perplexity: 27.8943\n",
            "Epoch [1/3], Batch [8502/21340], Train Loss: 3.2808, Train Perplexity: 26.5974\n",
            "Epoch [1/3], Batch [8503/21340], Train Loss: 3.2291, Train Perplexity: 25.2569\n",
            "Epoch [1/3], Batch [8504/21340], Train Loss: 3.3113, Train Perplexity: 27.4217\n",
            "Epoch [1/3], Batch [8505/21340], Train Loss: 3.2310, Train Perplexity: 25.3042\n",
            "Epoch [1/3], Batch [8506/21340], Train Loss: 3.2475, Train Perplexity: 25.7250\n",
            "Epoch [1/3], Batch [8507/21340], Train Loss: 3.1925, Train Perplexity: 24.3485\n",
            "Epoch [1/3], Batch [8508/21340], Train Loss: 3.2916, Train Perplexity: 26.8868\n",
            "Epoch [1/3], Batch [8509/21340], Train Loss: 3.2942, Train Perplexity: 26.9566\n",
            "Epoch [1/3], Batch [8510/21340], Train Loss: 3.2335, Train Perplexity: 25.3683\n",
            "Epoch [1/3], Batch [8511/21340], Train Loss: 3.2654, Train Perplexity: 26.1912\n",
            "Epoch [1/3], Batch [8512/21340], Train Loss: 3.2229, Train Perplexity: 25.0997\n",
            "Epoch [1/3], Batch [8513/21340], Train Loss: 3.3621, Train Perplexity: 28.8490\n",
            "Epoch [1/3], Batch [8514/21340], Train Loss: 3.2550, Train Perplexity: 25.9205\n",
            "Epoch [1/3], Batch [8515/21340], Train Loss: 3.1637, Train Perplexity: 23.6580\n",
            "Epoch [1/3], Batch [8516/21340], Train Loss: 3.2011, Train Perplexity: 24.5596\n",
            "Epoch [1/3], Batch [8517/21340], Train Loss: 3.2788, Train Perplexity: 26.5434\n",
            "Epoch [1/3], Batch [8518/21340], Train Loss: 3.2150, Train Perplexity: 24.9031\n",
            "Epoch [1/3], Batch [8519/21340], Train Loss: 3.1877, Train Perplexity: 24.2338\n",
            "Epoch [1/3], Batch [8520/21340], Train Loss: 3.2489, Train Perplexity: 25.7630\n",
            "Epoch [1/3], Batch [8521/21340], Train Loss: 3.2194, Train Perplexity: 25.0132\n",
            "Epoch [1/3], Batch [8522/21340], Train Loss: 3.3114, Train Perplexity: 27.4238\n",
            "Epoch [1/3], Batch [8523/21340], Train Loss: 3.2090, Train Perplexity: 24.7549\n",
            "Epoch [1/3], Batch [8524/21340], Train Loss: 3.1468, Train Perplexity: 23.2609\n",
            "Epoch [1/3], Batch [8525/21340], Train Loss: 3.3660, Train Perplexity: 28.9626\n",
            "Epoch [1/3], Batch [8526/21340], Train Loss: 3.2279, Train Perplexity: 25.2279\n",
            "Epoch [1/3], Batch [8527/21340], Train Loss: 3.2957, Train Perplexity: 26.9971\n",
            "Epoch [1/3], Batch [8528/21340], Train Loss: 3.2217, Train Perplexity: 25.0699\n",
            "Epoch [1/3], Batch [8529/21340], Train Loss: 3.2585, Train Perplexity: 26.0106\n",
            "Epoch [1/3], Batch [8530/21340], Train Loss: 3.1970, Train Perplexity: 24.4590\n",
            "Epoch [1/3], Batch [8531/21340], Train Loss: 3.2443, Train Perplexity: 25.6444\n",
            "Epoch [1/3], Batch [8532/21340], Train Loss: 3.1997, Train Perplexity: 24.5245\n",
            "Epoch [1/3], Batch [8533/21340], Train Loss: 3.3077, Train Perplexity: 27.3211\n",
            "Epoch [1/3], Batch [8534/21340], Train Loss: 3.1816, Train Perplexity: 24.0843\n",
            "Epoch [1/3], Batch [8535/21340], Train Loss: 3.3309, Train Perplexity: 27.9649\n",
            "Epoch [1/3], Batch [8536/21340], Train Loss: 3.2326, Train Perplexity: 25.3466\n",
            "Epoch [1/3], Batch [8537/21340], Train Loss: 3.1555, Train Perplexity: 23.4639\n",
            "Epoch [1/3], Batch [8538/21340], Train Loss: 3.2879, Train Perplexity: 26.7858\n",
            "Epoch [1/3], Batch [8539/21340], Train Loss: 3.2908, Train Perplexity: 26.8652\n",
            "Epoch [1/3], Batch [8540/21340], Train Loss: 3.1700, Train Perplexity: 23.8075\n",
            "Epoch [1/3], Batch [8541/21340], Train Loss: 3.3181, Train Perplexity: 27.6087\n",
            "Epoch [1/3], Batch [8542/21340], Train Loss: 3.2596, Train Perplexity: 26.0383\n",
            "Epoch [1/3], Batch [8543/21340], Train Loss: 3.3051, Train Perplexity: 27.2504\n",
            "Epoch [1/3], Batch [8544/21340], Train Loss: 3.2282, Train Perplexity: 25.2330\n",
            "Epoch [1/3], Batch [8545/21340], Train Loss: 3.2996, Train Perplexity: 27.1023\n",
            "Epoch [1/3], Batch [8546/21340], Train Loss: 3.1724, Train Perplexity: 23.8640\n",
            "Epoch [1/3], Batch [8547/21340], Train Loss: 3.1693, Train Perplexity: 23.7897\n",
            "Epoch [1/3], Batch [8548/21340], Train Loss: 3.2404, Train Perplexity: 25.5434\n",
            "Epoch [1/3], Batch [8549/21340], Train Loss: 3.1845, Train Perplexity: 24.1556\n",
            "Epoch [1/3], Batch [8550/21340], Train Loss: 3.2388, Train Perplexity: 25.5027\n",
            "Epoch [1/3], Batch [8551/21340], Train Loss: 3.1770, Train Perplexity: 23.9741\n",
            "Epoch [1/3], Batch [8552/21340], Train Loss: 3.2874, Train Perplexity: 26.7734\n",
            "Epoch [1/3], Batch [8553/21340], Train Loss: 3.2725, Train Perplexity: 26.3769\n",
            "Epoch [1/3], Batch [8554/21340], Train Loss: 3.3358, Train Perplexity: 28.1022\n",
            "Epoch [1/3], Batch [8555/21340], Train Loss: 3.3411, Train Perplexity: 28.2507\n",
            "Epoch [1/3], Batch [8556/21340], Train Loss: 3.2082, Train Perplexity: 24.7343\n",
            "Epoch [1/3], Batch [8557/21340], Train Loss: 3.2901, Train Perplexity: 26.8463\n",
            "Epoch [1/3], Batch [8558/21340], Train Loss: 3.2838, Train Perplexity: 26.6772\n",
            "Epoch [1/3], Batch [8559/21340], Train Loss: 3.2767, Train Perplexity: 26.4876\n",
            "Epoch [1/3], Batch [8560/21340], Train Loss: 3.3088, Train Perplexity: 27.3511\n",
            "Epoch [1/3], Batch [8561/21340], Train Loss: 3.3062, Train Perplexity: 27.2804\n",
            "Epoch [1/3], Batch [8562/21340], Train Loss: 3.3091, Train Perplexity: 27.3599\n",
            "Epoch [1/3], Batch [8563/21340], Train Loss: 3.2483, Train Perplexity: 25.7459\n",
            "Epoch [1/3], Batch [8564/21340], Train Loss: 3.1465, Train Perplexity: 23.2538\n",
            "Epoch [1/3], Batch [8565/21340], Train Loss: 3.1791, Train Perplexity: 24.0247\n",
            "Epoch [1/3], Batch [8566/21340], Train Loss: 3.2942, Train Perplexity: 26.9568\n",
            "Epoch [1/3], Batch [8567/21340], Train Loss: 3.2571, Train Perplexity: 25.9749\n",
            "Epoch [1/3], Batch [8568/21340], Train Loss: 3.2990, Train Perplexity: 27.0844\n",
            "Epoch [1/3], Batch [8569/21340], Train Loss: 3.2528, Train Perplexity: 25.8627\n",
            "Epoch [1/3], Batch [8570/21340], Train Loss: 3.5200, Train Perplexity: 33.7829\n",
            "Epoch [1/3], Batch [8571/21340], Train Loss: 3.2799, Train Perplexity: 26.5743\n",
            "Epoch [1/3], Batch [8572/21340], Train Loss: 3.2138, Train Perplexity: 24.8730\n",
            "Epoch [1/3], Batch [8573/21340], Train Loss: 3.2419, Train Perplexity: 25.5811\n",
            "Epoch [1/3], Batch [8574/21340], Train Loss: 3.3831, Train Perplexity: 29.4618\n",
            "Epoch [1/3], Batch [8575/21340], Train Loss: 3.2920, Train Perplexity: 26.8971\n",
            "Epoch [1/3], Batch [8576/21340], Train Loss: 3.1582, Train Perplexity: 23.5293\n",
            "Epoch [1/3], Batch [8577/21340], Train Loss: 3.1763, Train Perplexity: 23.9575\n",
            "Epoch [1/3], Batch [8578/21340], Train Loss: 3.3132, Train Perplexity: 27.4741\n",
            "Epoch [1/3], Batch [8579/21340], Train Loss: 3.3318, Train Perplexity: 27.9883\n",
            "Epoch [1/3], Batch [8580/21340], Train Loss: 3.3207, Train Perplexity: 27.6807\n",
            "Epoch [1/3], Batch [8581/21340], Train Loss: 3.1650, Train Perplexity: 23.6887\n",
            "Epoch [1/3], Batch [8582/21340], Train Loss: 3.2128, Train Perplexity: 24.8486\n",
            "Epoch [1/3], Batch [8583/21340], Train Loss: 3.2666, Train Perplexity: 26.2230\n",
            "Epoch [1/3], Batch [8584/21340], Train Loss: 3.2896, Train Perplexity: 26.8327\n",
            "Epoch [1/3], Batch [8585/21340], Train Loss: 3.2394, Train Perplexity: 25.5185\n",
            "Epoch [1/3], Batch [8586/21340], Train Loss: 3.2736, Train Perplexity: 26.4064\n",
            "Epoch [1/3], Batch [8587/21340], Train Loss: 3.2559, Train Perplexity: 25.9428\n",
            "Epoch [1/3], Batch [8588/21340], Train Loss: 3.1808, Train Perplexity: 24.0663\n",
            "Epoch [1/3], Batch [8589/21340], Train Loss: 3.2522, Train Perplexity: 25.8459\n",
            "Epoch [1/3], Batch [8590/21340], Train Loss: 3.2655, Train Perplexity: 26.1939\n",
            "Epoch [1/3], Batch [8591/21340], Train Loss: 3.2795, Train Perplexity: 26.5617\n",
            "Epoch [1/3], Batch [8592/21340], Train Loss: 3.2796, Train Perplexity: 26.5651\n",
            "Epoch [1/3], Batch [8593/21340], Train Loss: 3.2405, Train Perplexity: 25.5477\n",
            "Epoch [1/3], Batch [8594/21340], Train Loss: 3.2190, Train Perplexity: 25.0019\n",
            "Epoch [1/3], Batch [8595/21340], Train Loss: 3.2399, Train Perplexity: 25.5313\n",
            "Epoch [1/3], Batch [8596/21340], Train Loss: 3.1858, Train Perplexity: 24.1854\n",
            "Epoch [1/3], Batch [8597/21340], Train Loss: 3.2129, Train Perplexity: 24.8509\n",
            "Epoch [1/3], Batch [8598/21340], Train Loss: 3.2558, Train Perplexity: 25.9404\n",
            "Epoch [1/3], Batch [8599/21340], Train Loss: 3.1277, Train Perplexity: 22.8222\n",
            "Epoch [1/3], Batch [8600/21340], Train Loss: 3.3085, Train Perplexity: 27.3453\n",
            "Epoch [1/3], Batch [8601/21340], Train Loss: 3.3247, Train Perplexity: 27.7914\n",
            "Epoch [1/3], Batch [8602/21340], Train Loss: 3.1799, Train Perplexity: 24.0450\n",
            "Epoch [1/3], Batch [8603/21340], Train Loss: 3.3365, Train Perplexity: 28.1199\n",
            "Epoch [1/3], Batch [8604/21340], Train Loss: 3.4104, Train Perplexity: 30.2769\n",
            "Epoch [1/3], Batch [8605/21340], Train Loss: 3.2560, Train Perplexity: 25.9466\n",
            "Epoch [1/3], Batch [8606/21340], Train Loss: 3.2465, Train Perplexity: 25.7003\n",
            "Epoch [1/3], Batch [8607/21340], Train Loss: 3.1752, Train Perplexity: 23.9326\n",
            "Epoch [1/3], Batch [8608/21340], Train Loss: 3.2348, Train Perplexity: 25.4023\n",
            "Epoch [1/3], Batch [8609/21340], Train Loss: 3.1904, Train Perplexity: 24.2969\n",
            "Epoch [1/3], Batch [8610/21340], Train Loss: 3.2947, Train Perplexity: 26.9687\n",
            "Epoch [1/3], Batch [8611/21340], Train Loss: 3.4996, Train Perplexity: 33.1012\n",
            "Epoch [1/3], Batch [8612/21340], Train Loss: 3.2180, Train Perplexity: 24.9773\n",
            "Epoch [1/3], Batch [8613/21340], Train Loss: 3.3099, Train Perplexity: 27.3829\n",
            "Epoch [1/3], Batch [8614/21340], Train Loss: 3.2963, Train Perplexity: 27.0116\n",
            "Epoch [1/3], Batch [8615/21340], Train Loss: 3.4282, Train Perplexity: 30.8218\n",
            "Epoch [1/3], Batch [8616/21340], Train Loss: 3.2663, Train Perplexity: 26.2152\n",
            "Epoch [1/3], Batch [8617/21340], Train Loss: 3.2670, Train Perplexity: 26.2331\n",
            "Epoch [1/3], Batch [8618/21340], Train Loss: 3.1878, Train Perplexity: 24.2351\n",
            "Epoch [1/3], Batch [8619/21340], Train Loss: 3.2070, Train Perplexity: 24.7039\n",
            "Epoch [1/3], Batch [8620/21340], Train Loss: 3.2887, Train Perplexity: 26.8072\n",
            "Epoch [1/3], Batch [8621/21340], Train Loss: 3.2061, Train Perplexity: 24.6820\n",
            "Epoch [1/3], Batch [8622/21340], Train Loss: 3.2359, Train Perplexity: 25.4283\n",
            "Epoch [1/3], Batch [8623/21340], Train Loss: 3.2283, Train Perplexity: 25.2362\n",
            "Epoch [1/3], Batch [8624/21340], Train Loss: 3.3334, Train Perplexity: 28.0339\n",
            "Epoch [1/3], Batch [8625/21340], Train Loss: 3.2738, Train Perplexity: 26.4115\n",
            "Epoch [1/3], Batch [8626/21340], Train Loss: 3.1835, Train Perplexity: 24.1300\n",
            "Epoch [1/3], Batch [8627/21340], Train Loss: 3.2137, Train Perplexity: 24.8717\n",
            "Epoch [1/3], Batch [8628/21340], Train Loss: 3.2613, Train Perplexity: 26.0833\n",
            "Epoch [1/3], Batch [8629/21340], Train Loss: 3.2025, Train Perplexity: 24.5946\n",
            "Epoch [1/3], Batch [8630/21340], Train Loss: 3.1638, Train Perplexity: 23.6612\n",
            "Epoch [1/3], Batch [8631/21340], Train Loss: 3.1148, Train Perplexity: 22.5295\n",
            "Epoch [1/3], Batch [8632/21340], Train Loss: 3.2645, Train Perplexity: 26.1681\n",
            "Epoch [1/3], Batch [8633/21340], Train Loss: 3.1267, Train Perplexity: 22.7996\n",
            "Epoch [1/3], Batch [8634/21340], Train Loss: 3.1363, Train Perplexity: 23.0181\n",
            "Epoch [1/3], Batch [8635/21340], Train Loss: 3.2401, Train Perplexity: 25.5365\n",
            "Epoch [1/3], Batch [8636/21340], Train Loss: 3.2863, Train Perplexity: 26.7447\n",
            "Epoch [1/3], Batch [8637/21340], Train Loss: 3.5070, Train Perplexity: 33.3471\n",
            "Epoch [1/3], Batch [8638/21340], Train Loss: 3.2288, Train Perplexity: 25.2491\n",
            "Epoch [1/3], Batch [8639/21340], Train Loss: 3.1556, Train Perplexity: 23.4668\n",
            "Epoch [1/3], Batch [8640/21340], Train Loss: 3.1999, Train Perplexity: 24.5312\n",
            "Epoch [1/3], Batch [8641/21340], Train Loss: 3.2706, Train Perplexity: 26.3260\n",
            "Epoch [1/3], Batch [8642/21340], Train Loss: 3.2110, Train Perplexity: 24.8049\n",
            "Epoch [1/3], Batch [8643/21340], Train Loss: 3.3063, Train Perplexity: 27.2848\n",
            "Epoch [1/3], Batch [8644/21340], Train Loss: 3.2515, Train Perplexity: 25.8289\n",
            "Epoch [1/3], Batch [8645/21340], Train Loss: 3.2508, Train Perplexity: 25.8099\n",
            "Epoch [1/3], Batch [8646/21340], Train Loss: 3.3353, Train Perplexity: 28.0857\n",
            "Epoch [1/3], Batch [8647/21340], Train Loss: 3.3244, Train Perplexity: 27.7823\n",
            "Epoch [1/3], Batch [8648/21340], Train Loss: 3.3346, Train Perplexity: 28.0662\n",
            "Epoch [1/3], Batch [8649/21340], Train Loss: 3.2552, Train Perplexity: 25.9245\n",
            "Epoch [1/3], Batch [8650/21340], Train Loss: 3.2957, Train Perplexity: 26.9950\n",
            "Epoch [1/3], Batch [8651/21340], Train Loss: 3.2523, Train Perplexity: 25.8490\n",
            "Epoch [1/3], Batch [8652/21340], Train Loss: 3.2195, Train Perplexity: 25.0163\n",
            "Epoch [1/3], Batch [8653/21340], Train Loss: 3.4477, Train Perplexity: 31.4283\n",
            "Epoch [1/3], Batch [8654/21340], Train Loss: 3.2431, Train Perplexity: 25.6141\n",
            "Epoch [1/3], Batch [8655/21340], Train Loss: 3.2422, Train Perplexity: 25.5904\n",
            "Epoch [1/3], Batch [8656/21340], Train Loss: 3.2274, Train Perplexity: 25.2150\n",
            "Epoch [1/3], Batch [8657/21340], Train Loss: 3.2464, Train Perplexity: 25.6976\n",
            "Epoch [1/3], Batch [8658/21340], Train Loss: 3.2467, Train Perplexity: 25.7054\n",
            "Epoch [1/3], Batch [8659/21340], Train Loss: 3.3434, Train Perplexity: 28.3161\n",
            "Epoch [1/3], Batch [8660/21340], Train Loss: 3.1301, Train Perplexity: 22.8753\n",
            "Epoch [1/3], Batch [8661/21340], Train Loss: 3.1813, Train Perplexity: 24.0784\n",
            "Epoch [1/3], Batch [8662/21340], Train Loss: 3.2659, Train Perplexity: 26.2043\n",
            "Epoch [1/3], Batch [8663/21340], Train Loss: 3.1739, Train Perplexity: 23.9010\n",
            "Epoch [1/3], Batch [8664/21340], Train Loss: 3.1929, Train Perplexity: 24.3588\n",
            "Epoch [1/3], Batch [8665/21340], Train Loss: 3.2028, Train Perplexity: 24.6002\n",
            "Epoch [1/3], Batch [8666/21340], Train Loss: 3.2725, Train Perplexity: 26.3762\n",
            "Epoch [1/3], Batch [8667/21340], Train Loss: 3.3111, Train Perplexity: 27.4158\n",
            "Epoch [1/3], Batch [8668/21340], Train Loss: 3.2761, Train Perplexity: 26.4722\n",
            "Epoch [1/3], Batch [8669/21340], Train Loss: 3.4131, Train Perplexity: 30.3581\n",
            "Epoch [1/3], Batch [8670/21340], Train Loss: 3.3680, Train Perplexity: 29.0211\n",
            "Epoch [1/3], Batch [8671/21340], Train Loss: 3.1941, Train Perplexity: 24.3872\n",
            "Epoch [1/3], Batch [8672/21340], Train Loss: 3.5079, Train Perplexity: 33.3779\n",
            "Epoch [1/3], Batch [8673/21340], Train Loss: 3.2887, Train Perplexity: 26.8082\n",
            "Epoch [1/3], Batch [8674/21340], Train Loss: 3.2781, Train Perplexity: 26.5240\n",
            "Epoch [1/3], Batch [8675/21340], Train Loss: 3.2470, Train Perplexity: 25.7134\n",
            "Epoch [1/3], Batch [8676/21340], Train Loss: 3.1569, Train Perplexity: 23.4981\n",
            "Epoch [1/3], Batch [8677/21340], Train Loss: 3.2852, Train Perplexity: 26.7133\n",
            "Epoch [1/3], Batch [8678/21340], Train Loss: 3.3685, Train Perplexity: 29.0347\n",
            "Epoch [1/3], Batch [8679/21340], Train Loss: 3.3243, Train Perplexity: 27.7806\n",
            "Epoch [1/3], Batch [8680/21340], Train Loss: 3.2928, Train Perplexity: 26.9194\n",
            "Epoch [1/3], Batch [8681/21340], Train Loss: 3.3050, Train Perplexity: 27.2479\n",
            "Epoch [1/3], Batch [8682/21340], Train Loss: 3.2964, Train Perplexity: 27.0158\n",
            "Epoch [1/3], Batch [8683/21340], Train Loss: 3.2354, Train Perplexity: 25.4162\n",
            "Epoch [1/3], Batch [8684/21340], Train Loss: 3.1791, Train Perplexity: 24.0256\n",
            "Epoch [1/3], Batch [8685/21340], Train Loss: 3.3507, Train Perplexity: 28.5234\n",
            "Epoch [1/3], Batch [8686/21340], Train Loss: 3.2852, Train Perplexity: 26.7140\n",
            "Epoch [1/3], Batch [8687/21340], Train Loss: 3.1587, Train Perplexity: 23.5398\n",
            "Epoch [1/3], Batch [8688/21340], Train Loss: 3.2289, Train Perplexity: 25.2509\n",
            "Epoch [1/3], Batch [8689/21340], Train Loss: 3.1948, Train Perplexity: 24.4050\n",
            "Epoch [1/3], Batch [8690/21340], Train Loss: 3.3053, Train Perplexity: 27.2577\n",
            "Epoch [1/3], Batch [8691/21340], Train Loss: 3.1979, Train Perplexity: 24.4813\n",
            "Epoch [1/3], Batch [8692/21340], Train Loss: 3.3901, Train Perplexity: 29.6703\n",
            "Epoch [1/3], Batch [8693/21340], Train Loss: 3.2909, Train Perplexity: 26.8660\n",
            "Epoch [1/3], Batch [8694/21340], Train Loss: 3.1780, Train Perplexity: 23.9987\n",
            "Epoch [1/3], Batch [8695/21340], Train Loss: 3.1539, Train Perplexity: 23.4267\n",
            "Epoch [1/3], Batch [8696/21340], Train Loss: 3.2285, Train Perplexity: 25.2410\n",
            "Epoch [1/3], Batch [8697/21340], Train Loss: 3.2605, Train Perplexity: 26.0621\n",
            "Epoch [1/3], Batch [8698/21340], Train Loss: 3.1905, Train Perplexity: 24.3016\n",
            "Epoch [1/3], Batch [8699/21340], Train Loss: 3.2019, Train Perplexity: 24.5802\n",
            "Epoch [1/3], Batch [8700/21340], Train Loss: 3.3867, Train Perplexity: 29.5673\n",
            "Epoch [1/3], Batch [8701/21340], Train Loss: 3.2448, Train Perplexity: 25.6572\n",
            "Epoch [1/3], Batch [8702/21340], Train Loss: 3.3132, Train Perplexity: 27.4742\n",
            "Epoch [1/3], Batch [8703/21340], Train Loss: 3.1874, Train Perplexity: 24.2256\n",
            "Epoch [1/3], Batch [8704/21340], Train Loss: 3.1887, Train Perplexity: 24.2578\n",
            "Epoch [1/3], Batch [8705/21340], Train Loss: 3.3520, Train Perplexity: 28.5601\n",
            "Epoch [1/3], Batch [8706/21340], Train Loss: 3.2985, Train Perplexity: 27.0720\n",
            "Epoch [1/3], Batch [8707/21340], Train Loss: 3.2560, Train Perplexity: 25.9448\n",
            "Epoch [1/3], Batch [8708/21340], Train Loss: 3.1899, Train Perplexity: 24.2867\n",
            "Epoch [1/3], Batch [8709/21340], Train Loss: 3.3050, Train Perplexity: 27.2495\n",
            "Epoch [1/3], Batch [8710/21340], Train Loss: 3.1872, Train Perplexity: 24.2197\n",
            "Epoch [1/3], Batch [8711/21340], Train Loss: 3.3052, Train Perplexity: 27.2544\n",
            "Epoch [1/3], Batch [8712/21340], Train Loss: 3.2744, Train Perplexity: 26.4276\n",
            "Epoch [1/3], Batch [8713/21340], Train Loss: 3.3016, Train Perplexity: 27.1570\n",
            "Epoch [1/3], Batch [8714/21340], Train Loss: 3.2006, Train Perplexity: 24.5467\n",
            "Epoch [1/3], Batch [8715/21340], Train Loss: 3.2995, Train Perplexity: 27.0986\n",
            "Epoch [1/3], Batch [8716/21340], Train Loss: 3.3217, Train Perplexity: 27.7075\n",
            "Epoch [1/3], Batch [8717/21340], Train Loss: 3.2330, Train Perplexity: 25.3545\n",
            "Epoch [1/3], Batch [8718/21340], Train Loss: 3.2449, Train Perplexity: 25.6583\n",
            "Epoch [1/3], Batch [8719/21340], Train Loss: 3.2396, Train Perplexity: 25.5247\n",
            "Epoch [1/3], Batch [8720/21340], Train Loss: 3.1041, Train Perplexity: 22.2893\n",
            "Epoch [1/3], Batch [8721/21340], Train Loss: 3.3009, Train Perplexity: 27.1369\n",
            "Epoch [1/3], Batch [8722/21340], Train Loss: 3.2001, Train Perplexity: 24.5362\n",
            "Epoch [1/3], Batch [8723/21340], Train Loss: 3.2196, Train Perplexity: 25.0171\n",
            "Epoch [1/3], Batch [8724/21340], Train Loss: 3.3006, Train Perplexity: 27.1290\n",
            "Epoch [1/3], Batch [8725/21340], Train Loss: 3.2119, Train Perplexity: 24.8271\n",
            "Epoch [1/3], Batch [8726/21340], Train Loss: 3.3389, Train Perplexity: 28.1884\n",
            "Epoch [1/3], Batch [8727/21340], Train Loss: 3.2369, Train Perplexity: 25.4541\n",
            "Epoch [1/3], Batch [8728/21340], Train Loss: 3.2686, Train Perplexity: 26.2741\n",
            "Epoch [1/3], Batch [8729/21340], Train Loss: 3.3795, Train Perplexity: 29.3574\n",
            "Epoch [1/3], Batch [8730/21340], Train Loss: 3.2695, Train Perplexity: 26.2978\n",
            "Epoch [1/3], Batch [8731/21340], Train Loss: 3.3159, Train Perplexity: 27.5470\n",
            "Epoch [1/3], Batch [8732/21340], Train Loss: 3.2960, Train Perplexity: 27.0055\n",
            "Epoch [1/3], Batch [8733/21340], Train Loss: 3.1797, Train Perplexity: 24.0383\n",
            "Epoch [1/3], Batch [8734/21340], Train Loss: 3.2873, Train Perplexity: 26.7710\n",
            "Epoch [1/3], Batch [8735/21340], Train Loss: 3.2838, Train Perplexity: 26.6756\n",
            "Epoch [1/3], Batch [8736/21340], Train Loss: 3.2508, Train Perplexity: 25.8118\n",
            "Epoch [1/3], Batch [8737/21340], Train Loss: 3.1526, Train Perplexity: 23.3963\n",
            "Epoch [1/3], Batch [8738/21340], Train Loss: 3.2098, Train Perplexity: 24.7742\n",
            "Epoch [1/3], Batch [8739/21340], Train Loss: 3.3151, Train Perplexity: 27.5244\n",
            "Epoch [1/3], Batch [8740/21340], Train Loss: 3.2093, Train Perplexity: 24.7623\n",
            "Epoch [1/3], Batch [8741/21340], Train Loss: 3.2248, Train Perplexity: 25.1483\n",
            "Epoch [1/3], Batch [8742/21340], Train Loss: 3.1669, Train Perplexity: 23.7336\n",
            "Epoch [1/3], Batch [8743/21340], Train Loss: 3.3213, Train Perplexity: 27.6964\n",
            "Epoch [1/3], Batch [8744/21340], Train Loss: 3.1918, Train Perplexity: 24.3324\n",
            "Epoch [1/3], Batch [8745/21340], Train Loss: 3.2260, Train Perplexity: 25.1799\n",
            "Epoch [1/3], Batch [8746/21340], Train Loss: 3.2424, Train Perplexity: 25.5963\n",
            "Epoch [1/3], Batch [8747/21340], Train Loss: 3.2239, Train Perplexity: 25.1254\n",
            "Epoch [1/3], Batch [8748/21340], Train Loss: 3.2892, Train Perplexity: 26.8216\n",
            "Epoch [1/3], Batch [8749/21340], Train Loss: 3.3186, Train Perplexity: 27.6223\n",
            "Epoch [1/3], Batch [8750/21340], Train Loss: 3.3122, Train Perplexity: 27.4468\n",
            "Epoch [1/3], Batch [8751/21340], Train Loss: 3.2484, Train Perplexity: 25.7500\n",
            "Epoch [1/3], Batch [8752/21340], Train Loss: 3.3345, Train Perplexity: 28.0649\n",
            "Epoch [1/3], Batch [8753/21340], Train Loss: 3.2513, Train Perplexity: 25.8242\n",
            "Epoch [1/3], Batch [8754/21340], Train Loss: 3.2388, Train Perplexity: 25.5040\n",
            "Epoch [1/3], Batch [8755/21340], Train Loss: 3.2934, Train Perplexity: 26.9333\n",
            "Epoch [1/3], Batch [8756/21340], Train Loss: 3.3293, Train Perplexity: 27.9174\n",
            "Epoch [1/3], Batch [8757/21340], Train Loss: 3.2208, Train Perplexity: 25.0472\n",
            "Epoch [1/3], Batch [8758/21340], Train Loss: 3.2913, Train Perplexity: 26.8781\n",
            "Epoch [1/3], Batch [8759/21340], Train Loss: 3.1655, Train Perplexity: 23.7007\n",
            "Epoch [1/3], Batch [8760/21340], Train Loss: 3.0969, Train Perplexity: 22.1298\n",
            "Epoch [1/3], Batch [8761/21340], Train Loss: 3.1404, Train Perplexity: 23.1128\n",
            "Epoch [1/3], Batch [8762/21340], Train Loss: 3.2689, Train Perplexity: 26.2826\n",
            "Epoch [1/3], Batch [8763/21340], Train Loss: 3.1889, Train Perplexity: 24.2623\n",
            "Epoch [1/3], Batch [8764/21340], Train Loss: 3.1715, Train Perplexity: 23.8428\n",
            "Epoch [1/3], Batch [8765/21340], Train Loss: 3.2590, Train Perplexity: 26.0240\n",
            "Epoch [1/3], Batch [8766/21340], Train Loss: 3.3168, Train Perplexity: 27.5728\n",
            "Epoch [1/3], Batch [8767/21340], Train Loss: 3.2782, Train Perplexity: 26.5279\n",
            "Epoch [1/3], Batch [8768/21340], Train Loss: 3.2517, Train Perplexity: 25.8334\n",
            "Epoch [1/3], Batch [8769/21340], Train Loss: 3.3229, Train Perplexity: 27.7413\n",
            "Epoch [1/3], Batch [8770/21340], Train Loss: 3.4158, Train Perplexity: 30.4426\n",
            "Epoch [1/3], Batch [8771/21340], Train Loss: 3.1909, Train Perplexity: 24.3094\n",
            "Epoch [1/3], Batch [8772/21340], Train Loss: 3.2792, Train Perplexity: 26.5537\n",
            "Epoch [1/3], Batch [8773/21340], Train Loss: 3.3191, Train Perplexity: 27.6345\n",
            "Epoch [1/3], Batch [8774/21340], Train Loss: 3.2239, Train Perplexity: 25.1265\n",
            "Epoch [1/3], Batch [8775/21340], Train Loss: 3.2397, Train Perplexity: 25.5266\n",
            "Epoch [1/3], Batch [8776/21340], Train Loss: 3.1967, Train Perplexity: 24.4516\n",
            "Epoch [1/3], Batch [8777/21340], Train Loss: 3.3901, Train Perplexity: 29.6677\n",
            "Epoch [1/3], Batch [8778/21340], Train Loss: 3.3595, Train Perplexity: 28.7762\n",
            "Epoch [1/3], Batch [8779/21340], Train Loss: 3.3142, Train Perplexity: 27.5007\n",
            "Epoch [1/3], Batch [8780/21340], Train Loss: 3.1645, Train Perplexity: 23.6768\n",
            "Epoch [1/3], Batch [8781/21340], Train Loss: 3.1970, Train Perplexity: 24.4582\n",
            "Epoch [1/3], Batch [8782/21340], Train Loss: 3.1901, Train Perplexity: 24.2901\n",
            "Epoch [1/3], Batch [8783/21340], Train Loss: 3.2946, Train Perplexity: 26.9662\n",
            "Epoch [1/3], Batch [8784/21340], Train Loss: 3.2390, Train Perplexity: 25.5075\n",
            "Epoch [1/3], Batch [8785/21340], Train Loss: 3.1892, Train Perplexity: 24.2692\n",
            "Epoch [1/3], Batch [8786/21340], Train Loss: 3.2131, Train Perplexity: 24.8548\n",
            "Epoch [1/3], Batch [8787/21340], Train Loss: 3.2553, Train Perplexity: 25.9273\n",
            "Epoch [1/3], Batch [8788/21340], Train Loss: 3.2543, Train Perplexity: 25.9004\n",
            "Epoch [1/3], Batch [8789/21340], Train Loss: 3.2559, Train Perplexity: 25.9438\n",
            "Epoch [1/3], Batch [8790/21340], Train Loss: 3.2290, Train Perplexity: 25.2541\n",
            "Epoch [1/3], Batch [8791/21340], Train Loss: 3.3037, Train Perplexity: 27.2140\n",
            "Epoch [1/3], Batch [8792/21340], Train Loss: 3.3522, Train Perplexity: 28.5654\n",
            "Epoch [1/3], Batch [8793/21340], Train Loss: 3.2548, Train Perplexity: 25.9135\n",
            "Epoch [1/3], Batch [8794/21340], Train Loss: 3.2270, Train Perplexity: 25.2036\n",
            "Epoch [1/3], Batch [8795/21340], Train Loss: 3.2245, Train Perplexity: 25.1402\n",
            "Epoch [1/3], Batch [8796/21340], Train Loss: 3.2882, Train Perplexity: 26.7956\n",
            "Epoch [1/3], Batch [8797/21340], Train Loss: 3.2851, Train Perplexity: 26.7120\n",
            "Epoch [1/3], Batch [8798/21340], Train Loss: 3.1748, Train Perplexity: 23.9218\n",
            "Epoch [1/3], Batch [8799/21340], Train Loss: 3.2988, Train Perplexity: 27.0790\n",
            "Epoch [1/3], Batch [8800/21340], Train Loss: 3.2335, Train Perplexity: 25.3688\n",
            "Epoch [1/3], Batch [8801/21340], Train Loss: 3.2498, Train Perplexity: 25.7839\n",
            "Epoch [1/3], Batch [8802/21340], Train Loss: 3.3330, Train Perplexity: 28.0218\n",
            "Epoch [1/3], Batch [8803/21340], Train Loss: 3.2030, Train Perplexity: 24.6052\n",
            "Epoch [1/3], Batch [8804/21340], Train Loss: 3.3351, Train Perplexity: 28.0821\n",
            "Epoch [1/3], Batch [8805/21340], Train Loss: 3.1845, Train Perplexity: 24.1554\n",
            "Epoch [1/3], Batch [8806/21340], Train Loss: 3.2672, Train Perplexity: 26.2370\n",
            "Epoch [1/3], Batch [8807/21340], Train Loss: 3.2272, Train Perplexity: 25.2087\n",
            "Epoch [1/3], Batch [8808/21340], Train Loss: 3.3190, Train Perplexity: 27.6335\n",
            "Epoch [1/3], Batch [8809/21340], Train Loss: 3.2760, Train Perplexity: 26.4694\n",
            "Epoch [1/3], Batch [8810/21340], Train Loss: 3.3007, Train Perplexity: 27.1310\n",
            "Epoch [1/3], Batch [8811/21340], Train Loss: 3.1493, Train Perplexity: 23.3189\n",
            "Epoch [1/3], Batch [8812/21340], Train Loss: 3.2249, Train Perplexity: 25.1522\n",
            "Epoch [1/3], Batch [8813/21340], Train Loss: 3.1466, Train Perplexity: 23.2573\n",
            "Epoch [1/3], Batch [8814/21340], Train Loss: 3.2438, Train Perplexity: 25.6304\n",
            "Epoch [1/3], Batch [8815/21340], Train Loss: 3.1878, Train Perplexity: 24.2351\n",
            "Epoch [1/3], Batch [8816/21340], Train Loss: 3.2574, Train Perplexity: 25.9818\n",
            "Epoch [1/3], Batch [8817/21340], Train Loss: 3.2137, Train Perplexity: 24.8717\n",
            "Epoch [1/3], Batch [8818/21340], Train Loss: 3.2254, Train Perplexity: 25.1642\n",
            "Epoch [1/3], Batch [8819/21340], Train Loss: 3.2829, Train Perplexity: 26.6525\n",
            "Epoch [1/3], Batch [8820/21340], Train Loss: 3.2750, Train Perplexity: 26.4422\n",
            "Epoch [1/3], Batch [8821/21340], Train Loss: 3.3005, Train Perplexity: 27.1264\n",
            "Epoch [1/3], Batch [8822/21340], Train Loss: 3.2377, Train Perplexity: 25.4743\n",
            "Epoch [1/3], Batch [8823/21340], Train Loss: 3.3003, Train Perplexity: 27.1209\n",
            "Epoch [1/3], Batch [8824/21340], Train Loss: 3.2585, Train Perplexity: 26.0115\n",
            "Epoch [1/3], Batch [8825/21340], Train Loss: 3.1856, Train Perplexity: 24.1811\n",
            "Epoch [1/3], Batch [8826/21340], Train Loss: 3.2875, Train Perplexity: 26.7764\n",
            "Epoch [1/3], Batch [8827/21340], Train Loss: 3.1764, Train Perplexity: 23.9594\n",
            "Epoch [1/3], Batch [8828/21340], Train Loss: 3.2206, Train Perplexity: 25.0422\n",
            "Epoch [1/3], Batch [8829/21340], Train Loss: 3.2648, Train Perplexity: 26.1744\n",
            "Epoch [1/3], Batch [8830/21340], Train Loss: 3.2002, Train Perplexity: 24.5379\n",
            "Epoch [1/3], Batch [8831/21340], Train Loss: 3.3658, Train Perplexity: 28.9552\n",
            "Epoch [1/3], Batch [8832/21340], Train Loss: 3.1548, Train Perplexity: 23.4482\n",
            "Epoch [1/3], Batch [8833/21340], Train Loss: 3.1573, Train Perplexity: 23.5076\n",
            "Epoch [1/3], Batch [8834/21340], Train Loss: 3.2758, Train Perplexity: 26.4647\n",
            "Epoch [1/3], Batch [8835/21340], Train Loss: 3.2600, Train Perplexity: 26.0492\n",
            "Epoch [1/3], Batch [8836/21340], Train Loss: 3.3297, Train Perplexity: 27.9290\n",
            "Epoch [1/3], Batch [8837/21340], Train Loss: 3.2482, Train Perplexity: 25.7427\n",
            "Epoch [1/3], Batch [8838/21340], Train Loss: 3.2902, Train Perplexity: 26.8491\n",
            "Epoch [1/3], Batch [8839/21340], Train Loss: 3.4508, Train Perplexity: 31.5241\n",
            "Epoch [1/3], Batch [8840/21340], Train Loss: 3.2284, Train Perplexity: 25.2397\n",
            "Epoch [1/3], Batch [8841/21340], Train Loss: 3.2838, Train Perplexity: 26.6781\n",
            "Epoch [1/3], Batch [8842/21340], Train Loss: 3.3153, Train Perplexity: 27.5295\n",
            "Epoch [1/3], Batch [8843/21340], Train Loss: 3.2875, Train Perplexity: 26.7766\n",
            "Epoch [1/3], Batch [8844/21340], Train Loss: 3.1826, Train Perplexity: 24.1084\n",
            "Epoch [1/3], Batch [8845/21340], Train Loss: 3.2873, Train Perplexity: 26.7708\n",
            "Epoch [1/3], Batch [8846/21340], Train Loss: 3.2565, Train Perplexity: 25.9576\n",
            "Epoch [1/3], Batch [8847/21340], Train Loss: 3.1971, Train Perplexity: 24.4607\n",
            "Epoch [1/3], Batch [8848/21340], Train Loss: 3.2930, Train Perplexity: 26.9224\n",
            "Epoch [1/3], Batch [8849/21340], Train Loss: 3.1175, Train Perplexity: 22.5898\n",
            "Epoch [1/3], Batch [8850/21340], Train Loss: 3.1962, Train Perplexity: 24.4394\n",
            "Epoch [1/3], Batch [8851/21340], Train Loss: 3.1910, Train Perplexity: 24.3137\n",
            "Epoch [1/3], Batch [8852/21340], Train Loss: 3.2829, Train Perplexity: 26.6533\n",
            "Epoch [1/3], Batch [8853/21340], Train Loss: 3.2149, Train Perplexity: 24.9007\n",
            "Epoch [1/3], Batch [8854/21340], Train Loss: 3.1602, Train Perplexity: 23.5764\n",
            "Epoch [1/3], Batch [8855/21340], Train Loss: 3.1807, Train Perplexity: 24.0636\n",
            "Epoch [1/3], Batch [8856/21340], Train Loss: 3.2533, Train Perplexity: 25.8767\n",
            "Epoch [1/3], Batch [8857/21340], Train Loss: 3.1669, Train Perplexity: 23.7343\n",
            "Epoch [1/3], Batch [8858/21340], Train Loss: 3.4382, Train Perplexity: 31.1306\n",
            "Epoch [1/3], Batch [8859/21340], Train Loss: 3.1804, Train Perplexity: 24.0560\n",
            "Epoch [1/3], Batch [8860/21340], Train Loss: 3.1875, Train Perplexity: 24.2266\n",
            "Epoch [1/3], Batch [8861/21340], Train Loss: 3.2578, Train Perplexity: 25.9921\n",
            "Epoch [1/3], Batch [8862/21340], Train Loss: 3.2733, Train Perplexity: 26.3993\n",
            "Epoch [1/3], Batch [8863/21340], Train Loss: 3.1634, Train Perplexity: 23.6520\n",
            "Epoch [1/3], Batch [8864/21340], Train Loss: 3.1554, Train Perplexity: 23.4618\n",
            "Epoch [1/3], Batch [8865/21340], Train Loss: 3.1858, Train Perplexity: 24.1866\n",
            "Epoch [1/3], Batch [8866/21340], Train Loss: 3.2207, Train Perplexity: 25.0460\n",
            "Epoch [1/3], Batch [8867/21340], Train Loss: 3.2393, Train Perplexity: 25.5169\n",
            "Epoch [1/3], Batch [8868/21340], Train Loss: 3.2317, Train Perplexity: 25.3232\n",
            "Epoch [1/3], Batch [8869/21340], Train Loss: 3.3178, Train Perplexity: 27.5984\n",
            "Epoch [1/3], Batch [8870/21340], Train Loss: 3.2415, Train Perplexity: 25.5712\n",
            "Epoch [1/3], Batch [8871/21340], Train Loss: 3.2689, Train Perplexity: 26.2826\n",
            "Epoch [1/3], Batch [8872/21340], Train Loss: 3.2089, Train Perplexity: 24.7516\n",
            "Epoch [1/3], Batch [8873/21340], Train Loss: 3.3634, Train Perplexity: 28.8862\n",
            "Epoch [1/3], Batch [8874/21340], Train Loss: 3.3077, Train Perplexity: 27.3210\n",
            "Epoch [1/3], Batch [8875/21340], Train Loss: 3.2611, Train Perplexity: 26.0780\n",
            "Epoch [1/3], Batch [8876/21340], Train Loss: 3.2305, Train Perplexity: 25.2929\n",
            "Epoch [1/3], Batch [8877/21340], Train Loss: 3.1595, Train Perplexity: 23.5592\n",
            "Epoch [1/3], Batch [8878/21340], Train Loss: 3.2503, Train Perplexity: 25.7976\n",
            "Epoch [1/3], Batch [8879/21340], Train Loss: 3.4272, Train Perplexity: 30.7913\n",
            "Epoch [1/3], Batch [8880/21340], Train Loss: 3.2401, Train Perplexity: 25.5363\n",
            "Epoch [1/3], Batch [8881/21340], Train Loss: 3.2153, Train Perplexity: 24.9096\n",
            "Epoch [1/3], Batch [8882/21340], Train Loss: 3.2860, Train Perplexity: 26.7367\n",
            "Epoch [1/3], Batch [8883/21340], Train Loss: 3.2149, Train Perplexity: 24.9000\n",
            "Epoch [1/3], Batch [8884/21340], Train Loss: 3.2744, Train Perplexity: 26.4274\n",
            "Epoch [1/3], Batch [8885/21340], Train Loss: 3.1972, Train Perplexity: 24.4639\n",
            "Epoch [1/3], Batch [8886/21340], Train Loss: 3.1776, Train Perplexity: 23.9885\n",
            "Epoch [1/3], Batch [8887/21340], Train Loss: 3.2227, Train Perplexity: 25.0966\n",
            "Epoch [1/3], Batch [8888/21340], Train Loss: 3.2156, Train Perplexity: 24.9176\n",
            "Epoch [1/3], Batch [8889/21340], Train Loss: 3.2019, Train Perplexity: 24.5803\n",
            "Epoch [1/3], Batch [8890/21340], Train Loss: 3.2754, Train Perplexity: 26.4546\n",
            "Epoch [1/3], Batch [8891/21340], Train Loss: 3.3451, Train Perplexity: 28.3646\n",
            "Epoch [1/3], Batch [8892/21340], Train Loss: 3.2545, Train Perplexity: 25.9073\n",
            "Epoch [1/3], Batch [8893/21340], Train Loss: 3.3504, Train Perplexity: 28.5140\n",
            "Epoch [1/3], Batch [8894/21340], Train Loss: 3.1828, Train Perplexity: 24.1146\n",
            "Epoch [1/3], Batch [8895/21340], Train Loss: 3.2490, Train Perplexity: 25.7634\n",
            "Epoch [1/3], Batch [8896/21340], Train Loss: 3.2556, Train Perplexity: 25.9358\n",
            "Epoch [1/3], Batch [8897/21340], Train Loss: 3.2269, Train Perplexity: 25.2009\n",
            "Epoch [1/3], Batch [8898/21340], Train Loss: 3.4298, Train Perplexity: 30.8715\n",
            "Epoch [1/3], Batch [8899/21340], Train Loss: 3.2642, Train Perplexity: 26.1588\n",
            "Epoch [1/3], Batch [8900/21340], Train Loss: 3.3676, Train Perplexity: 29.0079\n",
            "Epoch [1/3], Batch [8901/21340], Train Loss: 3.2596, Train Perplexity: 26.0403\n",
            "Epoch [1/3], Batch [8902/21340], Train Loss: 3.2149, Train Perplexity: 24.9000\n",
            "Epoch [1/3], Batch [8903/21340], Train Loss: 3.2525, Train Perplexity: 25.8550\n",
            "Epoch [1/3], Batch [8904/21340], Train Loss: 3.1890, Train Perplexity: 24.2634\n",
            "Epoch [1/3], Batch [8905/21340], Train Loss: 3.1987, Train Perplexity: 24.4996\n",
            "Epoch [1/3], Batch [8906/21340], Train Loss: 3.1770, Train Perplexity: 23.9749\n",
            "Epoch [1/3], Batch [8907/21340], Train Loss: 3.2264, Train Perplexity: 25.1878\n",
            "Epoch [1/3], Batch [8908/21340], Train Loss: 3.2948, Train Perplexity: 26.9708\n",
            "Epoch [1/3], Batch [8909/21340], Train Loss: 3.2137, Train Perplexity: 24.8718\n",
            "Epoch [1/3], Batch [8910/21340], Train Loss: 3.1824, Train Perplexity: 24.1044\n",
            "Epoch [1/3], Batch [8911/21340], Train Loss: 3.3390, Train Perplexity: 28.1915\n",
            "Epoch [1/3], Batch [8912/21340], Train Loss: 3.3553, Train Perplexity: 28.6548\n",
            "Epoch [1/3], Batch [8913/21340], Train Loss: 3.3366, Train Perplexity: 28.1227\n",
            "Epoch [1/3], Batch [8914/21340], Train Loss: 3.2490, Train Perplexity: 25.7639\n",
            "Epoch [1/3], Batch [8915/21340], Train Loss: 3.3972, Train Perplexity: 29.8789\n",
            "Epoch [1/3], Batch [8916/21340], Train Loss: 3.2484, Train Perplexity: 25.7486\n",
            "Epoch [1/3], Batch [8917/21340], Train Loss: 3.3538, Train Perplexity: 28.6123\n",
            "Epoch [1/3], Batch [8918/21340], Train Loss: 3.2042, Train Perplexity: 24.6359\n",
            "Epoch [1/3], Batch [8919/21340], Train Loss: 3.3325, Train Perplexity: 28.0084\n",
            "Epoch [1/3], Batch [8920/21340], Train Loss: 3.1939, Train Perplexity: 24.3824\n",
            "Epoch [1/3], Batch [8921/21340], Train Loss: 3.2619, Train Perplexity: 26.0989\n",
            "Epoch [1/3], Batch [8922/21340], Train Loss: 3.3844, Train Perplexity: 29.4992\n",
            "Epoch [1/3], Batch [8923/21340], Train Loss: 3.3186, Train Perplexity: 27.6219\n",
            "Epoch [1/3], Batch [8924/21340], Train Loss: 3.2934, Train Perplexity: 26.9353\n",
            "Epoch [1/3], Batch [8925/21340], Train Loss: 3.3214, Train Perplexity: 27.6990\n",
            "Epoch [1/3], Batch [8926/21340], Train Loss: 3.3498, Train Perplexity: 28.4958\n",
            "Epoch [1/3], Batch [8927/21340], Train Loss: 3.1952, Train Perplexity: 24.4151\n",
            "Epoch [1/3], Batch [8928/21340], Train Loss: 3.1047, Train Perplexity: 22.3022\n",
            "Epoch [1/3], Batch [8929/21340], Train Loss: 3.2465, Train Perplexity: 25.6991\n",
            "Epoch [1/3], Batch [8930/21340], Train Loss: 3.2930, Train Perplexity: 26.9240\n",
            "Epoch [1/3], Batch [8931/21340], Train Loss: 3.1208, Train Perplexity: 22.6640\n",
            "Epoch [1/3], Batch [8932/21340], Train Loss: 3.2819, Train Perplexity: 26.6271\n",
            "Epoch [1/3], Batch [8933/21340], Train Loss: 3.2205, Train Perplexity: 25.0401\n",
            "Epoch [1/3], Batch [8934/21340], Train Loss: 3.2097, Train Perplexity: 24.7716\n",
            "Epoch [1/3], Batch [8935/21340], Train Loss: 3.2663, Train Perplexity: 26.2134\n",
            "Epoch [1/3], Batch [8936/21340], Train Loss: 3.1925, Train Perplexity: 24.3497\n",
            "Epoch [1/3], Batch [8937/21340], Train Loss: 3.2622, Train Perplexity: 26.1075\n",
            "Epoch [1/3], Batch [8938/21340], Train Loss: 3.2747, Train Perplexity: 26.4350\n",
            "Epoch [1/3], Batch [8939/21340], Train Loss: 3.2395, Train Perplexity: 25.5204\n",
            "Epoch [1/3], Batch [8940/21340], Train Loss: 3.2970, Train Perplexity: 27.0316\n",
            "Epoch [1/3], Batch [8941/21340], Train Loss: 3.3320, Train Perplexity: 27.9956\n",
            "Epoch [1/3], Batch [8942/21340], Train Loss: 3.2649, Train Perplexity: 26.1764\n",
            "Epoch [1/3], Batch [8943/21340], Train Loss: 3.1667, Train Perplexity: 23.7298\n",
            "Epoch [1/3], Batch [8944/21340], Train Loss: 3.2192, Train Perplexity: 25.0085\n",
            "Epoch [1/3], Batch [8945/21340], Train Loss: 3.2674, Train Perplexity: 26.2425\n",
            "Epoch [1/3], Batch [8946/21340], Train Loss: 3.1902, Train Perplexity: 24.2939\n",
            "Epoch [1/3], Batch [8947/21340], Train Loss: 3.2313, Train Perplexity: 25.3122\n",
            "Epoch [1/3], Batch [8948/21340], Train Loss: 3.3109, Train Perplexity: 27.4090\n",
            "Epoch [1/3], Batch [8949/21340], Train Loss: 3.2406, Train Perplexity: 25.5502\n",
            "Epoch [1/3], Batch [8950/21340], Train Loss: 3.1793, Train Perplexity: 24.0300\n",
            "Epoch [1/3], Batch [8951/21340], Train Loss: 3.2850, Train Perplexity: 26.7088\n",
            "Epoch [1/3], Batch [8952/21340], Train Loss: 3.3221, Train Perplexity: 27.7194\n",
            "Epoch [1/3], Batch [8953/21340], Train Loss: 3.1667, Train Perplexity: 23.7293\n",
            "Epoch [1/3], Batch [8954/21340], Train Loss: 3.2914, Train Perplexity: 26.8793\n",
            "Epoch [1/3], Batch [8955/21340], Train Loss: 3.1592, Train Perplexity: 23.5514\n",
            "Epoch [1/3], Batch [8956/21340], Train Loss: 3.2610, Train Perplexity: 26.0758\n",
            "Epoch [1/3], Batch [8957/21340], Train Loss: 3.4299, Train Perplexity: 30.8749\n",
            "Epoch [1/3], Batch [8958/21340], Train Loss: 3.2310, Train Perplexity: 25.3054\n",
            "Epoch [1/3], Batch [8959/21340], Train Loss: 3.2400, Train Perplexity: 25.5349\n",
            "Epoch [1/3], Batch [8960/21340], Train Loss: 3.2874, Train Perplexity: 26.7726\n",
            "Epoch [1/3], Batch [8961/21340], Train Loss: 3.2265, Train Perplexity: 25.1915\n",
            "Epoch [1/3], Batch [8962/21340], Train Loss: 3.2027, Train Perplexity: 24.5998\n",
            "Epoch [1/3], Batch [8963/21340], Train Loss: 3.2226, Train Perplexity: 25.0936\n",
            "Epoch [1/3], Batch [8964/21340], Train Loss: 3.2025, Train Perplexity: 24.5941\n",
            "Epoch [1/3], Batch [8965/21340], Train Loss: 3.2194, Train Perplexity: 25.0127\n",
            "Epoch [1/3], Batch [8966/21340], Train Loss: 3.2929, Train Perplexity: 26.9214\n",
            "Epoch [1/3], Batch [8967/21340], Train Loss: 3.2181, Train Perplexity: 24.9802\n",
            "Epoch [1/3], Batch [8968/21340], Train Loss: 3.1921, Train Perplexity: 24.3399\n",
            "Epoch [1/3], Batch [8969/21340], Train Loss: 3.2034, Train Perplexity: 24.6160\n",
            "Epoch [1/3], Batch [8970/21340], Train Loss: 3.2599, Train Perplexity: 26.0468\n",
            "Epoch [1/3], Batch [8971/21340], Train Loss: 3.1900, Train Perplexity: 24.2892\n",
            "Epoch [1/3], Batch [8972/21340], Train Loss: 3.3711, Train Perplexity: 29.1118\n",
            "Epoch [1/3], Batch [8973/21340], Train Loss: 3.5094, Train Perplexity: 33.4282\n",
            "Epoch [1/3], Batch [8974/21340], Train Loss: 3.2146, Train Perplexity: 24.8930\n",
            "Epoch [1/3], Batch [8975/21340], Train Loss: 3.4423, Train Perplexity: 31.2586\n",
            "Epoch [1/3], Batch [8976/21340], Train Loss: 3.2461, Train Perplexity: 25.6910\n",
            "Epoch [1/3], Batch [8977/21340], Train Loss: 3.1835, Train Perplexity: 24.1305\n",
            "Epoch [1/3], Batch [8978/21340], Train Loss: 3.2652, Train Perplexity: 26.1845\n",
            "Epoch [1/3], Batch [8979/21340], Train Loss: 3.3081, Train Perplexity: 27.3329\n",
            "Epoch [1/3], Batch [8980/21340], Train Loss: 3.2945, Train Perplexity: 26.9643\n",
            "Epoch [1/3], Batch [8981/21340], Train Loss: 3.2512, Train Perplexity: 25.8206\n",
            "Epoch [1/3], Batch [8982/21340], Train Loss: 3.2372, Train Perplexity: 25.4630\n",
            "Epoch [1/3], Batch [8983/21340], Train Loss: 3.3238, Train Perplexity: 27.7649\n",
            "Epoch [1/3], Batch [8984/21340], Train Loss: 3.3035, Train Perplexity: 27.2065\n",
            "Epoch [1/3], Batch [8985/21340], Train Loss: 3.2630, Train Perplexity: 26.1285\n",
            "Epoch [1/3], Batch [8986/21340], Train Loss: 3.3361, Train Perplexity: 28.1089\n",
            "Epoch [1/3], Batch [8987/21340], Train Loss: 3.2623, Train Perplexity: 26.1093\n",
            "Epoch [1/3], Batch [8988/21340], Train Loss: 3.2600, Train Perplexity: 26.0488\n",
            "Epoch [1/3], Batch [8989/21340], Train Loss: 3.2437, Train Perplexity: 25.6285\n",
            "Epoch [1/3], Batch [8990/21340], Train Loss: 3.1375, Train Perplexity: 23.0462\n",
            "Epoch [1/3], Batch [8991/21340], Train Loss: 3.1859, Train Perplexity: 24.1894\n",
            "Epoch [1/3], Batch [8992/21340], Train Loss: 3.3456, Train Perplexity: 28.3777\n",
            "Epoch [1/3], Batch [8993/21340], Train Loss: 3.2452, Train Perplexity: 25.6679\n",
            "Epoch [1/3], Batch [8994/21340], Train Loss: 3.2729, Train Perplexity: 26.3872\n",
            "Epoch [1/3], Batch [8995/21340], Train Loss: 3.4068, Train Perplexity: 30.1674\n",
            "Epoch [1/3], Batch [8996/21340], Train Loss: 3.2302, Train Perplexity: 25.2849\n",
            "Epoch [1/3], Batch [8997/21340], Train Loss: 3.2745, Train Perplexity: 26.4309\n",
            "Epoch [1/3], Batch [8998/21340], Train Loss: 3.2140, Train Perplexity: 24.8778\n",
            "Epoch [1/3], Batch [8999/21340], Train Loss: 3.4415, Train Perplexity: 31.2334\n",
            "Epoch [1/3], Batch [9000/21340], Train Loss: 3.1946, Train Perplexity: 24.3998\n",
            "Epoch [1/3], Batch [9001/21340], Train Loss: 3.1121, Train Perplexity: 22.4682\n",
            "Epoch [1/3], Batch [9002/21340], Train Loss: 3.1578, Train Perplexity: 23.5181\n",
            "Epoch [1/3], Batch [9003/21340], Train Loss: 3.2728, Train Perplexity: 26.3844\n",
            "Epoch [1/3], Batch [9004/21340], Train Loss: 3.3059, Train Perplexity: 27.2725\n",
            "Epoch [1/3], Batch [9005/21340], Train Loss: 3.1976, Train Perplexity: 24.4730\n",
            "Epoch [1/3], Batch [9006/21340], Train Loss: 3.2085, Train Perplexity: 24.7421\n",
            "Epoch [1/3], Batch [9007/21340], Train Loss: 3.2195, Train Perplexity: 25.0156\n",
            "Epoch [1/3], Batch [9008/21340], Train Loss: 3.1705, Train Perplexity: 23.8195\n",
            "Epoch [1/3], Batch [9009/21340], Train Loss: 3.2358, Train Perplexity: 25.4262\n",
            "Epoch [1/3], Batch [9010/21340], Train Loss: 3.2223, Train Perplexity: 25.0863\n",
            "Epoch [1/3], Batch [9011/21340], Train Loss: 3.2913, Train Perplexity: 26.8776\n",
            "Epoch [1/3], Batch [9012/21340], Train Loss: 3.3129, Train Perplexity: 27.4653\n",
            "Epoch [1/3], Batch [9013/21340], Train Loss: 3.2683, Train Perplexity: 26.2659\n",
            "Epoch [1/3], Batch [9014/21340], Train Loss: 3.2823, Train Perplexity: 26.6368\n",
            "Epoch [1/3], Batch [9015/21340], Train Loss: 3.2036, Train Perplexity: 24.6207\n",
            "Epoch [1/3], Batch [9016/21340], Train Loss: 3.2095, Train Perplexity: 24.7662\n",
            "Epoch [1/3], Batch [9017/21340], Train Loss: 3.2529, Train Perplexity: 25.8649\n",
            "Epoch [1/3], Batch [9018/21340], Train Loss: 3.2978, Train Perplexity: 27.0535\n",
            "Epoch [1/3], Batch [9019/21340], Train Loss: 3.1663, Train Perplexity: 23.7189\n",
            "Epoch [1/3], Batch [9020/21340], Train Loss: 3.2701, Train Perplexity: 26.3144\n",
            "Epoch [1/3], Batch [9021/21340], Train Loss: 3.1208, Train Perplexity: 22.6649\n",
            "Epoch [1/3], Batch [9022/21340], Train Loss: 3.2265, Train Perplexity: 25.1906\n",
            "Epoch [1/3], Batch [9023/21340], Train Loss: 3.2975, Train Perplexity: 27.0447\n",
            "Epoch [1/3], Batch [9024/21340], Train Loss: 3.2072, Train Perplexity: 24.7096\n",
            "Epoch [1/3], Batch [9025/21340], Train Loss: 3.1890, Train Perplexity: 24.2639\n",
            "Epoch [1/3], Batch [9026/21340], Train Loss: 3.2737, Train Perplexity: 26.4088\n",
            "Epoch [1/3], Batch [9027/21340], Train Loss: 3.2828, Train Perplexity: 26.6501\n",
            "Epoch [1/3], Batch [9028/21340], Train Loss: 3.2311, Train Perplexity: 25.3080\n",
            "Epoch [1/3], Batch [9029/21340], Train Loss: 3.2662, Train Perplexity: 26.2127\n",
            "Epoch [1/3], Batch [9030/21340], Train Loss: 3.2234, Train Perplexity: 25.1130\n",
            "Epoch [1/3], Batch [9031/21340], Train Loss: 3.2873, Train Perplexity: 26.7718\n",
            "Epoch [1/3], Batch [9032/21340], Train Loss: 3.2025, Train Perplexity: 24.5934\n",
            "Epoch [1/3], Batch [9033/21340], Train Loss: 3.2050, Train Perplexity: 24.6557\n",
            "Epoch [1/3], Batch [9034/21340], Train Loss: 3.2625, Train Perplexity: 26.1143\n",
            "Epoch [1/3], Batch [9035/21340], Train Loss: 3.3149, Train Perplexity: 27.5203\n",
            "Epoch [1/3], Batch [9036/21340], Train Loss: 3.2266, Train Perplexity: 25.1934\n",
            "Epoch [1/3], Batch [9037/21340], Train Loss: 3.2921, Train Perplexity: 26.8992\n",
            "Epoch [1/3], Batch [9038/21340], Train Loss: 3.2669, Train Perplexity: 26.2306\n",
            "Epoch [1/3], Batch [9039/21340], Train Loss: 3.2977, Train Perplexity: 27.0492\n",
            "Epoch [1/3], Batch [9040/21340], Train Loss: 3.2167, Train Perplexity: 24.9447\n",
            "Epoch [1/3], Batch [9041/21340], Train Loss: 3.1297, Train Perplexity: 22.8670\n",
            "Epoch [1/3], Batch [9042/21340], Train Loss: 3.2366, Train Perplexity: 25.4476\n",
            "Epoch [1/3], Batch [9043/21340], Train Loss: 3.3112, Train Perplexity: 27.4178\n",
            "Epoch [1/3], Batch [9044/21340], Train Loss: 3.2512, Train Perplexity: 25.8214\n",
            "Epoch [1/3], Batch [9045/21340], Train Loss: 3.2640, Train Perplexity: 26.1531\n",
            "Epoch [1/3], Batch [9046/21340], Train Loss: 3.2271, Train Perplexity: 25.2061\n",
            "Epoch [1/3], Batch [9047/21340], Train Loss: 3.2711, Train Perplexity: 26.3391\n",
            "Epoch [1/3], Batch [9048/21340], Train Loss: 3.3177, Train Perplexity: 27.5955\n",
            "Epoch [1/3], Batch [9049/21340], Train Loss: 3.3267, Train Perplexity: 27.8474\n",
            "Epoch [1/3], Batch [9050/21340], Train Loss: 3.2314, Train Perplexity: 25.3149\n",
            "Epoch [1/3], Batch [9051/21340], Train Loss: 3.1760, Train Perplexity: 23.9498\n",
            "Epoch [1/3], Batch [9052/21340], Train Loss: 3.3751, Train Perplexity: 29.2275\n",
            "Epoch [1/3], Batch [9053/21340], Train Loss: 3.2561, Train Perplexity: 25.9493\n",
            "Epoch [1/3], Batch [9054/21340], Train Loss: 3.2779, Train Perplexity: 26.5206\n",
            "Epoch [1/3], Batch [9055/21340], Train Loss: 3.3233, Train Perplexity: 27.7510\n",
            "Epoch [1/3], Batch [9056/21340], Train Loss: 3.2214, Train Perplexity: 25.0622\n",
            "Epoch [1/3], Batch [9057/21340], Train Loss: 3.2663, Train Perplexity: 26.2153\n",
            "Epoch [1/3], Batch [9058/21340], Train Loss: 3.1436, Train Perplexity: 23.1881\n",
            "Epoch [1/3], Batch [9059/21340], Train Loss: 3.1926, Train Perplexity: 24.3507\n",
            "Epoch [1/3], Batch [9060/21340], Train Loss: 3.1526, Train Perplexity: 23.3968\n",
            "Epoch [1/3], Batch [9061/21340], Train Loss: 3.3078, Train Perplexity: 27.3261\n",
            "Epoch [1/3], Batch [9062/21340], Train Loss: 3.2767, Train Perplexity: 26.4891\n",
            "Epoch [1/3], Batch [9063/21340], Train Loss: 3.2940, Train Perplexity: 26.9499\n",
            "Epoch [1/3], Batch [9064/21340], Train Loss: 3.2292, Train Perplexity: 25.2585\n",
            "Epoch [1/3], Batch [9065/21340], Train Loss: 3.3275, Train Perplexity: 27.8692\n",
            "Epoch [1/3], Batch [9066/21340], Train Loss: 3.2219, Train Perplexity: 25.0768\n",
            "Epoch [1/3], Batch [9067/21340], Train Loss: 3.2848, Train Perplexity: 26.7039\n",
            "Epoch [1/3], Batch [9068/21340], Train Loss: 3.2926, Train Perplexity: 26.9118\n",
            "Epoch [1/3], Batch [9069/21340], Train Loss: 3.1861, Train Perplexity: 24.1938\n",
            "Epoch [1/3], Batch [9070/21340], Train Loss: 3.2764, Train Perplexity: 26.4811\n",
            "Epoch [1/3], Batch [9071/21340], Train Loss: 3.1963, Train Perplexity: 24.4427\n",
            "Epoch [1/3], Batch [9072/21340], Train Loss: 3.3091, Train Perplexity: 27.3595\n",
            "Epoch [1/3], Batch [9073/21340], Train Loss: 3.2792, Train Perplexity: 26.5558\n",
            "Epoch [1/3], Batch [9074/21340], Train Loss: 3.2061, Train Perplexity: 24.6829\n",
            "Epoch [1/3], Batch [9075/21340], Train Loss: 3.2232, Train Perplexity: 25.1072\n",
            "Epoch [1/3], Batch [9076/21340], Train Loss: 3.2602, Train Perplexity: 26.0557\n",
            "Epoch [1/3], Batch [9077/21340], Train Loss: 3.1972, Train Perplexity: 24.4647\n",
            "Epoch [1/3], Batch [9078/21340], Train Loss: 3.2154, Train Perplexity: 24.9144\n",
            "Epoch [1/3], Batch [9079/21340], Train Loss: 3.2863, Train Perplexity: 26.7442\n",
            "Epoch [1/3], Batch [9080/21340], Train Loss: 3.3477, Train Perplexity: 28.4374\n",
            "Epoch [1/3], Batch [9081/21340], Train Loss: 3.1890, Train Perplexity: 24.2637\n",
            "Epoch [1/3], Batch [9082/21340], Train Loss: 3.1823, Train Perplexity: 24.1026\n",
            "Epoch [1/3], Batch [9083/21340], Train Loss: 3.2881, Train Perplexity: 26.7908\n",
            "Epoch [1/3], Batch [9084/21340], Train Loss: 3.2944, Train Perplexity: 26.9601\n",
            "Epoch [1/3], Batch [9085/21340], Train Loss: 3.3144, Train Perplexity: 27.5047\n",
            "Epoch [1/3], Batch [9086/21340], Train Loss: 3.2583, Train Perplexity: 26.0059\n",
            "Epoch [1/3], Batch [9087/21340], Train Loss: 3.1625, Train Perplexity: 23.6285\n",
            "Epoch [1/3], Batch [9088/21340], Train Loss: 3.3882, Train Perplexity: 29.6138\n",
            "Epoch [1/3], Batch [9089/21340], Train Loss: 3.2566, Train Perplexity: 25.9614\n",
            "Epoch [1/3], Batch [9090/21340], Train Loss: 3.2151, Train Perplexity: 24.9058\n",
            "Epoch [1/3], Batch [9091/21340], Train Loss: 3.2821, Train Perplexity: 26.6325\n",
            "Epoch [1/3], Batch [9092/21340], Train Loss: 3.2627, Train Perplexity: 26.1196\n",
            "Epoch [1/3], Batch [9093/21340], Train Loss: 3.1590, Train Perplexity: 23.5467\n",
            "Epoch [1/3], Batch [9094/21340], Train Loss: 3.2305, Train Perplexity: 25.2917\n",
            "Epoch [1/3], Batch [9095/21340], Train Loss: 3.1448, Train Perplexity: 23.2141\n",
            "Epoch [1/3], Batch [9096/21340], Train Loss: 3.3653, Train Perplexity: 28.9411\n",
            "Epoch [1/3], Batch [9097/21340], Train Loss: 3.2901, Train Perplexity: 26.8450\n",
            "Epoch [1/3], Batch [9098/21340], Train Loss: 3.1852, Train Perplexity: 24.1731\n",
            "Epoch [1/3], Batch [9099/21340], Train Loss: 3.2143, Train Perplexity: 24.8847\n",
            "Epoch [1/3], Batch [9100/21340], Train Loss: 3.1679, Train Perplexity: 23.7579\n",
            "Epoch [1/3], Batch [9101/21340], Train Loss: 3.4581, Train Perplexity: 31.7561\n",
            "Epoch [1/3], Batch [9102/21340], Train Loss: 3.2971, Train Perplexity: 27.0335\n",
            "Epoch [1/3], Batch [9103/21340], Train Loss: 3.2426, Train Perplexity: 25.5998\n",
            "Epoch [1/3], Batch [9104/21340], Train Loss: 3.1768, Train Perplexity: 23.9709\n",
            "Epoch [1/3], Batch [9105/21340], Train Loss: 3.2450, Train Perplexity: 25.6611\n",
            "Epoch [1/3], Batch [9106/21340], Train Loss: 3.2104, Train Perplexity: 24.7886\n",
            "Epoch [1/3], Batch [9107/21340], Train Loss: 3.2440, Train Perplexity: 25.6350\n",
            "Epoch [1/3], Batch [9108/21340], Train Loss: 3.2667, Train Perplexity: 26.2243\n",
            "Epoch [1/3], Batch [9109/21340], Train Loss: 3.2025, Train Perplexity: 24.5932\n",
            "Epoch [1/3], Batch [9110/21340], Train Loss: 3.3525, Train Perplexity: 28.5751\n",
            "Epoch [1/3], Batch [9111/21340], Train Loss: 3.2699, Train Perplexity: 26.3077\n",
            "Epoch [1/3], Batch [9112/21340], Train Loss: 3.2051, Train Perplexity: 24.6571\n",
            "Epoch [1/3], Batch [9113/21340], Train Loss: 3.3236, Train Perplexity: 27.7613\n",
            "Epoch [1/3], Batch [9114/21340], Train Loss: 3.1378, Train Perplexity: 23.0534\n",
            "Epoch [1/3], Batch [9115/21340], Train Loss: 3.3192, Train Perplexity: 27.6389\n",
            "Epoch [1/3], Batch [9116/21340], Train Loss: 3.4254, Train Perplexity: 30.7360\n",
            "Epoch [1/3], Batch [9117/21340], Train Loss: 3.1451, Train Perplexity: 23.2220\n",
            "Epoch [1/3], Batch [9118/21340], Train Loss: 3.2976, Train Perplexity: 27.0466\n",
            "Epoch [1/3], Batch [9119/21340], Train Loss: 3.2438, Train Perplexity: 25.6305\n",
            "Epoch [1/3], Batch [9120/21340], Train Loss: 3.2596, Train Perplexity: 26.0391\n",
            "Epoch [1/3], Batch [9121/21340], Train Loss: 3.2064, Train Perplexity: 24.6908\n",
            "Epoch [1/3], Batch [9122/21340], Train Loss: 3.3889, Train Perplexity: 29.6338\n",
            "Epoch [1/3], Batch [9123/21340], Train Loss: 3.2877, Train Perplexity: 26.7804\n",
            "Epoch [1/3], Batch [9124/21340], Train Loss: 3.4437, Train Perplexity: 31.3018\n",
            "Epoch [1/3], Batch [9125/21340], Train Loss: 3.2326, Train Perplexity: 25.3446\n",
            "Epoch [1/3], Batch [9126/21340], Train Loss: 3.3283, Train Perplexity: 27.8920\n",
            "Epoch [1/3], Batch [9127/21340], Train Loss: 3.2480, Train Perplexity: 25.7382\n",
            "Epoch [1/3], Batch [9128/21340], Train Loss: 3.3483, Train Perplexity: 28.4533\n",
            "Epoch [1/3], Batch [9129/21340], Train Loss: 3.1386, Train Perplexity: 23.0713\n",
            "Epoch [1/3], Batch [9130/21340], Train Loss: 3.4051, Train Perplexity: 30.1188\n",
            "Epoch [1/3], Batch [9131/21340], Train Loss: 3.2513, Train Perplexity: 25.8226\n",
            "Epoch [1/3], Batch [9132/21340], Train Loss: 3.2292, Train Perplexity: 25.2591\n",
            "Epoch [1/3], Batch [9133/21340], Train Loss: 3.3119, Train Perplexity: 27.4367\n",
            "Epoch [1/3], Batch [9134/21340], Train Loss: 3.2046, Train Perplexity: 24.6464\n",
            "Epoch [1/3], Batch [9135/21340], Train Loss: 3.2281, Train Perplexity: 25.2324\n",
            "Epoch [1/3], Batch [9136/21340], Train Loss: 3.5235, Train Perplexity: 33.9022\n",
            "Epoch [1/3], Batch [9137/21340], Train Loss: 3.2561, Train Perplexity: 25.9483\n",
            "Epoch [1/3], Batch [9138/21340], Train Loss: 3.2148, Train Perplexity: 24.8978\n",
            "Epoch [1/3], Batch [9139/21340], Train Loss: 3.2561, Train Perplexity: 25.9481\n",
            "Epoch [1/3], Batch [9140/21340], Train Loss: 3.2513, Train Perplexity: 25.8229\n",
            "Epoch [1/3], Batch [9141/21340], Train Loss: 3.2369, Train Perplexity: 25.4553\n",
            "Epoch [1/3], Batch [9142/21340], Train Loss: 3.3112, Train Perplexity: 27.4179\n",
            "Epoch [1/3], Batch [9143/21340], Train Loss: 3.0900, Train Perplexity: 21.9766\n",
            "Epoch [1/3], Batch [9144/21340], Train Loss: 3.1613, Train Perplexity: 23.6016\n",
            "Epoch [1/3], Batch [9145/21340], Train Loss: 3.2713, Train Perplexity: 26.3444\n",
            "Epoch [1/3], Batch [9146/21340], Train Loss: 3.2792, Train Perplexity: 26.5557\n",
            "Epoch [1/3], Batch [9147/21340], Train Loss: 3.2544, Train Perplexity: 25.9029\n",
            "Epoch [1/3], Batch [9148/21340], Train Loss: 3.2172, Train Perplexity: 24.9587\n",
            "Epoch [1/3], Batch [9149/21340], Train Loss: 3.2730, Train Perplexity: 26.3907\n",
            "Epoch [1/3], Batch [9150/21340], Train Loss: 3.2402, Train Perplexity: 25.5391\n",
            "Epoch [1/3], Batch [9151/21340], Train Loss: 3.1478, Train Perplexity: 23.2851\n",
            "Epoch [1/3], Batch [9152/21340], Train Loss: 3.2893, Train Perplexity: 26.8237\n",
            "Epoch [1/3], Batch [9153/21340], Train Loss: 3.2451, Train Perplexity: 25.6654\n",
            "Epoch [1/3], Batch [9154/21340], Train Loss: 3.2441, Train Perplexity: 25.6385\n",
            "Epoch [1/3], Batch [9155/21340], Train Loss: 3.2076, Train Perplexity: 24.7197\n",
            "Epoch [1/3], Batch [9156/21340], Train Loss: 3.1921, Train Perplexity: 24.3402\n",
            "Epoch [1/3], Batch [9157/21340], Train Loss: 3.2485, Train Perplexity: 25.7519\n",
            "Epoch [1/3], Batch [9158/21340], Train Loss: 3.3592, Train Perplexity: 28.7650\n",
            "Epoch [1/3], Batch [9159/21340], Train Loss: 3.2604, Train Perplexity: 26.0602\n",
            "Epoch [1/3], Batch [9160/21340], Train Loss: 3.1774, Train Perplexity: 23.9839\n",
            "Epoch [1/3], Batch [9161/21340], Train Loss: 3.2852, Train Perplexity: 26.7148\n",
            "Epoch [1/3], Batch [9162/21340], Train Loss: 3.1450, Train Perplexity: 23.2190\n",
            "Epoch [1/3], Batch [9163/21340], Train Loss: 3.2436, Train Perplexity: 25.6259\n",
            "Epoch [1/3], Batch [9164/21340], Train Loss: 3.1165, Train Perplexity: 22.5675\n",
            "Epoch [1/3], Batch [9165/21340], Train Loss: 3.2332, Train Perplexity: 25.3613\n",
            "Epoch [1/3], Batch [9166/21340], Train Loss: 3.2378, Train Perplexity: 25.4783\n",
            "Epoch [1/3], Batch [9167/21340], Train Loss: 3.2608, Train Perplexity: 26.0691\n",
            "Epoch [1/3], Batch [9168/21340], Train Loss: 3.3919, Train Perplexity: 29.7226\n",
            "Epoch [1/3], Batch [9169/21340], Train Loss: 3.2417, Train Perplexity: 25.5782\n",
            "Epoch [1/3], Batch [9170/21340], Train Loss: 3.2111, Train Perplexity: 24.8059\n",
            "Epoch [1/3], Batch [9171/21340], Train Loss: 3.2217, Train Perplexity: 25.0696\n",
            "Epoch [1/3], Batch [9172/21340], Train Loss: 3.2618, Train Perplexity: 26.0956\n",
            "Epoch [1/3], Batch [9173/21340], Train Loss: 3.2295, Train Perplexity: 25.2666\n",
            "Epoch [1/3], Batch [9174/21340], Train Loss: 3.2617, Train Perplexity: 26.0948\n",
            "Epoch [1/3], Batch [9175/21340], Train Loss: 3.1738, Train Perplexity: 23.8976\n",
            "Epoch [1/3], Batch [9176/21340], Train Loss: 3.5026, Train Perplexity: 33.2011\n",
            "Epoch [1/3], Batch [9177/21340], Train Loss: 3.1486, Train Perplexity: 23.3045\n",
            "Epoch [1/3], Batch [9178/21340], Train Loss: 3.2892, Train Perplexity: 26.8208\n",
            "Epoch [1/3], Batch [9179/21340], Train Loss: 3.1374, Train Perplexity: 23.0447\n",
            "Epoch [1/3], Batch [9180/21340], Train Loss: 3.2614, Train Perplexity: 26.0868\n",
            "Epoch [1/3], Batch [9181/21340], Train Loss: 3.2519, Train Perplexity: 25.8383\n",
            "Epoch [1/3], Batch [9182/21340], Train Loss: 3.4449, Train Perplexity: 31.3411\n",
            "Epoch [1/3], Batch [9183/21340], Train Loss: 3.2882, Train Perplexity: 26.7944\n",
            "Epoch [1/3], Batch [9184/21340], Train Loss: 3.2794, Train Perplexity: 26.5604\n",
            "Epoch [1/3], Batch [9185/21340], Train Loss: 3.1596, Train Perplexity: 23.5616\n",
            "Epoch [1/3], Batch [9186/21340], Train Loss: 3.2095, Train Perplexity: 24.7656\n",
            "Epoch [1/3], Batch [9187/21340], Train Loss: 3.3645, Train Perplexity: 28.9189\n",
            "Epoch [1/3], Batch [9188/21340], Train Loss: 3.1804, Train Perplexity: 24.0564\n",
            "Epoch [1/3], Batch [9189/21340], Train Loss: 3.2371, Train Perplexity: 25.4591\n",
            "Epoch [1/3], Batch [9190/21340], Train Loss: 3.2404, Train Perplexity: 25.5438\n",
            "Epoch [1/3], Batch [9191/21340], Train Loss: 3.2020, Train Perplexity: 24.5819\n",
            "Epoch [1/3], Batch [9192/21340], Train Loss: 3.2242, Train Perplexity: 25.1337\n",
            "Epoch [1/3], Batch [9193/21340], Train Loss: 3.2975, Train Perplexity: 27.0457\n",
            "Epoch [1/3], Batch [9194/21340], Train Loss: 3.2451, Train Perplexity: 25.6632\n",
            "Epoch [1/3], Batch [9195/21340], Train Loss: 3.2788, Train Perplexity: 26.5441\n",
            "Epoch [1/3], Batch [9196/21340], Train Loss: 3.2952, Train Perplexity: 26.9837\n",
            "Epoch [1/3], Batch [9197/21340], Train Loss: 3.3374, Train Perplexity: 28.1450\n",
            "Epoch [1/3], Batch [9198/21340], Train Loss: 3.2499, Train Perplexity: 25.7869\n",
            "Epoch [1/3], Batch [9199/21340], Train Loss: 3.1749, Train Perplexity: 23.9242\n",
            "Epoch [1/3], Batch [9200/21340], Train Loss: 3.2365, Train Perplexity: 25.4447\n",
            "Epoch [1/3], Batch [9201/21340], Train Loss: 3.3620, Train Perplexity: 28.8463\n",
            "Epoch [1/3], Batch [9202/21340], Train Loss: 3.3223, Train Perplexity: 27.7230\n",
            "Epoch [1/3], Batch [9203/21340], Train Loss: 3.2336, Train Perplexity: 25.3719\n",
            "Epoch [1/3], Batch [9204/21340], Train Loss: 3.2165, Train Perplexity: 24.9415\n",
            "Epoch [1/3], Batch [9205/21340], Train Loss: 3.2686, Train Perplexity: 26.2734\n",
            "Epoch [1/3], Batch [9206/21340], Train Loss: 3.2108, Train Perplexity: 24.8000\n",
            "Epoch [1/3], Batch [9207/21340], Train Loss: 3.2582, Train Perplexity: 26.0028\n",
            "Epoch [1/3], Batch [9208/21340], Train Loss: 3.3362, Train Perplexity: 28.1128\n",
            "Epoch [1/3], Batch [9209/21340], Train Loss: 3.2594, Train Perplexity: 26.0349\n",
            "Epoch [1/3], Batch [9210/21340], Train Loss: 3.3831, Train Perplexity: 29.4611\n",
            "Epoch [1/3], Batch [9211/21340], Train Loss: 3.3826, Train Perplexity: 29.4474\n",
            "Epoch [1/3], Batch [9212/21340], Train Loss: 3.2485, Train Perplexity: 25.7515\n",
            "Epoch [1/3], Batch [9213/21340], Train Loss: 3.4701, Train Perplexity: 32.1413\n",
            "Epoch [1/3], Batch [9214/21340], Train Loss: 3.2145, Train Perplexity: 24.8918\n",
            "Epoch [1/3], Batch [9215/21340], Train Loss: 3.1267, Train Perplexity: 22.7992\n",
            "Epoch [1/3], Batch [9216/21340], Train Loss: 3.2327, Train Perplexity: 25.3487\n",
            "Epoch [1/3], Batch [9217/21340], Train Loss: 3.2542, Train Perplexity: 25.8999\n",
            "Epoch [1/3], Batch [9218/21340], Train Loss: 3.2780, Train Perplexity: 26.5219\n",
            "Epoch [1/3], Batch [9219/21340], Train Loss: 3.2954, Train Perplexity: 26.9891\n",
            "Epoch [1/3], Batch [9220/21340], Train Loss: 3.2366, Train Perplexity: 25.4472\n",
            "Epoch [1/3], Batch [9221/21340], Train Loss: 3.2622, Train Perplexity: 26.1071\n",
            "Epoch [1/3], Batch [9222/21340], Train Loss: 3.2248, Train Perplexity: 25.1496\n",
            "Epoch [1/3], Batch [9223/21340], Train Loss: 3.2992, Train Perplexity: 27.0921\n",
            "Epoch [1/3], Batch [9224/21340], Train Loss: 3.3323, Train Perplexity: 28.0023\n",
            "Epoch [1/3], Batch [9225/21340], Train Loss: 3.3005, Train Perplexity: 27.1273\n",
            "Epoch [1/3], Batch [9226/21340], Train Loss: 3.2071, Train Perplexity: 24.7067\n",
            "Epoch [1/3], Batch [9227/21340], Train Loss: 3.2624, Train Perplexity: 26.1123\n",
            "Epoch [1/3], Batch [9228/21340], Train Loss: 3.2869, Train Perplexity: 26.7604\n",
            "Epoch [1/3], Batch [9229/21340], Train Loss: 3.2559, Train Perplexity: 25.9429\n",
            "Epoch [1/3], Batch [9230/21340], Train Loss: 3.2213, Train Perplexity: 25.0603\n",
            "Epoch [1/3], Batch [9231/21340], Train Loss: 3.2286, Train Perplexity: 25.2444\n",
            "Epoch [1/3], Batch [9232/21340], Train Loss: 3.2317, Train Perplexity: 25.3219\n",
            "Epoch [1/3], Batch [9233/21340], Train Loss: 3.1996, Train Perplexity: 24.5221\n",
            "Epoch [1/3], Batch [9234/21340], Train Loss: 3.1943, Train Perplexity: 24.3934\n",
            "Epoch [1/3], Batch [9235/21340], Train Loss: 3.2276, Train Perplexity: 25.2196\n",
            "Epoch [1/3], Batch [9236/21340], Train Loss: 3.2032, Train Perplexity: 24.6101\n",
            "Epoch [1/3], Batch [9237/21340], Train Loss: 3.2042, Train Perplexity: 24.6367\n",
            "Epoch [1/3], Batch [9238/21340], Train Loss: 3.2405, Train Perplexity: 25.5466\n",
            "Epoch [1/3], Batch [9239/21340], Train Loss: 3.3613, Train Perplexity: 28.8271\n",
            "Epoch [1/3], Batch [9240/21340], Train Loss: 3.2038, Train Perplexity: 24.6263\n",
            "Epoch [1/3], Batch [9241/21340], Train Loss: 3.4433, Train Perplexity: 31.2886\n",
            "Epoch [1/3], Batch [9242/21340], Train Loss: 3.3744, Train Perplexity: 29.2070\n",
            "Epoch [1/3], Batch [9243/21340], Train Loss: 3.2653, Train Perplexity: 26.1892\n",
            "Epoch [1/3], Batch [9244/21340], Train Loss: 3.2386, Train Perplexity: 25.4976\n",
            "Epoch [1/3], Batch [9245/21340], Train Loss: 3.3920, Train Perplexity: 29.7258\n",
            "Epoch [1/3], Batch [9246/21340], Train Loss: 3.2385, Train Perplexity: 25.4962\n",
            "Epoch [1/3], Batch [9247/21340], Train Loss: 3.2333, Train Perplexity: 25.3644\n",
            "Epoch [1/3], Batch [9248/21340], Train Loss: 3.2750, Train Perplexity: 26.4439\n",
            "Epoch [1/3], Batch [9249/21340], Train Loss: 3.3060, Train Perplexity: 27.2766\n",
            "Epoch [1/3], Batch [9250/21340], Train Loss: 3.3221, Train Perplexity: 27.7192\n",
            "Epoch [1/3], Batch [9251/21340], Train Loss: 3.2106, Train Perplexity: 24.7938\n",
            "Epoch [1/3], Batch [9252/21340], Train Loss: 3.3055, Train Perplexity: 27.2628\n",
            "Epoch [1/3], Batch [9253/21340], Train Loss: 3.3595, Train Perplexity: 28.7746\n",
            "Epoch [1/3], Batch [9254/21340], Train Loss: 3.2608, Train Perplexity: 26.0702\n",
            "Epoch [1/3], Batch [9255/21340], Train Loss: 3.1752, Train Perplexity: 23.9311\n",
            "Epoch [1/3], Batch [9256/21340], Train Loss: 3.2186, Train Perplexity: 24.9924\n",
            "Epoch [1/3], Batch [9257/21340], Train Loss: 3.2157, Train Perplexity: 24.9217\n",
            "Epoch [1/3], Batch [9258/21340], Train Loss: 3.2187, Train Perplexity: 24.9949\n",
            "Epoch [1/3], Batch [9259/21340], Train Loss: 3.2931, Train Perplexity: 26.9265\n",
            "Epoch [1/3], Batch [9260/21340], Train Loss: 3.3159, Train Perplexity: 27.5486\n",
            "Epoch [1/3], Batch [9261/21340], Train Loss: 3.2761, Train Perplexity: 26.4715\n",
            "Epoch [1/3], Batch [9262/21340], Train Loss: 3.2444, Train Perplexity: 25.6451\n",
            "Epoch [1/3], Batch [9263/21340], Train Loss: 3.2797, Train Perplexity: 26.5676\n",
            "Epoch [1/3], Batch [9264/21340], Train Loss: 3.1601, Train Perplexity: 23.5740\n",
            "Epoch [1/3], Batch [9265/21340], Train Loss: 3.1486, Train Perplexity: 23.3026\n",
            "Epoch [1/3], Batch [9266/21340], Train Loss: 3.2962, Train Perplexity: 27.0087\n",
            "Epoch [1/3], Batch [9267/21340], Train Loss: 3.1630, Train Perplexity: 23.6416\n",
            "Epoch [1/3], Batch [9268/21340], Train Loss: 3.2409, Train Perplexity: 25.5564\n",
            "Epoch [1/3], Batch [9269/21340], Train Loss: 3.2195, Train Perplexity: 25.0163\n",
            "Epoch [1/3], Batch [9270/21340], Train Loss: 3.3174, Train Perplexity: 27.5888\n",
            "Epoch [1/3], Batch [9271/21340], Train Loss: 3.1653, Train Perplexity: 23.6947\n",
            "Epoch [1/3], Batch [9272/21340], Train Loss: 3.3146, Train Perplexity: 27.5118\n",
            "Epoch [1/3], Batch [9273/21340], Train Loss: 3.3062, Train Perplexity: 27.2810\n",
            "Epoch [1/3], Batch [9274/21340], Train Loss: 3.2527, Train Perplexity: 25.8588\n",
            "Epoch [1/3], Batch [9275/21340], Train Loss: 3.2195, Train Perplexity: 25.0144\n",
            "Epoch [1/3], Batch [9276/21340], Train Loss: 3.2014, Train Perplexity: 24.5680\n",
            "Epoch [1/3], Batch [9277/21340], Train Loss: 3.3043, Train Perplexity: 27.2299\n",
            "Epoch [1/3], Batch [9278/21340], Train Loss: 3.3833, Train Perplexity: 29.4670\n",
            "Epoch [1/3], Batch [9279/21340], Train Loss: 3.5422, Train Perplexity: 34.5432\n",
            "Epoch [1/3], Batch [9280/21340], Train Loss: 3.3295, Train Perplexity: 27.9239\n",
            "Epoch [1/3], Batch [9281/21340], Train Loss: 3.1896, Train Perplexity: 24.2780\n",
            "Epoch [1/3], Batch [9282/21340], Train Loss: 3.1941, Train Perplexity: 24.3878\n",
            "Epoch [1/3], Batch [9283/21340], Train Loss: 3.3263, Train Perplexity: 27.8358\n",
            "Epoch [1/3], Batch [9284/21340], Train Loss: 3.3108, Train Perplexity: 27.4068\n",
            "Epoch [1/3], Batch [9285/21340], Train Loss: 3.3925, Train Perplexity: 29.7396\n",
            "Epoch [1/3], Batch [9286/21340], Train Loss: 3.3138, Train Perplexity: 27.4887\n",
            "Epoch [1/3], Batch [9287/21340], Train Loss: 3.2116, Train Perplexity: 24.8200\n",
            "Epoch [1/3], Batch [9288/21340], Train Loss: 3.2466, Train Perplexity: 25.7020\n",
            "Epoch [1/3], Batch [9289/21340], Train Loss: 3.2584, Train Perplexity: 26.0070\n",
            "Epoch [1/3], Batch [9290/21340], Train Loss: 3.3183, Train Perplexity: 27.6134\n",
            "Epoch [1/3], Batch [9291/21340], Train Loss: 3.3206, Train Perplexity: 27.6773\n",
            "Epoch [1/3], Batch [9292/21340], Train Loss: 3.1630, Train Perplexity: 23.6418\n",
            "Epoch [1/3], Batch [9293/21340], Train Loss: 3.3468, Train Perplexity: 28.4125\n",
            "Epoch [1/3], Batch [9294/21340], Train Loss: 3.1457, Train Perplexity: 23.2370\n",
            "Epoch [1/3], Batch [9295/21340], Train Loss: 3.2126, Train Perplexity: 24.8441\n",
            "Epoch [1/3], Batch [9296/21340], Train Loss: 3.2668, Train Perplexity: 26.2279\n",
            "Epoch [1/3], Batch [9297/21340], Train Loss: 3.2099, Train Perplexity: 24.7775\n",
            "Epoch [1/3], Batch [9298/21340], Train Loss: 3.2172, Train Perplexity: 24.9578\n",
            "Epoch [1/3], Batch [9299/21340], Train Loss: 3.3433, Train Perplexity: 28.3128\n",
            "Epoch [1/3], Batch [9300/21340], Train Loss: 3.2362, Train Perplexity: 25.4365\n",
            "Epoch [1/3], Batch [9301/21340], Train Loss: 3.3498, Train Perplexity: 28.4972\n",
            "Epoch [1/3], Batch [9302/21340], Train Loss: 3.2450, Train Perplexity: 25.6619\n",
            "Epoch [1/3], Batch [9303/21340], Train Loss: 3.3069, Train Perplexity: 27.3011\n",
            "Epoch [1/3], Batch [9304/21340], Train Loss: 3.3429, Train Perplexity: 28.3021\n",
            "Epoch [1/3], Batch [9305/21340], Train Loss: 3.2545, Train Perplexity: 25.9076\n",
            "Epoch [1/3], Batch [9306/21340], Train Loss: 3.2613, Train Perplexity: 26.0841\n",
            "Epoch [1/3], Batch [9307/21340], Train Loss: 3.2958, Train Perplexity: 27.0001\n",
            "Epoch [1/3], Batch [9308/21340], Train Loss: 3.3109, Train Perplexity: 27.4103\n",
            "Epoch [1/3], Batch [9309/21340], Train Loss: 3.3281, Train Perplexity: 27.8851\n",
            "Epoch [1/3], Batch [9310/21340], Train Loss: 3.2608, Train Perplexity: 26.0701\n",
            "Epoch [1/3], Batch [9311/21340], Train Loss: 3.2883, Train Perplexity: 26.7983\n",
            "Epoch [1/3], Batch [9312/21340], Train Loss: 3.2756, Train Perplexity: 26.4594\n",
            "Epoch [1/3], Batch [9313/21340], Train Loss: 3.1887, Train Perplexity: 24.2559\n",
            "Epoch [1/3], Batch [9314/21340], Train Loss: 3.2624, Train Perplexity: 26.1118\n",
            "Epoch [1/3], Batch [9315/21340], Train Loss: 3.1789, Train Perplexity: 24.0195\n",
            "Epoch [1/3], Batch [9316/21340], Train Loss: 3.3067, Train Perplexity: 27.2944\n",
            "Epoch [1/3], Batch [9317/21340], Train Loss: 3.1985, Train Perplexity: 24.4960\n",
            "Epoch [1/3], Batch [9318/21340], Train Loss: 3.2532, Train Perplexity: 25.8734\n",
            "Epoch [1/3], Batch [9319/21340], Train Loss: 3.2809, Train Perplexity: 26.5994\n",
            "Epoch [1/3], Batch [9320/21340], Train Loss: 3.2091, Train Perplexity: 24.7579\n",
            "Epoch [1/3], Batch [9321/21340], Train Loss: 3.2020, Train Perplexity: 24.5810\n",
            "Epoch [1/3], Batch [9322/21340], Train Loss: 3.2446, Train Perplexity: 25.6513\n",
            "Epoch [1/3], Batch [9323/21340], Train Loss: 3.2805, Train Perplexity: 26.5891\n",
            "Epoch [1/3], Batch [9324/21340], Train Loss: 3.2073, Train Perplexity: 24.7123\n",
            "Epoch [1/3], Batch [9325/21340], Train Loss: 3.2504, Train Perplexity: 25.7996\n",
            "Epoch [1/3], Batch [9326/21340], Train Loss: 3.4686, Train Perplexity: 32.0924\n",
            "Epoch [1/3], Batch [9327/21340], Train Loss: 3.2249, Train Perplexity: 25.1522\n",
            "Epoch [1/3], Batch [9328/21340], Train Loss: 3.1923, Train Perplexity: 24.3434\n",
            "Epoch [1/3], Batch [9329/21340], Train Loss: 3.3414, Train Perplexity: 28.2582\n",
            "Epoch [1/3], Batch [9330/21340], Train Loss: 3.2286, Train Perplexity: 25.2444\n",
            "Epoch [1/3], Batch [9331/21340], Train Loss: 3.3081, Train Perplexity: 27.3329\n",
            "Epoch [1/3], Batch [9332/21340], Train Loss: 3.2810, Train Perplexity: 26.6033\n",
            "Epoch [1/3], Batch [9333/21340], Train Loss: 3.3818, Train Perplexity: 29.4250\n",
            "Epoch [1/3], Batch [9334/21340], Train Loss: 3.2587, Train Perplexity: 26.0150\n",
            "Epoch [1/3], Batch [9335/21340], Train Loss: 3.1749, Train Perplexity: 23.9234\n",
            "Epoch [1/3], Batch [9336/21340], Train Loss: 3.2628, Train Perplexity: 26.1227\n",
            "Epoch [1/3], Batch [9337/21340], Train Loss: 3.1772, Train Perplexity: 23.9787\n",
            "Epoch [1/3], Batch [9338/21340], Train Loss: 3.2536, Train Perplexity: 25.8837\n",
            "Epoch [1/3], Batch [9339/21340], Train Loss: 3.2651, Train Perplexity: 26.1816\n",
            "Epoch [1/3], Batch [9340/21340], Train Loss: 3.3012, Train Perplexity: 27.1457\n",
            "Epoch [1/3], Batch [9341/21340], Train Loss: 3.2623, Train Perplexity: 26.1097\n",
            "Epoch [1/3], Batch [9342/21340], Train Loss: 3.2818, Train Perplexity: 26.6228\n",
            "Epoch [1/3], Batch [9343/21340], Train Loss: 3.1812, Train Perplexity: 24.0765\n",
            "Epoch [1/3], Batch [9344/21340], Train Loss: 3.2782, Train Perplexity: 26.5285\n",
            "Epoch [1/3], Batch [9345/21340], Train Loss: 3.1911, Train Perplexity: 24.3159\n",
            "Epoch [1/3], Batch [9346/21340], Train Loss: 3.2882, Train Perplexity: 26.7939\n",
            "Epoch [1/3], Batch [9347/21340], Train Loss: 3.2557, Train Perplexity: 25.9372\n",
            "Epoch [1/3], Batch [9348/21340], Train Loss: 3.2053, Train Perplexity: 24.6620\n",
            "Epoch [1/3], Batch [9349/21340], Train Loss: 3.2250, Train Perplexity: 25.1542\n",
            "Epoch [1/3], Batch [9350/21340], Train Loss: 3.1652, Train Perplexity: 23.6936\n",
            "Epoch [1/3], Batch [9351/21340], Train Loss: 3.3253, Train Perplexity: 27.8081\n",
            "Epoch [1/3], Batch [9352/21340], Train Loss: 3.2377, Train Perplexity: 25.4743\n",
            "Epoch [1/3], Batch [9353/21340], Train Loss: 3.3888, Train Perplexity: 29.6310\n",
            "Epoch [1/3], Batch [9354/21340], Train Loss: 3.3437, Train Perplexity: 28.3239\n",
            "Epoch [1/3], Batch [9355/21340], Train Loss: 3.1911, Train Perplexity: 24.3157\n",
            "Epoch [1/3], Batch [9356/21340], Train Loss: 3.1921, Train Perplexity: 24.3403\n",
            "Epoch [1/3], Batch [9357/21340], Train Loss: 3.1558, Train Perplexity: 23.4715\n",
            "Epoch [1/3], Batch [9358/21340], Train Loss: 3.2146, Train Perplexity: 24.8932\n",
            "Epoch [1/3], Batch [9359/21340], Train Loss: 3.1583, Train Perplexity: 23.5299\n",
            "Epoch [1/3], Batch [9360/21340], Train Loss: 3.2023, Train Perplexity: 24.5878\n",
            "Epoch [1/3], Batch [9361/21340], Train Loss: 3.1706, Train Perplexity: 23.8210\n",
            "Epoch [1/3], Batch [9362/21340], Train Loss: 3.2888, Train Perplexity: 26.8096\n",
            "Epoch [1/3], Batch [9363/21340], Train Loss: 3.3476, Train Perplexity: 28.4344\n",
            "Epoch [1/3], Batch [9364/21340], Train Loss: 3.3395, Train Perplexity: 28.2046\n",
            "Epoch [1/3], Batch [9365/21340], Train Loss: 3.1876, Train Perplexity: 24.2300\n",
            "Epoch [1/3], Batch [9366/21340], Train Loss: 3.1925, Train Perplexity: 24.3488\n",
            "Epoch [1/3], Batch [9367/21340], Train Loss: 3.1874, Train Perplexity: 24.2265\n",
            "Epoch [1/3], Batch [9368/21340], Train Loss: 3.1384, Train Perplexity: 23.0662\n",
            "Epoch [1/3], Batch [9369/21340], Train Loss: 3.2257, Train Perplexity: 25.1717\n",
            "Epoch [1/3], Batch [9370/21340], Train Loss: 3.3733, Train Perplexity: 29.1750\n",
            "Epoch [1/3], Batch [9371/21340], Train Loss: 3.3527, Train Perplexity: 28.5799\n",
            "Epoch [1/3], Batch [9372/21340], Train Loss: 3.2034, Train Perplexity: 24.6171\n",
            "Epoch [1/3], Batch [9373/21340], Train Loss: 3.1501, Train Perplexity: 23.3374\n",
            "Epoch [1/3], Batch [9374/21340], Train Loss: 3.3403, Train Perplexity: 28.2279\n",
            "Epoch [1/3], Batch [9375/21340], Train Loss: 3.2718, Train Perplexity: 26.3579\n",
            "Epoch [1/3], Batch [9376/21340], Train Loss: 3.2680, Train Perplexity: 26.2593\n",
            "Epoch [1/3], Batch [9377/21340], Train Loss: 3.1911, Train Perplexity: 24.3143\n",
            "Epoch [1/3], Batch [9378/21340], Train Loss: 3.3366, Train Perplexity: 28.1245\n",
            "Epoch [1/3], Batch [9379/21340], Train Loss: 3.1879, Train Perplexity: 24.2382\n",
            "Epoch [1/3], Batch [9380/21340], Train Loss: 3.2251, Train Perplexity: 25.1550\n",
            "Epoch [1/3], Batch [9381/21340], Train Loss: 3.2845, Train Perplexity: 26.6955\n",
            "Epoch [1/3], Batch [9382/21340], Train Loss: 3.2801, Train Perplexity: 26.5773\n",
            "Epoch [1/3], Batch [9383/21340], Train Loss: 3.1756, Train Perplexity: 23.9418\n",
            "Epoch [1/3], Batch [9384/21340], Train Loss: 3.3185, Train Perplexity: 27.6184\n",
            "Epoch [1/3], Batch [9385/21340], Train Loss: 3.3368, Train Perplexity: 28.1289\n",
            "Epoch [1/3], Batch [9386/21340], Train Loss: 3.1294, Train Perplexity: 22.8604\n",
            "Epoch [1/3], Batch [9387/21340], Train Loss: 3.2503, Train Perplexity: 25.7982\n",
            "Epoch [1/3], Batch [9388/21340], Train Loss: 3.3134, Train Perplexity: 27.4778\n",
            "Epoch [1/3], Batch [9389/21340], Train Loss: 3.2622, Train Perplexity: 26.1061\n",
            "Epoch [1/3], Batch [9390/21340], Train Loss: 3.2882, Train Perplexity: 26.7941\n",
            "Epoch [1/3], Batch [9391/21340], Train Loss: 3.3182, Train Perplexity: 27.6100\n",
            "Epoch [1/3], Batch [9392/21340], Train Loss: 3.1612, Train Perplexity: 23.5996\n",
            "Epoch [1/3], Batch [9393/21340], Train Loss: 3.2656, Train Perplexity: 26.1958\n",
            "Epoch [1/3], Batch [9394/21340], Train Loss: 3.2630, Train Perplexity: 26.1285\n",
            "Epoch [1/3], Batch [9395/21340], Train Loss: 3.4119, Train Perplexity: 30.3222\n",
            "Epoch [1/3], Batch [9396/21340], Train Loss: 3.1904, Train Perplexity: 24.2983\n",
            "Epoch [1/3], Batch [9397/21340], Train Loss: 3.3912, Train Perplexity: 29.7014\n",
            "Epoch [1/3], Batch [9398/21340], Train Loss: 3.2536, Train Perplexity: 25.8825\n",
            "Epoch [1/3], Batch [9399/21340], Train Loss: 3.1881, Train Perplexity: 24.2412\n",
            "Epoch [1/3], Batch [9400/21340], Train Loss: 3.2935, Train Perplexity: 26.9364\n",
            "Epoch [1/3], Batch [9401/21340], Train Loss: 3.3068, Train Perplexity: 27.2979\n",
            "Epoch [1/3], Batch [9402/21340], Train Loss: 3.2631, Train Perplexity: 26.1303\n",
            "Epoch [1/3], Batch [9403/21340], Train Loss: 3.1991, Train Perplexity: 24.5100\n",
            "Epoch [1/3], Batch [9404/21340], Train Loss: 3.2131, Train Perplexity: 24.8562\n",
            "Epoch [1/3], Batch [9405/21340], Train Loss: 3.1912, Train Perplexity: 24.3167\n",
            "Epoch [1/3], Batch [9406/21340], Train Loss: 3.2133, Train Perplexity: 24.8619\n",
            "Epoch [1/3], Batch [9407/21340], Train Loss: 3.2861, Train Perplexity: 26.7376\n",
            "Epoch [1/3], Batch [9408/21340], Train Loss: 3.3201, Train Perplexity: 27.6630\n",
            "Epoch [1/3], Batch [9409/21340], Train Loss: 3.1745, Train Perplexity: 23.9153\n",
            "Epoch [1/3], Batch [9410/21340], Train Loss: 3.1837, Train Perplexity: 24.1348\n",
            "Epoch [1/3], Batch [9411/21340], Train Loss: 3.2851, Train Perplexity: 26.7116\n",
            "Epoch [1/3], Batch [9412/21340], Train Loss: 3.2875, Train Perplexity: 26.7759\n",
            "Epoch [1/3], Batch [9413/21340], Train Loss: 3.3324, Train Perplexity: 28.0047\n",
            "Epoch [1/3], Batch [9414/21340], Train Loss: 3.3443, Train Perplexity: 28.3420\n",
            "Epoch [1/3], Batch [9415/21340], Train Loss: 3.3308, Train Perplexity: 27.9602\n",
            "Epoch [1/3], Batch [9416/21340], Train Loss: 3.2535, Train Perplexity: 25.8804\n",
            "Epoch [1/3], Batch [9417/21340], Train Loss: 3.2584, Train Perplexity: 26.0069\n",
            "Epoch [1/3], Batch [9418/21340], Train Loss: 3.1554, Train Perplexity: 23.4631\n",
            "Epoch [1/3], Batch [9419/21340], Train Loss: 3.1954, Train Perplexity: 24.4209\n",
            "Epoch [1/3], Batch [9420/21340], Train Loss: 3.2327, Train Perplexity: 25.3488\n",
            "Epoch [1/3], Batch [9421/21340], Train Loss: 3.1829, Train Perplexity: 24.1169\n",
            "Epoch [1/3], Batch [9422/21340], Train Loss: 3.4156, Train Perplexity: 30.4359\n",
            "Epoch [1/3], Batch [9423/21340], Train Loss: 3.2166, Train Perplexity: 24.9431\n",
            "Epoch [1/3], Batch [9424/21340], Train Loss: 3.2702, Train Perplexity: 26.3163\n",
            "Epoch [1/3], Batch [9425/21340], Train Loss: 3.1840, Train Perplexity: 24.1424\n",
            "Epoch [1/3], Batch [9426/21340], Train Loss: 3.2667, Train Perplexity: 26.2245\n",
            "Epoch [1/3], Batch [9427/21340], Train Loss: 3.3279, Train Perplexity: 27.8787\n",
            "Epoch [1/3], Batch [9428/21340], Train Loss: 3.2486, Train Perplexity: 25.7545\n",
            "Epoch [1/3], Batch [9429/21340], Train Loss: 3.3244, Train Perplexity: 27.7811\n",
            "Epoch [1/3], Batch [9430/21340], Train Loss: 3.2536, Train Perplexity: 25.8839\n",
            "Epoch [1/3], Batch [9431/21340], Train Loss: 3.2241, Train Perplexity: 25.1307\n",
            "Epoch [1/3], Batch [9432/21340], Train Loss: 3.2571, Train Perplexity: 25.9735\n",
            "Epoch [1/3], Batch [9433/21340], Train Loss: 3.1664, Train Perplexity: 23.7219\n",
            "Epoch [1/3], Batch [9434/21340], Train Loss: 3.2381, Train Perplexity: 25.4849\n",
            "Epoch [1/3], Batch [9435/21340], Train Loss: 3.1809, Train Perplexity: 24.0683\n",
            "Epoch [1/3], Batch [9436/21340], Train Loss: 3.2617, Train Perplexity: 26.0935\n",
            "Epoch [1/3], Batch [9437/21340], Train Loss: 3.2462, Train Perplexity: 25.6926\n",
            "Epoch [1/3], Batch [9438/21340], Train Loss: 3.2063, Train Perplexity: 24.6884\n",
            "Epoch [1/3], Batch [9439/21340], Train Loss: 3.3155, Train Perplexity: 27.5365\n",
            "Epoch [1/3], Batch [9440/21340], Train Loss: 3.2800, Train Perplexity: 26.5760\n",
            "Epoch [1/3], Batch [9441/21340], Train Loss: 3.2916, Train Perplexity: 26.8853\n",
            "Epoch [1/3], Batch [9442/21340], Train Loss: 3.2313, Train Perplexity: 25.3117\n",
            "Epoch [1/3], Batch [9443/21340], Train Loss: 3.3112, Train Perplexity: 27.4189\n",
            "Epoch [1/3], Batch [9444/21340], Train Loss: 3.2787, Train Perplexity: 26.5400\n",
            "Epoch [1/3], Batch [9445/21340], Train Loss: 3.2740, Train Perplexity: 26.4160\n",
            "Epoch [1/3], Batch [9446/21340], Train Loss: 3.1536, Train Perplexity: 23.4206\n",
            "Epoch [1/3], Batch [9447/21340], Train Loss: 3.2032, Train Perplexity: 24.6120\n",
            "Epoch [1/3], Batch [9448/21340], Train Loss: 3.2954, Train Perplexity: 26.9892\n",
            "Epoch [1/3], Batch [9449/21340], Train Loss: 3.2102, Train Perplexity: 24.7838\n",
            "Epoch [1/3], Batch [9450/21340], Train Loss: 3.3655, Train Perplexity: 28.9469\n",
            "Epoch [1/3], Batch [9451/21340], Train Loss: 3.3018, Train Perplexity: 27.1606\n",
            "Epoch [1/3], Batch [9452/21340], Train Loss: 3.1883, Train Perplexity: 24.2475\n",
            "Epoch [1/3], Batch [9453/21340], Train Loss: 3.1903, Train Perplexity: 24.2953\n",
            "Epoch [1/3], Batch [9454/21340], Train Loss: 3.2829, Train Perplexity: 26.6517\n",
            "Epoch [1/3], Batch [9455/21340], Train Loss: 3.2655, Train Perplexity: 26.1941\n",
            "Epoch [1/3], Batch [9456/21340], Train Loss: 3.2045, Train Perplexity: 24.6425\n",
            "Epoch [1/3], Batch [9457/21340], Train Loss: 3.1956, Train Perplexity: 24.4251\n",
            "Epoch [1/3], Batch [9458/21340], Train Loss: 3.1487, Train Perplexity: 23.3066\n",
            "Epoch [1/3], Batch [9459/21340], Train Loss: 3.2813, Train Perplexity: 26.6101\n",
            "Epoch [1/3], Batch [9460/21340], Train Loss: 3.1760, Train Perplexity: 23.9517\n",
            "Epoch [1/3], Batch [9461/21340], Train Loss: 3.2749, Train Perplexity: 26.4406\n",
            "Epoch [1/3], Batch [9462/21340], Train Loss: 3.2778, Train Perplexity: 26.5183\n",
            "Epoch [1/3], Batch [9463/21340], Train Loss: 3.3714, Train Perplexity: 29.1190\n",
            "Epoch [1/3], Batch [9464/21340], Train Loss: 3.2431, Train Perplexity: 25.6124\n",
            "Epoch [1/3], Batch [9465/21340], Train Loss: 3.2050, Train Perplexity: 24.6544\n",
            "Epoch [1/3], Batch [9466/21340], Train Loss: 3.2195, Train Perplexity: 25.0151\n",
            "Epoch [1/3], Batch [9467/21340], Train Loss: 3.1392, Train Perplexity: 23.0858\n",
            "Epoch [1/3], Batch [9468/21340], Train Loss: 3.2193, Train Perplexity: 25.0106\n",
            "Epoch [1/3], Batch [9469/21340], Train Loss: 3.2802, Train Perplexity: 26.5806\n",
            "Epoch [1/3], Batch [9470/21340], Train Loss: 3.2297, Train Perplexity: 25.2726\n",
            "Epoch [1/3], Batch [9471/21340], Train Loss: 3.2994, Train Perplexity: 27.0971\n",
            "Epoch [1/3], Batch [9472/21340], Train Loss: 3.3222, Train Perplexity: 27.7225\n",
            "Epoch [1/3], Batch [9473/21340], Train Loss: 3.2154, Train Perplexity: 24.9121\n",
            "Epoch [1/3], Batch [9474/21340], Train Loss: 3.1634, Train Perplexity: 23.6503\n",
            "Epoch [1/3], Batch [9475/21340], Train Loss: 3.3778, Train Perplexity: 29.3053\n",
            "Epoch [1/3], Batch [9476/21340], Train Loss: 3.2614, Train Perplexity: 26.0857\n",
            "Epoch [1/3], Batch [9477/21340], Train Loss: 3.1873, Train Perplexity: 24.2225\n",
            "Epoch [1/3], Batch [9478/21340], Train Loss: 3.1578, Train Perplexity: 23.5179\n",
            "Epoch [1/3], Batch [9479/21340], Train Loss: 3.2972, Train Perplexity: 27.0377\n",
            "Epoch [1/3], Batch [9480/21340], Train Loss: 3.2587, Train Perplexity: 26.0146\n",
            "Epoch [1/3], Batch [9481/21340], Train Loss: 3.2025, Train Perplexity: 24.5942\n",
            "Epoch [1/3], Batch [9482/21340], Train Loss: 3.1886, Train Perplexity: 24.2538\n",
            "Epoch [1/3], Batch [9483/21340], Train Loss: 3.3792, Train Perplexity: 29.3474\n",
            "Epoch [1/3], Batch [9484/21340], Train Loss: 3.2532, Train Perplexity: 25.8736\n",
            "Epoch [1/3], Batch [9485/21340], Train Loss: 3.2094, Train Perplexity: 24.7649\n",
            "Epoch [1/3], Batch [9486/21340], Train Loss: 3.2759, Train Perplexity: 26.4664\n",
            "Epoch [1/3], Batch [9487/21340], Train Loss: 3.2360, Train Perplexity: 25.4312\n",
            "Epoch [1/3], Batch [9488/21340], Train Loss: 3.4891, Train Perplexity: 32.7566\n",
            "Epoch [1/3], Batch [9489/21340], Train Loss: 3.3179, Train Perplexity: 27.6015\n",
            "Epoch [1/3], Batch [9490/21340], Train Loss: 3.1984, Train Perplexity: 24.4934\n",
            "Epoch [1/3], Batch [9491/21340], Train Loss: 3.2310, Train Perplexity: 25.3050\n",
            "Epoch [1/3], Batch [9492/21340], Train Loss: 3.2056, Train Perplexity: 24.6713\n",
            "Epoch [1/3], Batch [9493/21340], Train Loss: 3.2649, Train Perplexity: 26.1764\n",
            "Epoch [1/3], Batch [9494/21340], Train Loss: 3.2899, Train Perplexity: 26.8389\n",
            "Epoch [1/3], Batch [9495/21340], Train Loss: 3.2971, Train Perplexity: 27.0341\n",
            "Epoch [1/3], Batch [9496/21340], Train Loss: 3.4207, Train Perplexity: 30.5907\n",
            "Epoch [1/3], Batch [9497/21340], Train Loss: 3.2491, Train Perplexity: 25.7669\n",
            "Epoch [1/3], Batch [9498/21340], Train Loss: 3.2332, Train Perplexity: 25.3617\n",
            "Epoch [1/3], Batch [9499/21340], Train Loss: 3.1711, Train Perplexity: 23.8343\n",
            "Epoch [1/3], Batch [9500/21340], Train Loss: 3.2770, Train Perplexity: 26.4960\n",
            "Epoch [1/3], Batch [9501/21340], Train Loss: 3.1821, Train Perplexity: 24.0983\n",
            "Epoch [1/3], Batch [9502/21340], Train Loss: 3.2215, Train Perplexity: 25.0666\n",
            "Epoch [1/3], Batch [9503/21340], Train Loss: 3.2715, Train Perplexity: 26.3498\n",
            "Epoch [1/3], Batch [9504/21340], Train Loss: 3.2319, Train Perplexity: 25.3274\n",
            "Epoch [1/3], Batch [9505/21340], Train Loss: 3.2721, Train Perplexity: 26.3667\n",
            "Epoch [1/3], Batch [9506/21340], Train Loss: 3.3157, Train Perplexity: 27.5430\n",
            "Epoch [1/3], Batch [9507/21340], Train Loss: 3.2001, Train Perplexity: 24.5348\n",
            "Epoch [1/3], Batch [9508/21340], Train Loss: 3.2801, Train Perplexity: 26.5796\n",
            "Epoch [1/3], Batch [9509/21340], Train Loss: 3.2947, Train Perplexity: 26.9694\n",
            "Epoch [1/3], Batch [9510/21340], Train Loss: 3.2443, Train Perplexity: 25.6426\n",
            "Epoch [1/3], Batch [9511/21340], Train Loss: 3.2867, Train Perplexity: 26.7532\n",
            "Epoch [1/3], Batch [9512/21340], Train Loss: 3.2585, Train Perplexity: 26.0095\n",
            "Epoch [1/3], Batch [9513/21340], Train Loss: 3.2433, Train Perplexity: 25.6172\n",
            "Epoch [1/3], Batch [9514/21340], Train Loss: 3.2766, Train Perplexity: 26.4853\n",
            "Epoch [1/3], Batch [9515/21340], Train Loss: 3.1521, Train Perplexity: 23.3861\n",
            "Epoch [1/3], Batch [9516/21340], Train Loss: 3.1829, Train Perplexity: 24.1178\n",
            "Epoch [1/3], Batch [9517/21340], Train Loss: 3.3459, Train Perplexity: 28.3853\n",
            "Epoch [1/3], Batch [9518/21340], Train Loss: 3.2521, Train Perplexity: 25.8444\n",
            "Epoch [1/3], Batch [9519/21340], Train Loss: 3.2636, Train Perplexity: 26.1447\n",
            "Epoch [1/3], Batch [9520/21340], Train Loss: 3.3146, Train Perplexity: 27.5115\n",
            "Epoch [1/3], Batch [9521/21340], Train Loss: 3.3136, Train Perplexity: 27.4845\n",
            "Epoch [1/3], Batch [9522/21340], Train Loss: 3.2262, Train Perplexity: 25.1837\n",
            "Epoch [1/3], Batch [9523/21340], Train Loss: 3.2930, Train Perplexity: 26.9247\n",
            "Epoch [1/3], Batch [9524/21340], Train Loss: 3.3832, Train Perplexity: 29.4655\n",
            "Epoch [1/3], Batch [9525/21340], Train Loss: 3.2477, Train Perplexity: 25.7322\n",
            "Epoch [1/3], Batch [9526/21340], Train Loss: 3.1841, Train Perplexity: 24.1443\n",
            "Epoch [1/3], Batch [9527/21340], Train Loss: 3.1632, Train Perplexity: 23.6463\n",
            "Epoch [1/3], Batch [9528/21340], Train Loss: 3.2939, Train Perplexity: 26.9482\n",
            "Epoch [1/3], Batch [9529/21340], Train Loss: 3.2534, Train Perplexity: 25.8785\n",
            "Epoch [1/3], Batch [9530/21340], Train Loss: 3.4153, Train Perplexity: 30.4249\n",
            "Epoch [1/3], Batch [9531/21340], Train Loss: 3.1762, Train Perplexity: 23.9561\n",
            "Epoch [1/3], Batch [9532/21340], Train Loss: 3.3696, Train Perplexity: 29.0665\n",
            "Epoch [1/3], Batch [9533/21340], Train Loss: 3.2822, Train Perplexity: 26.6341\n",
            "Epoch [1/3], Batch [9534/21340], Train Loss: 3.1889, Train Perplexity: 24.2618\n",
            "Epoch [1/3], Batch [9535/21340], Train Loss: 3.3742, Train Perplexity: 29.2007\n",
            "Epoch [1/3], Batch [9536/21340], Train Loss: 3.2396, Train Perplexity: 25.5243\n",
            "Epoch [1/3], Batch [9537/21340], Train Loss: 3.2248, Train Perplexity: 25.1484\n",
            "Epoch [1/3], Batch [9538/21340], Train Loss: 3.1997, Train Perplexity: 24.5260\n",
            "Epoch [1/3], Batch [9539/21340], Train Loss: 3.3018, Train Perplexity: 27.1607\n",
            "Epoch [1/3], Batch [9540/21340], Train Loss: 3.1935, Train Perplexity: 24.3740\n",
            "Epoch [1/3], Batch [9541/21340], Train Loss: 3.2882, Train Perplexity: 26.7951\n",
            "Epoch [1/3], Batch [9542/21340], Train Loss: 3.3473, Train Perplexity: 28.4271\n",
            "Epoch [1/3], Batch [9543/21340], Train Loss: 3.3338, Train Perplexity: 28.0440\n",
            "Epoch [1/3], Batch [9544/21340], Train Loss: 3.2308, Train Perplexity: 25.2996\n",
            "Epoch [1/3], Batch [9545/21340], Train Loss: 3.2867, Train Perplexity: 26.7531\n",
            "Epoch [1/3], Batch [9546/21340], Train Loss: 3.1651, Train Perplexity: 23.6901\n",
            "Epoch [1/3], Batch [9547/21340], Train Loss: 3.2564, Train Perplexity: 25.9547\n",
            "Epoch [1/3], Batch [9548/21340], Train Loss: 3.1322, Train Perplexity: 22.9239\n",
            "Epoch [1/3], Batch [9549/21340], Train Loss: 3.2266, Train Perplexity: 25.1927\n",
            "Epoch [1/3], Batch [9550/21340], Train Loss: 3.2830, Train Perplexity: 26.6567\n",
            "Epoch [1/3], Batch [9551/21340], Train Loss: 3.2284, Train Perplexity: 25.2392\n",
            "Epoch [1/3], Batch [9552/21340], Train Loss: 3.3127, Train Perplexity: 27.4585\n",
            "Epoch [1/3], Batch [9553/21340], Train Loss: 3.2167, Train Perplexity: 24.9469\n",
            "Epoch [1/3], Batch [9554/21340], Train Loss: 3.2644, Train Perplexity: 26.1636\n",
            "Epoch [1/3], Batch [9555/21340], Train Loss: 3.3345, Train Perplexity: 28.0650\n",
            "Epoch [1/3], Batch [9556/21340], Train Loss: 3.1720, Train Perplexity: 23.8545\n",
            "Epoch [1/3], Batch [9557/21340], Train Loss: 3.1995, Train Perplexity: 24.5198\n",
            "Epoch [1/3], Batch [9558/21340], Train Loss: 3.2411, Train Perplexity: 25.5615\n",
            "Epoch [1/3], Batch [9559/21340], Train Loss: 3.2852, Train Perplexity: 26.7142\n",
            "Epoch [1/3], Batch [9560/21340], Train Loss: 3.2725, Train Perplexity: 26.3759\n",
            "Epoch [1/3], Batch [9561/21340], Train Loss: 3.2770, Train Perplexity: 26.4960\n",
            "Epoch [1/3], Batch [9562/21340], Train Loss: 3.1853, Train Perplexity: 24.1750\n",
            "Epoch [1/3], Batch [9563/21340], Train Loss: 3.2915, Train Perplexity: 26.8826\n",
            "Epoch [1/3], Batch [9564/21340], Train Loss: 3.2370, Train Perplexity: 25.4575\n",
            "Epoch [1/3], Batch [9565/21340], Train Loss: 3.2573, Train Perplexity: 25.9785\n",
            "Epoch [1/3], Batch [9566/21340], Train Loss: 3.2131, Train Perplexity: 24.8565\n",
            "Epoch [1/3], Batch [9567/21340], Train Loss: 3.2983, Train Perplexity: 27.0656\n",
            "Epoch [1/3], Batch [9568/21340], Train Loss: 3.2723, Train Perplexity: 26.3709\n",
            "Epoch [1/3], Batch [9569/21340], Train Loss: 3.2642, Train Perplexity: 26.1604\n",
            "Epoch [1/3], Batch [9570/21340], Train Loss: 3.2981, Train Perplexity: 27.0611\n",
            "Epoch [1/3], Batch [9571/21340], Train Loss: 3.2038, Train Perplexity: 24.6258\n",
            "Epoch [1/3], Batch [9572/21340], Train Loss: 3.3225, Train Perplexity: 27.7287\n",
            "Epoch [1/3], Batch [9573/21340], Train Loss: 3.2645, Train Perplexity: 26.1683\n",
            "Epoch [1/3], Batch [9574/21340], Train Loss: 3.2654, Train Perplexity: 26.1896\n",
            "Epoch [1/3], Batch [9575/21340], Train Loss: 3.2869, Train Perplexity: 26.7587\n",
            "Epoch [1/3], Batch [9576/21340], Train Loss: 3.2880, Train Perplexity: 26.7881\n",
            "Epoch [1/3], Batch [9577/21340], Train Loss: 3.2912, Train Perplexity: 26.8745\n",
            "Epoch [1/3], Batch [9578/21340], Train Loss: 3.2246, Train Perplexity: 25.1447\n",
            "Epoch [1/3], Batch [9579/21340], Train Loss: 3.3768, Train Perplexity: 29.2757\n",
            "Epoch [1/3], Batch [9580/21340], Train Loss: 3.1226, Train Perplexity: 22.7060\n",
            "Epoch [1/3], Batch [9581/21340], Train Loss: 3.2265, Train Perplexity: 25.1919\n",
            "Epoch [1/3], Batch [9582/21340], Train Loss: 3.1972, Train Perplexity: 24.4644\n",
            "Epoch [1/3], Batch [9583/21340], Train Loss: 3.3106, Train Perplexity: 27.4024\n",
            "Epoch [1/3], Batch [9584/21340], Train Loss: 3.1683, Train Perplexity: 23.7674\n",
            "Epoch [1/3], Batch [9585/21340], Train Loss: 3.2872, Train Perplexity: 26.7670\n",
            "Epoch [1/3], Batch [9586/21340], Train Loss: 3.3532, Train Perplexity: 28.5944\n",
            "Epoch [1/3], Batch [9587/21340], Train Loss: 3.2821, Train Perplexity: 26.6323\n",
            "Epoch [1/3], Batch [9588/21340], Train Loss: 3.2622, Train Perplexity: 26.1076\n",
            "Epoch [1/3], Batch [9589/21340], Train Loss: 3.2591, Train Perplexity: 26.0256\n",
            "Epoch [1/3], Batch [9590/21340], Train Loss: 3.3194, Train Perplexity: 27.6425\n",
            "Epoch [1/3], Batch [9591/21340], Train Loss: 3.1650, Train Perplexity: 23.6883\n",
            "Epoch [1/3], Batch [9592/21340], Train Loss: 3.3491, Train Perplexity: 28.4777\n",
            "Epoch [1/3], Batch [9593/21340], Train Loss: 3.2883, Train Perplexity: 26.7978\n",
            "Epoch [1/3], Batch [9594/21340], Train Loss: 3.2633, Train Perplexity: 26.1348\n",
            "Epoch [1/3], Batch [9595/21340], Train Loss: 3.2738, Train Perplexity: 26.4115\n",
            "Epoch [1/3], Batch [9596/21340], Train Loss: 3.1290, Train Perplexity: 22.8514\n",
            "Epoch [1/3], Batch [9597/21340], Train Loss: 3.1889, Train Perplexity: 24.2621\n",
            "Epoch [1/3], Batch [9598/21340], Train Loss: 3.1904, Train Perplexity: 24.2985\n",
            "Epoch [1/3], Batch [9599/21340], Train Loss: 3.3928, Train Perplexity: 29.7477\n",
            "Epoch [1/3], Batch [9600/21340], Train Loss: 3.2214, Train Perplexity: 25.0630\n",
            "Epoch [1/3], Batch [9601/21340], Train Loss: 3.3208, Train Perplexity: 27.6825\n",
            "Epoch [1/3], Batch [9602/21340], Train Loss: 3.1472, Train Perplexity: 23.2719\n",
            "Epoch [1/3], Batch [9603/21340], Train Loss: 3.2605, Train Perplexity: 26.0613\n",
            "Epoch [1/3], Batch [9604/21340], Train Loss: 3.2415, Train Perplexity: 25.5728\n",
            "Epoch [1/3], Batch [9605/21340], Train Loss: 3.2843, Train Perplexity: 26.6891\n",
            "Epoch [1/3], Batch [9606/21340], Train Loss: 3.2083, Train Perplexity: 24.7382\n",
            "Epoch [1/3], Batch [9607/21340], Train Loss: 3.3814, Train Perplexity: 29.4134\n",
            "Epoch [1/3], Batch [9608/21340], Train Loss: 3.2610, Train Perplexity: 26.0761\n",
            "Epoch [1/3], Batch [9609/21340], Train Loss: 3.2646, Train Perplexity: 26.1703\n",
            "Epoch [1/3], Batch [9610/21340], Train Loss: 3.3320, Train Perplexity: 27.9956\n",
            "Epoch [1/3], Batch [9611/21340], Train Loss: 3.2223, Train Perplexity: 25.0860\n",
            "Epoch [1/3], Batch [9612/21340], Train Loss: 3.2206, Train Perplexity: 25.0434\n",
            "Epoch [1/3], Batch [9613/21340], Train Loss: 3.3076, Train Perplexity: 27.3188\n",
            "Epoch [1/3], Batch [9614/21340], Train Loss: 3.2344, Train Perplexity: 25.3912\n",
            "Epoch [1/3], Batch [9615/21340], Train Loss: 3.3465, Train Perplexity: 28.4041\n",
            "Epoch [1/3], Batch [9616/21340], Train Loss: 3.3936, Train Perplexity: 29.7729\n",
            "Epoch [1/3], Batch [9617/21340], Train Loss: 3.4484, Train Perplexity: 31.4511\n",
            "Epoch [1/3], Batch [9618/21340], Train Loss: 3.3228, Train Perplexity: 27.7375\n",
            "Epoch [1/3], Batch [9619/21340], Train Loss: 3.3998, Train Perplexity: 29.9575\n",
            "Epoch [1/3], Batch [9620/21340], Train Loss: 3.2788, Train Perplexity: 26.5426\n",
            "Epoch [1/3], Batch [9621/21340], Train Loss: 3.3024, Train Perplexity: 27.1772\n",
            "Epoch [1/3], Batch [9622/21340], Train Loss: 3.2289, Train Perplexity: 25.2516\n",
            "Epoch [1/3], Batch [9623/21340], Train Loss: 3.1451, Train Perplexity: 23.2225\n",
            "Epoch [1/3], Batch [9624/21340], Train Loss: 3.2863, Train Perplexity: 26.7433\n",
            "Epoch [1/3], Batch [9625/21340], Train Loss: 3.2030, Train Perplexity: 24.6068\n",
            "Epoch [1/3], Batch [9626/21340], Train Loss: 3.2394, Train Perplexity: 25.5195\n",
            "Epoch [1/3], Batch [9627/21340], Train Loss: 3.5220, Train Perplexity: 33.8536\n",
            "Epoch [1/3], Batch [9628/21340], Train Loss: 3.2132, Train Perplexity: 24.8581\n",
            "Epoch [1/3], Batch [9629/21340], Train Loss: 3.3080, Train Perplexity: 27.3294\n",
            "Epoch [1/3], Batch [9630/21340], Train Loss: 3.2784, Train Perplexity: 26.5336\n",
            "Epoch [1/3], Batch [9631/21340], Train Loss: 3.2333, Train Perplexity: 25.3632\n",
            "Epoch [1/3], Batch [9632/21340], Train Loss: 3.3280, Train Perplexity: 27.8819\n",
            "Epoch [1/3], Batch [9633/21340], Train Loss: 3.2220, Train Perplexity: 25.0775\n",
            "Epoch [1/3], Batch [9634/21340], Train Loss: 3.2748, Train Perplexity: 26.4387\n",
            "Epoch [1/3], Batch [9635/21340], Train Loss: 3.3908, Train Perplexity: 29.6894\n",
            "Epoch [1/3], Batch [9636/21340], Train Loss: 3.3019, Train Perplexity: 27.1639\n",
            "Epoch [1/3], Batch [9637/21340], Train Loss: 3.3313, Train Perplexity: 27.9754\n",
            "Epoch [1/3], Batch [9638/21340], Train Loss: 3.3181, Train Perplexity: 27.6092\n",
            "Epoch [1/3], Batch [9639/21340], Train Loss: 3.3112, Train Perplexity: 27.4174\n",
            "Epoch [1/3], Batch [9640/21340], Train Loss: 3.2944, Train Perplexity: 26.9617\n",
            "Epoch [1/3], Batch [9641/21340], Train Loss: 3.2275, Train Perplexity: 25.2174\n",
            "Epoch [1/3], Batch [9642/21340], Train Loss: 3.1565, Train Perplexity: 23.4893\n",
            "Epoch [1/3], Batch [9643/21340], Train Loss: 3.3119, Train Perplexity: 27.4364\n",
            "Epoch [1/3], Batch [9644/21340], Train Loss: 3.1954, Train Perplexity: 24.4189\n",
            "Epoch [1/3], Batch [9645/21340], Train Loss: 3.2266, Train Perplexity: 25.1947\n",
            "Epoch [1/3], Batch [9646/21340], Train Loss: 3.2620, Train Perplexity: 26.1022\n",
            "Epoch [1/3], Batch [9647/21340], Train Loss: 3.3151, Train Perplexity: 27.5252\n",
            "Epoch [1/3], Batch [9648/21340], Train Loss: 3.2298, Train Perplexity: 25.2750\n",
            "Epoch [1/3], Batch [9649/21340], Train Loss: 3.1801, Train Perplexity: 24.0480\n",
            "Epoch [1/3], Batch [9650/21340], Train Loss: 3.2393, Train Perplexity: 25.5147\n",
            "Epoch [1/3], Batch [9651/21340], Train Loss: 3.2736, Train Perplexity: 26.4052\n",
            "Epoch [1/3], Batch [9652/21340], Train Loss: 3.4369, Train Perplexity: 31.0915\n",
            "Epoch [1/3], Batch [9653/21340], Train Loss: 3.1956, Train Perplexity: 24.4247\n",
            "Epoch [1/3], Batch [9654/21340], Train Loss: 3.1883, Train Perplexity: 24.2460\n",
            "Epoch [1/3], Batch [9655/21340], Train Loss: 3.2326, Train Perplexity: 25.3453\n",
            "Epoch [1/3], Batch [9656/21340], Train Loss: 3.3414, Train Perplexity: 28.2575\n",
            "Epoch [1/3], Batch [9657/21340], Train Loss: 3.1764, Train Perplexity: 23.9608\n",
            "Epoch [1/3], Batch [9658/21340], Train Loss: 3.3776, Train Perplexity: 29.3001\n",
            "Epoch [1/3], Batch [9659/21340], Train Loss: 3.2539, Train Perplexity: 25.8915\n",
            "Epoch [1/3], Batch [9660/21340], Train Loss: 3.2485, Train Perplexity: 25.7509\n",
            "Epoch [1/3], Batch [9661/21340], Train Loss: 3.1847, Train Perplexity: 24.1595\n",
            "Epoch [1/3], Batch [9662/21340], Train Loss: 3.1777, Train Perplexity: 23.9920\n",
            "Epoch [1/3], Batch [9663/21340], Train Loss: 3.2335, Train Perplexity: 25.3677\n",
            "Epoch [1/3], Batch [9664/21340], Train Loss: 3.2200, Train Perplexity: 25.0284\n",
            "Epoch [1/3], Batch [9665/21340], Train Loss: 3.2764, Train Perplexity: 26.4790\n",
            "Epoch [1/3], Batch [9666/21340], Train Loss: 3.2702, Train Perplexity: 26.3156\n",
            "Epoch [1/3], Batch [9667/21340], Train Loss: 3.1594, Train Perplexity: 23.5566\n",
            "Epoch [1/3], Batch [9668/21340], Train Loss: 3.1637, Train Perplexity: 23.6590\n",
            "Epoch [1/3], Batch [9669/21340], Train Loss: 3.2345, Train Perplexity: 25.3928\n",
            "Epoch [1/3], Batch [9670/21340], Train Loss: 3.2049, Train Perplexity: 24.6532\n",
            "Epoch [1/3], Batch [9671/21340], Train Loss: 3.2812, Train Perplexity: 26.6087\n",
            "Epoch [1/3], Batch [9672/21340], Train Loss: 3.2556, Train Perplexity: 25.9357\n",
            "Epoch [1/3], Batch [9673/21340], Train Loss: 3.3141, Train Perplexity: 27.4985\n",
            "Epoch [1/3], Batch [9674/21340], Train Loss: 3.2735, Train Perplexity: 26.4033\n",
            "Epoch [1/3], Batch [9675/21340], Train Loss: 3.2775, Train Perplexity: 26.5099\n",
            "Epoch [1/3], Batch [9676/21340], Train Loss: 3.1831, Train Perplexity: 24.1224\n",
            "Epoch [1/3], Batch [9677/21340], Train Loss: 3.2326, Train Perplexity: 25.3467\n",
            "Epoch [1/3], Batch [9678/21340], Train Loss: 3.2351, Train Perplexity: 25.4079\n",
            "Epoch [1/3], Batch [9679/21340], Train Loss: 3.1695, Train Perplexity: 23.7956\n",
            "Epoch [1/3], Batch [9680/21340], Train Loss: 3.2320, Train Perplexity: 25.3297\n",
            "Epoch [1/3], Batch [9681/21340], Train Loss: 3.2336, Train Perplexity: 25.3713\n",
            "Epoch [1/3], Batch [9682/21340], Train Loss: 3.2673, Train Perplexity: 26.2406\n",
            "Epoch [1/3], Batch [9683/21340], Train Loss: 3.3238, Train Perplexity: 27.7645\n",
            "Epoch [1/3], Batch [9684/21340], Train Loss: 3.1207, Train Perplexity: 22.6622\n",
            "Epoch [1/3], Batch [9685/21340], Train Loss: 3.2168, Train Perplexity: 24.9471\n",
            "Epoch [1/3], Batch [9686/21340], Train Loss: 3.2667, Train Perplexity: 26.2247\n",
            "Epoch [1/3], Batch [9687/21340], Train Loss: 3.2118, Train Perplexity: 24.8230\n",
            "Epoch [1/3], Batch [9688/21340], Train Loss: 3.2571, Train Perplexity: 25.9738\n",
            "Epoch [1/3], Batch [9689/21340], Train Loss: 3.1629, Train Perplexity: 23.6393\n",
            "Epoch [1/3], Batch [9690/21340], Train Loss: 3.3203, Train Perplexity: 27.6693\n",
            "Epoch [1/3], Batch [9691/21340], Train Loss: 3.2535, Train Perplexity: 25.8817\n",
            "Epoch [1/3], Batch [9692/21340], Train Loss: 3.3344, Train Perplexity: 28.0608\n",
            "Epoch [1/3], Batch [9693/21340], Train Loss: 3.3683, Train Perplexity: 29.0305\n",
            "Epoch [1/3], Batch [9694/21340], Train Loss: 3.1946, Train Perplexity: 24.4008\n",
            "Epoch [1/3], Batch [9695/21340], Train Loss: 3.2432, Train Perplexity: 25.6144\n",
            "Epoch [1/3], Batch [9696/21340], Train Loss: 3.2272, Train Perplexity: 25.2089\n",
            "Epoch [1/3], Batch [9697/21340], Train Loss: 3.2155, Train Perplexity: 24.9160\n",
            "Epoch [1/3], Batch [9698/21340], Train Loss: 3.3107, Train Perplexity: 27.4036\n",
            "Epoch [1/3], Batch [9699/21340], Train Loss: 3.1799, Train Perplexity: 24.0450\n",
            "Epoch [1/3], Batch [9700/21340], Train Loss: 3.2110, Train Perplexity: 24.8040\n",
            "Epoch [1/3], Batch [9701/21340], Train Loss: 3.2528, Train Perplexity: 25.8636\n",
            "Epoch [1/3], Batch [9702/21340], Train Loss: 3.2214, Train Perplexity: 25.0643\n",
            "Epoch [1/3], Batch [9703/21340], Train Loss: 3.2454, Train Perplexity: 25.6711\n",
            "Epoch [1/3], Batch [9704/21340], Train Loss: 3.1449, Train Perplexity: 23.2172\n",
            "Epoch [1/3], Batch [9705/21340], Train Loss: 3.3468, Train Perplexity: 28.4119\n",
            "Epoch [1/3], Batch [9706/21340], Train Loss: 3.2429, Train Perplexity: 25.6089\n",
            "Epoch [1/3], Batch [9707/21340], Train Loss: 3.3167, Train Perplexity: 27.5693\n",
            "Epoch [1/3], Batch [9708/21340], Train Loss: 3.2575, Train Perplexity: 25.9850\n",
            "Epoch [1/3], Batch [9709/21340], Train Loss: 3.2697, Train Perplexity: 26.3044\n",
            "Epoch [1/3], Batch [9710/21340], Train Loss: 3.2045, Train Perplexity: 24.6429\n",
            "Epoch [1/3], Batch [9711/21340], Train Loss: 3.3957, Train Perplexity: 29.8359\n",
            "Epoch [1/3], Batch [9712/21340], Train Loss: 3.3223, Train Perplexity: 27.7227\n",
            "Epoch [1/3], Batch [9713/21340], Train Loss: 3.3298, Train Perplexity: 27.9320\n",
            "Epoch [1/3], Batch [9714/21340], Train Loss: 3.2003, Train Perplexity: 24.5396\n",
            "Epoch [1/3], Batch [9715/21340], Train Loss: 3.2382, Train Perplexity: 25.4879\n",
            "Epoch [1/3], Batch [9716/21340], Train Loss: 3.2811, Train Perplexity: 26.6057\n",
            "Epoch [1/3], Batch [9717/21340], Train Loss: 3.2102, Train Perplexity: 24.7849\n",
            "Epoch [1/3], Batch [9718/21340], Train Loss: 3.2429, Train Perplexity: 25.6080\n",
            "Epoch [1/3], Batch [9719/21340], Train Loss: 3.2610, Train Perplexity: 26.0757\n",
            "Epoch [1/3], Batch [9720/21340], Train Loss: 3.2890, Train Perplexity: 26.8159\n",
            "Epoch [1/3], Batch [9721/21340], Train Loss: 3.1985, Train Perplexity: 24.4948\n",
            "Epoch [1/3], Batch [9722/21340], Train Loss: 3.2168, Train Perplexity: 24.9487\n",
            "Epoch [1/3], Batch [9723/21340], Train Loss: 3.3140, Train Perplexity: 27.4959\n",
            "Epoch [1/3], Batch [9724/21340], Train Loss: 3.1294, Train Perplexity: 22.8611\n",
            "Epoch [1/3], Batch [9725/21340], Train Loss: 3.2548, Train Perplexity: 25.9138\n",
            "Epoch [1/3], Batch [9726/21340], Train Loss: 3.2908, Train Perplexity: 26.8651\n",
            "Epoch [1/3], Batch [9727/21340], Train Loss: 3.3395, Train Perplexity: 28.2049\n",
            "Epoch [1/3], Batch [9728/21340], Train Loss: 3.2174, Train Perplexity: 24.9627\n",
            "Epoch [1/3], Batch [9729/21340], Train Loss: 3.2035, Train Perplexity: 24.6184\n",
            "Epoch [1/3], Batch [9730/21340], Train Loss: 3.3789, Train Perplexity: 29.3378\n",
            "Epoch [1/3], Batch [9731/21340], Train Loss: 3.2959, Train Perplexity: 27.0013\n",
            "Epoch [1/3], Batch [9732/21340], Train Loss: 3.4374, Train Perplexity: 31.1056\n",
            "Epoch [1/3], Batch [9733/21340], Train Loss: 3.2482, Train Perplexity: 25.7438\n",
            "Epoch [1/3], Batch [9734/21340], Train Loss: 3.1602, Train Perplexity: 23.5760\n",
            "Epoch [1/3], Batch [9735/21340], Train Loss: 3.2323, Train Perplexity: 25.3367\n",
            "Epoch [1/3], Batch [9736/21340], Train Loss: 3.2189, Train Perplexity: 24.9999\n",
            "Epoch [1/3], Batch [9737/21340], Train Loss: 3.1921, Train Perplexity: 24.3389\n",
            "Epoch [1/3], Batch [9738/21340], Train Loss: 3.3717, Train Perplexity: 29.1285\n",
            "Epoch [1/3], Batch [9739/21340], Train Loss: 3.3221, Train Perplexity: 27.7192\n",
            "Epoch [1/3], Batch [9740/21340], Train Loss: 3.2400, Train Perplexity: 25.5348\n",
            "Epoch [1/3], Batch [9741/21340], Train Loss: 3.1986, Train Perplexity: 24.4974\n",
            "Epoch [1/3], Batch [9742/21340], Train Loss: 3.2665, Train Perplexity: 26.2203\n",
            "Epoch [1/3], Batch [9743/21340], Train Loss: 3.1972, Train Perplexity: 24.4648\n",
            "Epoch [1/3], Batch [9744/21340], Train Loss: 3.3108, Train Perplexity: 27.4067\n",
            "Epoch [1/3], Batch [9745/21340], Train Loss: 3.1978, Train Perplexity: 24.4793\n",
            "Epoch [1/3], Batch [9746/21340], Train Loss: 3.1945, Train Perplexity: 24.3991\n",
            "Epoch [1/3], Batch [9747/21340], Train Loss: 3.1969, Train Perplexity: 24.4568\n",
            "Epoch [1/3], Batch [9748/21340], Train Loss: 3.2518, Train Perplexity: 25.8366\n",
            "Epoch [1/3], Batch [9749/21340], Train Loss: 3.2480, Train Perplexity: 25.7400\n",
            "Epoch [1/3], Batch [9750/21340], Train Loss: 3.2363, Train Perplexity: 25.4382\n",
            "Epoch [1/3], Batch [9751/21340], Train Loss: 3.3486, Train Perplexity: 28.4622\n",
            "Epoch [1/3], Batch [9752/21340], Train Loss: 3.2379, Train Perplexity: 25.4796\n",
            "Epoch [1/3], Batch [9753/21340], Train Loss: 3.2892, Train Perplexity: 26.8203\n",
            "Epoch [1/3], Batch [9754/21340], Train Loss: 3.2331, Train Perplexity: 25.3583\n",
            "Epoch [1/3], Batch [9755/21340], Train Loss: 3.2593, Train Perplexity: 26.0322\n",
            "Epoch [1/3], Batch [9756/21340], Train Loss: 3.2264, Train Perplexity: 25.1898\n",
            "Epoch [1/3], Batch [9757/21340], Train Loss: 3.2795, Train Perplexity: 26.5614\n",
            "Epoch [1/3], Batch [9758/21340], Train Loss: 3.1626, Train Perplexity: 23.6323\n",
            "Epoch [1/3], Batch [9759/21340], Train Loss: 3.2394, Train Perplexity: 25.5183\n",
            "Epoch [1/3], Batch [9760/21340], Train Loss: 3.3335, Train Perplexity: 28.0359\n",
            "Epoch [1/3], Batch [9761/21340], Train Loss: 3.2520, Train Perplexity: 25.8416\n",
            "Epoch [1/3], Batch [9762/21340], Train Loss: 3.2795, Train Perplexity: 26.5638\n",
            "Epoch [1/3], Batch [9763/21340], Train Loss: 3.3669, Train Perplexity: 28.9877\n",
            "Epoch [1/3], Batch [9764/21340], Train Loss: 3.1988, Train Perplexity: 24.5036\n",
            "Epoch [1/3], Batch [9765/21340], Train Loss: 3.2100, Train Perplexity: 24.7794\n",
            "Epoch [1/3], Batch [9766/21340], Train Loss: 3.2193, Train Perplexity: 25.0097\n",
            "Epoch [1/3], Batch [9767/21340], Train Loss: 3.2444, Train Perplexity: 25.6457\n",
            "Epoch [1/3], Batch [9768/21340], Train Loss: 3.3191, Train Perplexity: 27.6362\n",
            "Epoch [1/3], Batch [9769/21340], Train Loss: 3.3443, Train Perplexity: 28.3403\n",
            "Epoch [1/3], Batch [9770/21340], Train Loss: 3.2304, Train Perplexity: 25.2902\n",
            "Epoch [1/3], Batch [9771/21340], Train Loss: 3.2053, Train Perplexity: 24.6625\n",
            "Epoch [1/3], Batch [9772/21340], Train Loss: 3.1692, Train Perplexity: 23.7896\n",
            "Epoch [1/3], Batch [9773/21340], Train Loss: 3.3106, Train Perplexity: 27.4011\n",
            "Epoch [1/3], Batch [9774/21340], Train Loss: 3.2048, Train Perplexity: 24.6501\n",
            "Epoch [1/3], Batch [9775/21340], Train Loss: 3.2541, Train Perplexity: 25.8952\n",
            "Epoch [1/3], Batch [9776/21340], Train Loss: 3.1935, Train Perplexity: 24.3746\n",
            "Epoch [1/3], Batch [9777/21340], Train Loss: 3.2563, Train Perplexity: 25.9532\n",
            "Epoch [1/3], Batch [9778/21340], Train Loss: 3.3020, Train Perplexity: 27.1663\n",
            "Epoch [1/3], Batch [9779/21340], Train Loss: 3.1744, Train Perplexity: 23.9122\n",
            "Epoch [1/3], Batch [9780/21340], Train Loss: 3.2105, Train Perplexity: 24.7921\n",
            "Epoch [1/3], Batch [9781/21340], Train Loss: 3.1924, Train Perplexity: 24.3476\n",
            "Epoch [1/3], Batch [9782/21340], Train Loss: 3.3103, Train Perplexity: 27.3921\n",
            "Epoch [1/3], Batch [9783/21340], Train Loss: 3.3138, Train Perplexity: 27.4903\n",
            "Epoch [1/3], Batch [9784/21340], Train Loss: 3.2208, Train Perplexity: 25.0487\n",
            "Epoch [1/3], Batch [9785/21340], Train Loss: 3.1690, Train Perplexity: 23.7831\n",
            "Epoch [1/3], Batch [9786/21340], Train Loss: 3.2474, Train Perplexity: 25.7230\n",
            "Epoch [1/3], Batch [9787/21340], Train Loss: 3.2368, Train Perplexity: 25.4515\n",
            "Epoch [1/3], Batch [9788/21340], Train Loss: 3.3538, Train Perplexity: 28.6116\n",
            "Epoch [1/3], Batch [9789/21340], Train Loss: 3.2107, Train Perplexity: 24.7956\n",
            "Epoch [1/3], Batch [9790/21340], Train Loss: 3.3153, Train Perplexity: 27.5293\n",
            "Epoch [1/3], Batch [9791/21340], Train Loss: 3.1550, Train Perplexity: 23.4532\n",
            "Epoch [1/3], Batch [9792/21340], Train Loss: 3.2181, Train Perplexity: 24.9795\n",
            "Epoch [1/3], Batch [9793/21340], Train Loss: 3.3366, Train Perplexity: 28.1220\n",
            "Epoch [1/3], Batch [9794/21340], Train Loss: 3.2249, Train Perplexity: 25.1503\n",
            "Epoch [1/3], Batch [9795/21340], Train Loss: 3.1904, Train Perplexity: 24.2993\n",
            "Epoch [1/3], Batch [9796/21340], Train Loss: 3.2695, Train Perplexity: 26.2992\n",
            "Epoch [1/3], Batch [9797/21340], Train Loss: 3.2538, Train Perplexity: 25.8897\n",
            "Epoch [1/3], Batch [9798/21340], Train Loss: 3.3156, Train Perplexity: 27.5377\n",
            "Epoch [1/3], Batch [9799/21340], Train Loss: 3.2163, Train Perplexity: 24.9351\n",
            "Epoch [1/3], Batch [9800/21340], Train Loss: 3.1962, Train Perplexity: 24.4406\n",
            "Epoch [1/3], Batch [9801/21340], Train Loss: 3.2320, Train Perplexity: 25.3306\n",
            "Epoch [1/3], Batch [9802/21340], Train Loss: 3.2602, Train Perplexity: 26.0535\n",
            "Epoch [1/3], Batch [9803/21340], Train Loss: 3.2219, Train Perplexity: 25.0757\n",
            "Epoch [1/3], Batch [9804/21340], Train Loss: 3.2739, Train Perplexity: 26.4130\n",
            "Epoch [1/3], Batch [9805/21340], Train Loss: 3.4702, Train Perplexity: 32.1424\n",
            "Epoch [1/3], Batch [9806/21340], Train Loss: 3.2696, Train Perplexity: 26.2995\n",
            "Epoch [1/3], Batch [9807/21340], Train Loss: 3.2473, Train Perplexity: 25.7198\n",
            "Epoch [1/3], Batch [9808/21340], Train Loss: 3.2381, Train Perplexity: 25.4845\n",
            "Epoch [1/3], Batch [9809/21340], Train Loss: 3.3908, Train Perplexity: 29.6902\n",
            "Epoch [1/3], Batch [9810/21340], Train Loss: 3.2380, Train Perplexity: 25.4824\n",
            "Epoch [1/3], Batch [9811/21340], Train Loss: 3.1827, Train Perplexity: 24.1107\n",
            "Epoch [1/3], Batch [9812/21340], Train Loss: 3.1589, Train Perplexity: 23.5455\n",
            "Epoch [1/3], Batch [9813/21340], Train Loss: 3.3613, Train Perplexity: 28.8267\n",
            "Epoch [1/3], Batch [9814/21340], Train Loss: 3.3121, Train Perplexity: 27.4422\n",
            "Epoch [1/3], Batch [9815/21340], Train Loss: 3.1924, Train Perplexity: 24.3477\n",
            "Epoch [1/3], Batch [9816/21340], Train Loss: 3.2001, Train Perplexity: 24.5358\n",
            "Epoch [1/3], Batch [9817/21340], Train Loss: 3.2112, Train Perplexity: 24.8085\n",
            "Epoch [1/3], Batch [9818/21340], Train Loss: 3.2874, Train Perplexity: 26.7734\n",
            "Epoch [1/3], Batch [9819/21340], Train Loss: 3.2402, Train Perplexity: 25.5389\n",
            "Epoch [1/3], Batch [9820/21340], Train Loss: 3.2627, Train Perplexity: 26.1208\n",
            "Epoch [1/3], Batch [9821/21340], Train Loss: 3.3665, Train Perplexity: 28.9765\n",
            "Epoch [1/3], Batch [9822/21340], Train Loss: 3.2652, Train Perplexity: 26.1858\n",
            "Epoch [1/3], Batch [9823/21340], Train Loss: 3.1852, Train Perplexity: 24.1728\n",
            "Epoch [1/3], Batch [9824/21340], Train Loss: 3.2647, Train Perplexity: 26.1725\n",
            "Epoch [1/3], Batch [9825/21340], Train Loss: 3.2907, Train Perplexity: 26.8618\n",
            "Epoch [1/3], Batch [9826/21340], Train Loss: 3.1608, Train Perplexity: 23.5885\n",
            "Epoch [1/3], Batch [9827/21340], Train Loss: 3.2472, Train Perplexity: 25.7179\n",
            "Epoch [1/3], Batch [9828/21340], Train Loss: 3.2330, Train Perplexity: 25.3565\n",
            "Epoch [1/3], Batch [9829/21340], Train Loss: 3.3054, Train Perplexity: 27.2589\n",
            "Epoch [1/3], Batch [9830/21340], Train Loss: 3.4040, Train Perplexity: 30.0835\n",
            "Epoch [1/3], Batch [9831/21340], Train Loss: 3.2057, Train Perplexity: 24.6739\n",
            "Epoch [1/3], Batch [9832/21340], Train Loss: 3.2999, Train Perplexity: 27.1093\n",
            "Epoch [1/3], Batch [9833/21340], Train Loss: 3.2839, Train Perplexity: 26.6801\n",
            "Epoch [1/3], Batch [9834/21340], Train Loss: 3.2669, Train Perplexity: 26.2294\n",
            "Epoch [1/3], Batch [9835/21340], Train Loss: 3.2882, Train Perplexity: 26.7952\n",
            "Epoch [1/3], Batch [9836/21340], Train Loss: 3.2903, Train Perplexity: 26.8519\n",
            "Epoch [1/3], Batch [9837/21340], Train Loss: 3.2359, Train Perplexity: 25.4299\n",
            "Epoch [1/3], Batch [9838/21340], Train Loss: 3.2637, Train Perplexity: 26.1452\n",
            "Epoch [1/3], Batch [9839/21340], Train Loss: 3.2523, Train Perplexity: 25.8488\n",
            "Epoch [1/3], Batch [9840/21340], Train Loss: 3.2978, Train Perplexity: 27.0525\n",
            "Epoch [1/3], Batch [9841/21340], Train Loss: 3.1984, Train Perplexity: 24.4945\n",
            "Epoch [1/3], Batch [9842/21340], Train Loss: 3.2172, Train Perplexity: 24.9572\n",
            "Epoch [1/3], Batch [9843/21340], Train Loss: 3.1788, Train Perplexity: 24.0171\n",
            "Epoch [1/3], Batch [9844/21340], Train Loss: 3.2559, Train Perplexity: 25.9433\n",
            "Epoch [1/3], Batch [9845/21340], Train Loss: 3.1507, Train Perplexity: 23.3528\n",
            "Epoch [1/3], Batch [9846/21340], Train Loss: 3.2271, Train Perplexity: 25.2071\n",
            "Epoch [1/3], Batch [9847/21340], Train Loss: 3.2174, Train Perplexity: 24.9635\n",
            "Epoch [1/3], Batch [9848/21340], Train Loss: 3.4563, Train Perplexity: 31.7009\n",
            "Epoch [1/3], Batch [9849/21340], Train Loss: 3.2409, Train Perplexity: 25.5575\n",
            "Epoch [1/3], Batch [9850/21340], Train Loss: 3.3416, Train Perplexity: 28.2633\n",
            "Epoch [1/3], Batch [9851/21340], Train Loss: 3.3981, Train Perplexity: 29.9087\n",
            "Epoch [1/3], Batch [9852/21340], Train Loss: 3.2239, Train Perplexity: 25.1261\n",
            "Epoch [1/3], Batch [9853/21340], Train Loss: 3.3027, Train Perplexity: 27.1872\n",
            "Epoch [1/3], Batch [9854/21340], Train Loss: 3.3425, Train Perplexity: 28.2911\n",
            "Epoch [1/3], Batch [9855/21340], Train Loss: 3.1958, Train Perplexity: 24.4288\n",
            "Epoch [1/3], Batch [9856/21340], Train Loss: 3.4229, Train Perplexity: 30.6584\n",
            "Epoch [1/3], Batch [9857/21340], Train Loss: 3.2129, Train Perplexity: 24.8522\n",
            "Epoch [1/3], Batch [9858/21340], Train Loss: 3.2894, Train Perplexity: 26.8272\n",
            "Epoch [1/3], Batch [9859/21340], Train Loss: 3.2293, Train Perplexity: 25.2622\n",
            "Epoch [1/3], Batch [9860/21340], Train Loss: 3.2135, Train Perplexity: 24.8650\n",
            "Epoch [1/3], Batch [9861/21340], Train Loss: 3.2095, Train Perplexity: 24.7673\n",
            "Epoch [1/3], Batch [9862/21340], Train Loss: 3.2732, Train Perplexity: 26.3957\n",
            "Epoch [1/3], Batch [9863/21340], Train Loss: 3.1871, Train Perplexity: 24.2189\n",
            "Epoch [1/3], Batch [9864/21340], Train Loss: 3.3074, Train Perplexity: 27.3129\n",
            "Epoch [1/3], Batch [9865/21340], Train Loss: 3.2548, Train Perplexity: 25.9156\n",
            "Epoch [1/3], Batch [9866/21340], Train Loss: 3.2165, Train Perplexity: 24.9402\n",
            "Epoch [1/3], Batch [9867/21340], Train Loss: 3.4611, Train Perplexity: 31.8520\n",
            "Epoch [1/3], Batch [9868/21340], Train Loss: 3.2815, Train Perplexity: 26.6156\n",
            "Epoch [1/3], Batch [9869/21340], Train Loss: 3.2422, Train Perplexity: 25.5906\n",
            "Epoch [1/3], Batch [9870/21340], Train Loss: 3.1827, Train Perplexity: 24.1114\n",
            "Epoch [1/3], Batch [9871/21340], Train Loss: 3.2502, Train Perplexity: 25.7951\n",
            "Epoch [1/3], Batch [9872/21340], Train Loss: 3.2463, Train Perplexity: 25.6941\n",
            "Epoch [1/3], Batch [9873/21340], Train Loss: 3.2473, Train Perplexity: 25.7197\n",
            "Epoch [1/3], Batch [9874/21340], Train Loss: 3.2784, Train Perplexity: 26.5322\n",
            "Epoch [1/3], Batch [9875/21340], Train Loss: 3.1849, Train Perplexity: 24.1660\n",
            "Epoch [1/3], Batch [9876/21340], Train Loss: 3.2284, Train Perplexity: 25.2402\n",
            "Epoch [1/3], Batch [9877/21340], Train Loss: 3.2869, Train Perplexity: 26.7594\n",
            "Epoch [1/3], Batch [9878/21340], Train Loss: 3.2429, Train Perplexity: 25.6091\n",
            "Epoch [1/3], Batch [9879/21340], Train Loss: 3.2198, Train Perplexity: 25.0234\n",
            "Epoch [1/3], Batch [9880/21340], Train Loss: 3.2680, Train Perplexity: 26.2577\n",
            "Epoch [1/3], Batch [9881/21340], Train Loss: 3.1810, Train Perplexity: 24.0709\n",
            "Epoch [1/3], Batch [9882/21340], Train Loss: 3.2897, Train Perplexity: 26.8347\n",
            "Epoch [1/3], Batch [9883/21340], Train Loss: 3.2419, Train Perplexity: 25.5824\n",
            "Epoch [1/3], Batch [9884/21340], Train Loss: 3.4046, Train Perplexity: 30.1009\n",
            "Epoch [1/3], Batch [9885/21340], Train Loss: 3.2453, Train Perplexity: 25.6685\n",
            "Epoch [1/3], Batch [9886/21340], Train Loss: 3.2005, Train Perplexity: 24.5447\n",
            "Epoch [1/3], Batch [9887/21340], Train Loss: 3.1664, Train Perplexity: 23.7208\n",
            "Epoch [1/3], Batch [9888/21340], Train Loss: 3.2785, Train Perplexity: 26.5361\n",
            "Epoch [1/3], Batch [9889/21340], Train Loss: 3.3201, Train Perplexity: 27.6637\n",
            "Epoch [1/3], Batch [9890/21340], Train Loss: 3.2883, Train Perplexity: 26.7984\n",
            "Epoch [1/3], Batch [9891/21340], Train Loss: 3.2149, Train Perplexity: 24.9004\n",
            "Epoch [1/3], Batch [9892/21340], Train Loss: 3.1636, Train Perplexity: 23.6558\n",
            "Epoch [1/3], Batch [9893/21340], Train Loss: 3.2143, Train Perplexity: 24.8850\n",
            "Epoch [1/3], Batch [9894/21340], Train Loss: 3.2954, Train Perplexity: 26.9882\n",
            "Epoch [1/3], Batch [9895/21340], Train Loss: 3.2195, Train Perplexity: 25.0161\n",
            "Epoch [1/3], Batch [9896/21340], Train Loss: 3.2949, Train Perplexity: 26.9741\n",
            "Epoch [1/3], Batch [9897/21340], Train Loss: 3.2375, Train Perplexity: 25.4694\n",
            "Epoch [1/3], Batch [9898/21340], Train Loss: 3.4503, Train Perplexity: 31.5096\n",
            "Epoch [1/3], Batch [9899/21340], Train Loss: 3.2163, Train Perplexity: 24.9367\n",
            "Epoch [1/3], Batch [9900/21340], Train Loss: 3.3188, Train Perplexity: 27.6283\n",
            "Epoch [1/3], Batch [9901/21340], Train Loss: 3.2091, Train Perplexity: 24.7562\n",
            "Epoch [1/3], Batch [9902/21340], Train Loss: 3.2763, Train Perplexity: 26.4775\n",
            "Epoch [1/3], Batch [9903/21340], Train Loss: 3.2396, Train Perplexity: 25.5224\n",
            "Epoch [1/3], Batch [9904/21340], Train Loss: 3.3295, Train Perplexity: 27.9252\n",
            "Epoch [1/3], Batch [9905/21340], Train Loss: 3.1843, Train Perplexity: 24.1493\n",
            "Epoch [1/3], Batch [9906/21340], Train Loss: 3.3552, Train Perplexity: 28.6502\n",
            "Epoch [1/3], Batch [9907/21340], Train Loss: 3.2884, Train Perplexity: 26.8001\n",
            "Epoch [1/3], Batch [9908/21340], Train Loss: 3.2367, Train Perplexity: 25.4496\n",
            "Epoch [1/3], Batch [9909/21340], Train Loss: 3.3974, Train Perplexity: 29.8862\n",
            "Epoch [1/3], Batch [9910/21340], Train Loss: 3.2699, Train Perplexity: 26.3098\n",
            "Epoch [1/3], Batch [9911/21340], Train Loss: 3.3733, Train Perplexity: 29.1735\n",
            "Epoch [1/3], Batch [9912/21340], Train Loss: 3.2668, Train Perplexity: 26.2261\n",
            "Epoch [1/3], Batch [9913/21340], Train Loss: 3.3762, Train Perplexity: 29.2604\n",
            "Epoch [1/3], Batch [9914/21340], Train Loss: 3.2047, Train Perplexity: 24.6491\n",
            "Epoch [1/3], Batch [9915/21340], Train Loss: 3.1825, Train Perplexity: 24.1074\n",
            "Epoch [1/3], Batch [9916/21340], Train Loss: 3.2122, Train Perplexity: 24.8325\n",
            "Epoch [1/3], Batch [9917/21340], Train Loss: 3.1316, Train Perplexity: 22.9109\n",
            "Epoch [1/3], Batch [9918/21340], Train Loss: 3.2631, Train Perplexity: 26.1310\n",
            "Epoch [1/3], Batch [9919/21340], Train Loss: 3.2517, Train Perplexity: 25.8349\n",
            "Epoch [1/3], Batch [9920/21340], Train Loss: 3.3583, Train Perplexity: 28.7411\n",
            "Epoch [1/3], Batch [9921/21340], Train Loss: 3.3554, Train Perplexity: 28.6577\n",
            "Epoch [1/3], Batch [9922/21340], Train Loss: 3.2635, Train Perplexity: 26.1407\n",
            "Epoch [1/3], Batch [9923/21340], Train Loss: 3.2982, Train Perplexity: 27.0640\n",
            "Epoch [1/3], Batch [9924/21340], Train Loss: 3.2023, Train Perplexity: 24.5889\n",
            "Epoch [1/3], Batch [9925/21340], Train Loss: 3.2583, Train Perplexity: 26.0054\n",
            "Epoch [1/3], Batch [9926/21340], Train Loss: 3.2569, Train Perplexity: 25.9688\n",
            "Epoch [1/3], Batch [9927/21340], Train Loss: 3.1902, Train Perplexity: 24.2931\n",
            "Epoch [1/3], Batch [9928/21340], Train Loss: 3.1909, Train Perplexity: 24.3113\n",
            "Epoch [1/3], Batch [9929/21340], Train Loss: 3.1713, Train Perplexity: 23.8386\n",
            "Epoch [1/3], Batch [9930/21340], Train Loss: 3.2515, Train Perplexity: 25.8296\n",
            "Epoch [1/3], Batch [9931/21340], Train Loss: 3.3115, Train Perplexity: 27.4268\n",
            "Epoch [1/3], Batch [9932/21340], Train Loss: 3.2122, Train Perplexity: 24.8334\n",
            "Epoch [1/3], Batch [9933/21340], Train Loss: 3.1491, Train Perplexity: 23.3153\n",
            "Epoch [1/3], Batch [9934/21340], Train Loss: 3.1846, Train Perplexity: 24.1578\n",
            "Epoch [1/3], Batch [9935/21340], Train Loss: 3.2481, Train Perplexity: 25.7414\n",
            "Epoch [1/3], Batch [9936/21340], Train Loss: 3.2490, Train Perplexity: 25.7648\n",
            "Epoch [1/3], Batch [9937/21340], Train Loss: 3.2011, Train Perplexity: 24.5606\n",
            "Epoch [1/3], Batch [9938/21340], Train Loss: 3.2968, Train Perplexity: 27.0254\n",
            "Epoch [1/3], Batch [9939/21340], Train Loss: 3.3152, Train Perplexity: 27.5289\n",
            "Epoch [1/3], Batch [9940/21340], Train Loss: 3.2937, Train Perplexity: 26.9418\n",
            "Epoch [1/3], Batch [9941/21340], Train Loss: 3.2882, Train Perplexity: 26.7942\n",
            "Epoch [1/3], Batch [9942/21340], Train Loss: 3.2637, Train Perplexity: 26.1461\n",
            "Epoch [1/3], Batch [9943/21340], Train Loss: 3.2364, Train Perplexity: 25.4430\n",
            "Epoch [1/3], Batch [9944/21340], Train Loss: 3.2535, Train Perplexity: 25.8813\n",
            "Epoch [1/3], Batch [9945/21340], Train Loss: 3.2276, Train Perplexity: 25.2189\n",
            "Epoch [1/3], Batch [9946/21340], Train Loss: 3.2329, Train Perplexity: 25.3519\n",
            "Epoch [1/3], Batch [9947/21340], Train Loss: 3.2851, Train Perplexity: 26.7130\n",
            "Epoch [1/3], Batch [9948/21340], Train Loss: 3.3079, Train Perplexity: 27.3268\n",
            "Epoch [1/3], Batch [9949/21340], Train Loss: 3.2508, Train Perplexity: 25.8101\n",
            "Epoch [1/3], Batch [9950/21340], Train Loss: 3.1966, Train Perplexity: 24.4499\n",
            "Epoch [1/3], Batch [9951/21340], Train Loss: 3.3315, Train Perplexity: 27.9800\n",
            "Epoch [1/3], Batch [9952/21340], Train Loss: 3.2661, Train Perplexity: 26.2079\n",
            "Epoch [1/3], Batch [9953/21340], Train Loss: 3.2157, Train Perplexity: 24.9216\n",
            "Epoch [1/3], Batch [9954/21340], Train Loss: 3.2314, Train Perplexity: 25.3150\n",
            "Epoch [1/3], Batch [9955/21340], Train Loss: 3.3391, Train Perplexity: 28.1951\n",
            "Epoch [1/3], Batch [9956/21340], Train Loss: 3.2000, Train Perplexity: 24.5317\n",
            "Epoch [1/3], Batch [9957/21340], Train Loss: 3.4288, Train Perplexity: 30.8386\n",
            "Epoch [1/3], Batch [9958/21340], Train Loss: 3.1813, Train Perplexity: 24.0778\n",
            "Epoch [1/3], Batch [9959/21340], Train Loss: 3.3447, Train Perplexity: 28.3527\n",
            "Epoch [1/3], Batch [9960/21340], Train Loss: 3.3721, Train Perplexity: 29.1395\n",
            "Epoch [1/3], Batch [9961/21340], Train Loss: 3.2531, Train Perplexity: 25.8692\n",
            "Epoch [1/3], Batch [9962/21340], Train Loss: 3.1599, Train Perplexity: 23.5679\n",
            "Epoch [1/3], Batch [9963/21340], Train Loss: 3.3699, Train Perplexity: 29.0767\n",
            "Epoch [1/3], Batch [9964/21340], Train Loss: 3.2375, Train Perplexity: 25.4704\n",
            "Epoch [1/3], Batch [9965/21340], Train Loss: 3.1405, Train Perplexity: 23.1147\n",
            "Epoch [1/3], Batch [9966/21340], Train Loss: 3.2374, Train Perplexity: 25.4667\n",
            "Epoch [1/3], Batch [9967/21340], Train Loss: 3.2125, Train Perplexity: 24.8417\n",
            "Epoch [1/3], Batch [9968/21340], Train Loss: 3.2054, Train Perplexity: 24.6664\n",
            "Epoch [1/3], Batch [9969/21340], Train Loss: 3.3325, Train Perplexity: 28.0095\n",
            "Epoch [1/3], Batch [9970/21340], Train Loss: 3.4081, Train Perplexity: 30.2074\n",
            "Epoch [1/3], Batch [9971/21340], Train Loss: 3.3304, Train Perplexity: 27.9500\n",
            "Epoch [1/3], Batch [9972/21340], Train Loss: 3.2468, Train Perplexity: 25.7076\n",
            "Epoch [1/3], Batch [9973/21340], Train Loss: 3.2899, Train Perplexity: 26.8392\n",
            "Epoch [1/3], Batch [9974/21340], Train Loss: 3.2006, Train Perplexity: 24.5475\n",
            "Epoch [1/3], Batch [9975/21340], Train Loss: 3.1837, Train Perplexity: 24.1355\n",
            "Epoch [1/3], Batch [9976/21340], Train Loss: 3.3495, Train Perplexity: 28.4887\n",
            "Epoch [1/3], Batch [9977/21340], Train Loss: 3.3363, Train Perplexity: 28.1159\n",
            "Epoch [1/3], Batch [9978/21340], Train Loss: 3.2697, Train Perplexity: 26.3028\n",
            "Epoch [1/3], Batch [9979/21340], Train Loss: 3.3108, Train Perplexity: 27.4077\n",
            "Epoch [1/3], Batch [9980/21340], Train Loss: 3.1695, Train Perplexity: 23.7962\n",
            "Epoch [1/3], Batch [9981/21340], Train Loss: 3.3459, Train Perplexity: 28.3873\n",
            "Epoch [1/3], Batch [9982/21340], Train Loss: 3.1588, Train Perplexity: 23.5412\n",
            "Epoch [1/3], Batch [9983/21340], Train Loss: 3.2169, Train Perplexity: 24.9502\n",
            "Epoch [1/3], Batch [9984/21340], Train Loss: 3.3232, Train Perplexity: 27.7479\n",
            "Epoch [1/3], Batch [9985/21340], Train Loss: 3.2941, Train Perplexity: 26.9536\n",
            "Epoch [1/3], Batch [9986/21340], Train Loss: 3.2295, Train Perplexity: 25.2682\n",
            "Epoch [1/3], Batch [9987/21340], Train Loss: 3.2724, Train Perplexity: 26.3754\n",
            "Epoch [1/3], Batch [9988/21340], Train Loss: 3.2050, Train Perplexity: 24.6566\n",
            "Epoch [1/3], Batch [9989/21340], Train Loss: 3.3310, Train Perplexity: 27.9677\n",
            "Epoch [1/3], Batch [9990/21340], Train Loss: 3.2565, Train Perplexity: 25.9598\n",
            "Epoch [1/3], Batch [9991/21340], Train Loss: 3.2413, Train Perplexity: 25.5658\n",
            "Epoch [1/3], Batch [9992/21340], Train Loss: 3.1901, Train Perplexity: 24.2899\n",
            "Epoch [1/3], Batch [9993/21340], Train Loss: 3.2621, Train Perplexity: 26.1036\n",
            "Epoch [1/3], Batch [9994/21340], Train Loss: 3.2259, Train Perplexity: 25.1767\n",
            "Epoch [1/3], Batch [9995/21340], Train Loss: 3.1540, Train Perplexity: 23.4288\n",
            "Epoch [1/3], Batch [9996/21340], Train Loss: 3.4169, Train Perplexity: 30.4762\n",
            "Epoch [1/3], Batch [9997/21340], Train Loss: 3.2556, Train Perplexity: 25.9363\n",
            "Epoch [1/3], Batch [9998/21340], Train Loss: 3.2245, Train Perplexity: 25.1404\n",
            "Epoch [1/3], Batch [9999/21340], Train Loss: 3.2575, Train Perplexity: 25.9841\n",
            "Epoch [1/3], Batch [10000/21340], Train Loss: 3.3174, Train Perplexity: 27.5885\n",
            "Epoch [1/3], Batch [10001/21340], Train Loss: 3.2687, Train Perplexity: 26.2772\n",
            "Epoch [1/3], Batch [10002/21340], Train Loss: 3.1670, Train Perplexity: 23.7366\n",
            "Epoch [1/3], Batch [10003/21340], Train Loss: 3.2742, Train Perplexity: 26.4222\n",
            "Epoch [1/3], Batch [10004/21340], Train Loss: 3.1708, Train Perplexity: 23.8264\n",
            "Epoch [1/3], Batch [10005/21340], Train Loss: 3.3429, Train Perplexity: 28.3003\n",
            "Epoch [1/3], Batch [10006/21340], Train Loss: 3.2453, Train Perplexity: 25.6692\n",
            "Epoch [1/3], Batch [10007/21340], Train Loss: 3.1711, Train Perplexity: 23.8332\n",
            "Epoch [1/3], Batch [10008/21340], Train Loss: 3.1888, Train Perplexity: 24.2595\n",
            "Epoch [1/3], Batch [10009/21340], Train Loss: 3.2037, Train Perplexity: 24.6241\n",
            "Epoch [1/3], Batch [10010/21340], Train Loss: 3.1554, Train Perplexity: 23.4614\n",
            "Epoch [1/3], Batch [10011/21340], Train Loss: 3.2458, Train Perplexity: 25.6814\n",
            "Epoch [1/3], Batch [10012/21340], Train Loss: 3.5227, Train Perplexity: 33.8773\n",
            "Epoch [1/3], Batch [10013/21340], Train Loss: 3.2382, Train Perplexity: 25.4869\n",
            "Epoch [1/3], Batch [10014/21340], Train Loss: 3.2328, Train Perplexity: 25.3516\n",
            "Epoch [1/3], Batch [10015/21340], Train Loss: 3.2745, Train Perplexity: 26.4297\n",
            "Epoch [1/3], Batch [10016/21340], Train Loss: 3.2411, Train Perplexity: 25.5628\n",
            "Epoch [1/3], Batch [10017/21340], Train Loss: 3.2243, Train Perplexity: 25.1350\n",
            "Epoch [1/3], Batch [10018/21340], Train Loss: 3.1978, Train Perplexity: 24.4793\n",
            "Epoch [1/3], Batch [10019/21340], Train Loss: 3.1794, Train Perplexity: 24.0312\n",
            "Epoch [1/3], Batch [10020/21340], Train Loss: 3.2251, Train Perplexity: 25.1551\n",
            "Epoch [1/3], Batch [10021/21340], Train Loss: 3.2303, Train Perplexity: 25.2878\n",
            "Epoch [1/3], Batch [10022/21340], Train Loss: 3.2681, Train Perplexity: 26.2617\n",
            "Epoch [1/3], Batch [10023/21340], Train Loss: 3.4321, Train Perplexity: 30.9413\n",
            "Epoch [1/3], Batch [10024/21340], Train Loss: 3.2897, Train Perplexity: 26.8338\n",
            "Epoch [1/3], Batch [10025/21340], Train Loss: 3.2104, Train Perplexity: 24.7897\n",
            "Epoch [1/3], Batch [10026/21340], Train Loss: 3.2232, Train Perplexity: 25.1095\n",
            "Epoch [1/3], Batch [10027/21340], Train Loss: 3.3094, Train Perplexity: 27.3676\n",
            "Epoch [1/3], Batch [10028/21340], Train Loss: 3.2105, Train Perplexity: 24.7911\n",
            "Epoch [1/3], Batch [10029/21340], Train Loss: 3.1737, Train Perplexity: 23.8956\n",
            "Epoch [1/3], Batch [10030/21340], Train Loss: 3.1419, Train Perplexity: 23.1484\n",
            "Epoch [1/3], Batch [10031/21340], Train Loss: 3.5818, Train Perplexity: 35.9370\n",
            "Epoch [1/3], Batch [10032/21340], Train Loss: 3.1026, Train Perplexity: 22.2547\n",
            "Epoch [1/3], Batch [10033/21340], Train Loss: 3.2928, Train Perplexity: 26.9185\n",
            "Epoch [1/3], Batch [10034/21340], Train Loss: 3.2898, Train Perplexity: 26.8382\n",
            "Epoch [1/3], Batch [10035/21340], Train Loss: 3.2429, Train Perplexity: 25.6075\n",
            "Epoch [1/3], Batch [10036/21340], Train Loss: 3.1906, Train Perplexity: 24.3040\n",
            "Epoch [1/3], Batch [10037/21340], Train Loss: 3.2514, Train Perplexity: 25.8260\n",
            "Epoch [1/3], Batch [10038/21340], Train Loss: 3.2251, Train Perplexity: 25.1564\n",
            "Epoch [1/3], Batch [10039/21340], Train Loss: 3.2512, Train Perplexity: 25.8205\n",
            "Epoch [1/3], Batch [10040/21340], Train Loss: 3.3811, Train Perplexity: 29.4032\n",
            "Epoch [1/3], Batch [10041/21340], Train Loss: 3.1716, Train Perplexity: 23.8447\n",
            "Epoch [1/3], Batch [10042/21340], Train Loss: 3.4089, Train Perplexity: 30.2315\n",
            "Epoch [1/3], Batch [10043/21340], Train Loss: 3.3719, Train Perplexity: 29.1324\n",
            "Epoch [1/3], Batch [10044/21340], Train Loss: 3.1926, Train Perplexity: 24.3514\n",
            "Epoch [1/3], Batch [10045/21340], Train Loss: 3.1806, Train Perplexity: 24.0614\n",
            "Epoch [1/3], Batch [10046/21340], Train Loss: 3.3693, Train Perplexity: 29.0583\n",
            "Epoch [1/3], Batch [10047/21340], Train Loss: 3.1724, Train Perplexity: 23.8635\n",
            "Epoch [1/3], Batch [10048/21340], Train Loss: 3.2816, Train Perplexity: 26.6195\n",
            "Epoch [1/3], Batch [10049/21340], Train Loss: 3.3606, Train Perplexity: 28.8075\n",
            "Epoch [1/3], Batch [10050/21340], Train Loss: 3.2338, Train Perplexity: 25.3769\n",
            "Epoch [1/3], Batch [10051/21340], Train Loss: 3.1832, Train Perplexity: 24.1236\n",
            "Epoch [1/3], Batch [10052/21340], Train Loss: 3.2779, Train Perplexity: 26.5193\n",
            "Epoch [1/3], Batch [10053/21340], Train Loss: 3.3191, Train Perplexity: 27.6345\n",
            "Epoch [1/3], Batch [10054/21340], Train Loss: 3.2561, Train Perplexity: 25.9492\n",
            "Epoch [1/3], Batch [10055/21340], Train Loss: 3.3771, Train Perplexity: 29.2843\n",
            "Epoch [1/3], Batch [10056/21340], Train Loss: 3.2374, Train Perplexity: 25.4687\n",
            "Epoch [1/3], Batch [10057/21340], Train Loss: 3.2520, Train Perplexity: 25.8414\n",
            "Epoch [1/3], Batch [10058/21340], Train Loss: 3.2250, Train Perplexity: 25.1539\n",
            "Epoch [1/3], Batch [10059/21340], Train Loss: 3.2720, Train Perplexity: 26.3640\n",
            "Epoch [1/3], Batch [10060/21340], Train Loss: 3.2943, Train Perplexity: 26.9596\n",
            "Epoch [1/3], Batch [10061/21340], Train Loss: 3.1948, Train Perplexity: 24.4048\n",
            "Epoch [1/3], Batch [10062/21340], Train Loss: 3.3503, Train Perplexity: 28.5105\n",
            "Epoch [1/3], Batch [10063/21340], Train Loss: 3.2513, Train Perplexity: 25.8227\n",
            "Epoch [1/3], Batch [10064/21340], Train Loss: 3.2624, Train Perplexity: 26.1133\n",
            "Epoch [1/3], Batch [10065/21340], Train Loss: 3.2957, Train Perplexity: 26.9962\n",
            "Epoch [1/3], Batch [10066/21340], Train Loss: 3.2561, Train Perplexity: 25.9480\n",
            "Epoch [1/3], Batch [10067/21340], Train Loss: 3.3989, Train Perplexity: 29.9300\n",
            "Epoch [1/3], Batch [10068/21340], Train Loss: 3.1700, Train Perplexity: 23.8084\n",
            "Epoch [1/3], Batch [10069/21340], Train Loss: 3.3725, Train Perplexity: 29.1502\n",
            "Epoch [1/3], Batch [10070/21340], Train Loss: 3.2689, Train Perplexity: 26.2822\n",
            "Epoch [1/3], Batch [10071/21340], Train Loss: 3.3558, Train Perplexity: 28.6699\n",
            "Epoch [1/3], Batch [10072/21340], Train Loss: 3.2987, Train Perplexity: 27.0780\n",
            "Epoch [1/3], Batch [10073/21340], Train Loss: 3.2582, Train Perplexity: 26.0037\n",
            "Epoch [1/3], Batch [10074/21340], Train Loss: 3.1589, Train Perplexity: 23.5439\n",
            "Epoch [1/3], Batch [10075/21340], Train Loss: 3.2398, Train Perplexity: 25.5294\n",
            "Epoch [1/3], Batch [10076/21340], Train Loss: 3.2580, Train Perplexity: 25.9982\n",
            "Epoch [1/3], Batch [10077/21340], Train Loss: 3.2289, Train Perplexity: 25.2517\n",
            "Epoch [1/3], Batch [10078/21340], Train Loss: 3.2933, Train Perplexity: 26.9317\n",
            "Epoch [1/3], Batch [10079/21340], Train Loss: 3.2567, Train Perplexity: 25.9636\n",
            "Epoch [1/3], Batch [10080/21340], Train Loss: 3.3350, Train Perplexity: 28.0771\n",
            "Epoch [1/3], Batch [10081/21340], Train Loss: 3.2177, Train Perplexity: 24.9707\n",
            "Epoch [1/3], Batch [10082/21340], Train Loss: 3.3268, Train Perplexity: 27.8485\n",
            "Epoch [1/3], Batch [10083/21340], Train Loss: 3.3833, Train Perplexity: 29.4665\n",
            "Epoch [1/3], Batch [10084/21340], Train Loss: 3.1667, Train Perplexity: 23.7288\n",
            "Epoch [1/3], Batch [10085/21340], Train Loss: 3.2581, Train Perplexity: 26.0003\n",
            "Epoch [1/3], Batch [10086/21340], Train Loss: 3.1858, Train Perplexity: 24.1868\n",
            "Epoch [1/3], Batch [10087/21340], Train Loss: 3.1965, Train Perplexity: 24.4470\n",
            "Epoch [1/3], Batch [10088/21340], Train Loss: 3.4245, Train Perplexity: 30.7088\n",
            "Epoch [1/3], Batch [10089/21340], Train Loss: 3.2804, Train Perplexity: 26.5866\n",
            "Epoch [1/3], Batch [10090/21340], Train Loss: 3.1095, Train Perplexity: 22.4090\n",
            "Epoch [1/3], Batch [10091/21340], Train Loss: 3.1817, Train Perplexity: 24.0880\n",
            "Epoch [1/3], Batch [10092/21340], Train Loss: 3.4143, Train Perplexity: 30.3960\n",
            "Epoch [1/3], Batch [10093/21340], Train Loss: 3.2384, Train Perplexity: 25.4917\n",
            "Epoch [1/3], Batch [10094/21340], Train Loss: 3.1694, Train Perplexity: 23.7936\n",
            "Epoch [1/3], Batch [10095/21340], Train Loss: 3.1695, Train Perplexity: 23.7946\n",
            "Epoch [1/3], Batch [10096/21340], Train Loss: 3.3487, Train Perplexity: 28.4671\n",
            "Epoch [1/3], Batch [10097/21340], Train Loss: 3.2837, Train Perplexity: 26.6752\n",
            "Epoch [1/3], Batch [10098/21340], Train Loss: 3.1543, Train Perplexity: 23.4364\n",
            "Epoch [1/3], Batch [10099/21340], Train Loss: 3.4296, Train Perplexity: 30.8658\n",
            "Epoch [1/3], Batch [10100/21340], Train Loss: 3.2162, Train Perplexity: 24.9342\n",
            "Epoch [1/3], Batch [10101/21340], Train Loss: 3.2868, Train Perplexity: 26.7582\n",
            "Epoch [1/3], Batch [10102/21340], Train Loss: 3.3347, Train Perplexity: 28.0706\n",
            "Epoch [1/3], Batch [10103/21340], Train Loss: 3.2467, Train Perplexity: 25.7045\n",
            "Epoch [1/3], Batch [10104/21340], Train Loss: 3.2230, Train Perplexity: 25.1033\n",
            "Epoch [1/3], Batch [10105/21340], Train Loss: 3.2354, Train Perplexity: 25.4155\n",
            "Epoch [1/3], Batch [10106/21340], Train Loss: 3.1516, Train Perplexity: 23.3744\n",
            "Epoch [1/3], Batch [10107/21340], Train Loss: 3.3051, Train Perplexity: 27.2509\n",
            "Epoch [1/3], Batch [10108/21340], Train Loss: 3.3335, Train Perplexity: 28.0355\n",
            "Epoch [1/3], Batch [10109/21340], Train Loss: 3.1797, Train Perplexity: 24.0390\n",
            "Epoch [1/3], Batch [10110/21340], Train Loss: 3.1999, Train Perplexity: 24.5308\n",
            "Epoch [1/3], Batch [10111/21340], Train Loss: 3.2194, Train Perplexity: 25.0126\n",
            "Epoch [1/3], Batch [10112/21340], Train Loss: 3.2785, Train Perplexity: 26.5370\n",
            "Epoch [1/3], Batch [10113/21340], Train Loss: 3.2098, Train Perplexity: 24.7741\n",
            "Epoch [1/3], Batch [10114/21340], Train Loss: 3.3104, Train Perplexity: 27.3959\n",
            "Epoch [1/3], Batch [10115/21340], Train Loss: 3.4148, Train Perplexity: 30.4122\n",
            "Epoch [1/3], Batch [10116/21340], Train Loss: 3.2488, Train Perplexity: 25.7603\n",
            "Epoch [1/3], Batch [10117/21340], Train Loss: 3.3939, Train Perplexity: 29.7813\n",
            "Epoch [1/3], Batch [10118/21340], Train Loss: 3.2869, Train Perplexity: 26.7609\n",
            "Epoch [1/3], Batch [10119/21340], Train Loss: 3.2514, Train Perplexity: 25.8265\n",
            "Epoch [1/3], Batch [10120/21340], Train Loss: 3.2483, Train Perplexity: 25.7464\n",
            "Epoch [1/3], Batch [10121/21340], Train Loss: 3.1882, Train Perplexity: 24.2436\n",
            "Epoch [1/3], Batch [10122/21340], Train Loss: 3.2595, Train Perplexity: 26.0361\n",
            "Epoch [1/3], Batch [10123/21340], Train Loss: 3.3354, Train Perplexity: 28.0899\n",
            "Epoch [1/3], Batch [10124/21340], Train Loss: 3.2592, Train Perplexity: 26.0288\n",
            "Epoch [1/3], Batch [10125/21340], Train Loss: 3.2650, Train Perplexity: 26.1811\n",
            "Epoch [1/3], Batch [10126/21340], Train Loss: 3.3128, Train Perplexity: 27.4626\n",
            "Epoch [1/3], Batch [10127/21340], Train Loss: 3.3193, Train Perplexity: 27.6422\n",
            "Epoch [1/3], Batch [10128/21340], Train Loss: 3.4182, Train Perplexity: 30.5138\n",
            "Epoch [1/3], Batch [10129/21340], Train Loss: 3.1734, Train Perplexity: 23.8875\n",
            "Epoch [1/3], Batch [10130/21340], Train Loss: 3.1342, Train Perplexity: 22.9692\n",
            "Epoch [1/3], Batch [10131/21340], Train Loss: 3.2280, Train Perplexity: 25.2292\n",
            "Epoch [1/3], Batch [10132/21340], Train Loss: 3.2207, Train Perplexity: 25.0462\n",
            "Epoch [1/3], Batch [10133/21340], Train Loss: 3.2928, Train Perplexity: 26.9177\n",
            "Epoch [1/3], Batch [10134/21340], Train Loss: 3.2467, Train Perplexity: 25.7055\n",
            "Epoch [1/3], Batch [10135/21340], Train Loss: 3.1270, Train Perplexity: 22.8065\n",
            "Epoch [1/3], Batch [10136/21340], Train Loss: 3.1893, Train Perplexity: 24.2717\n",
            "Epoch [1/3], Batch [10137/21340], Train Loss: 3.3916, Train Perplexity: 29.7141\n",
            "Epoch [1/3], Batch [10138/21340], Train Loss: 3.1802, Train Perplexity: 24.0510\n",
            "Epoch [1/3], Batch [10139/21340], Train Loss: 3.2546, Train Perplexity: 25.9084\n",
            "Epoch [1/3], Batch [10140/21340], Train Loss: 3.2187, Train Perplexity: 24.9954\n",
            "Epoch [1/3], Batch [10141/21340], Train Loss: 3.1698, Train Perplexity: 23.8027\n",
            "Epoch [1/3], Batch [10142/21340], Train Loss: 3.4478, Train Perplexity: 31.4313\n",
            "Epoch [1/3], Batch [10143/21340], Train Loss: 3.4246, Train Perplexity: 30.7113\n",
            "Epoch [1/3], Batch [10144/21340], Train Loss: 3.3988, Train Perplexity: 29.9284\n",
            "Epoch [1/3], Batch [10145/21340], Train Loss: 3.2061, Train Perplexity: 24.6815\n",
            "Epoch [1/3], Batch [10146/21340], Train Loss: 3.3109, Train Perplexity: 27.4102\n",
            "Epoch [1/3], Batch [10147/21340], Train Loss: 3.3635, Train Perplexity: 28.8914\n",
            "Epoch [1/3], Batch [10148/21340], Train Loss: 3.3117, Train Perplexity: 27.4324\n",
            "Epoch [1/3], Batch [10149/21340], Train Loss: 3.2335, Train Perplexity: 25.3689\n",
            "Epoch [1/3], Batch [10150/21340], Train Loss: 3.2441, Train Perplexity: 25.6375\n",
            "Epoch [1/3], Batch [10151/21340], Train Loss: 3.3276, Train Perplexity: 27.8720\n",
            "Epoch [1/3], Batch [10152/21340], Train Loss: 3.2797, Train Perplexity: 26.5673\n",
            "Epoch [1/3], Batch [10153/21340], Train Loss: 3.1785, Train Perplexity: 24.0099\n",
            "Epoch [1/3], Batch [10154/21340], Train Loss: 3.2834, Train Perplexity: 26.6676\n",
            "Epoch [1/3], Batch [10155/21340], Train Loss: 3.3161, Train Perplexity: 27.5538\n",
            "Epoch [1/3], Batch [10156/21340], Train Loss: 3.2966, Train Perplexity: 27.0218\n",
            "Epoch [1/3], Batch [10157/21340], Train Loss: 3.1539, Train Perplexity: 23.4284\n",
            "Epoch [1/3], Batch [10158/21340], Train Loss: 3.3262, Train Perplexity: 27.8327\n",
            "Epoch [1/3], Batch [10159/21340], Train Loss: 3.3057, Train Perplexity: 27.2668\n",
            "Epoch [1/3], Batch [10160/21340], Train Loss: 3.2224, Train Perplexity: 25.0875\n",
            "Epoch [1/3], Batch [10161/21340], Train Loss: 3.2454, Train Perplexity: 25.6717\n",
            "Epoch [1/3], Batch [10162/21340], Train Loss: 3.2408, Train Perplexity: 25.5551\n",
            "Epoch [1/3], Batch [10163/21340], Train Loss: 3.1694, Train Perplexity: 23.7937\n",
            "Epoch [1/3], Batch [10164/21340], Train Loss: 3.2912, Train Perplexity: 26.8741\n",
            "Epoch [1/3], Batch [10165/21340], Train Loss: 3.2710, Train Perplexity: 26.3369\n",
            "Epoch [1/3], Batch [10166/21340], Train Loss: 3.3335, Train Perplexity: 28.0357\n",
            "Epoch [1/3], Batch [10167/21340], Train Loss: 3.3120, Train Perplexity: 27.4406\n",
            "Epoch [1/3], Batch [10168/21340], Train Loss: 3.2518, Train Perplexity: 25.8360\n",
            "Epoch [1/3], Batch [10169/21340], Train Loss: 3.2429, Train Perplexity: 25.6075\n",
            "Epoch [1/3], Batch [10170/21340], Train Loss: 3.2261, Train Perplexity: 25.1811\n",
            "Epoch [1/3], Batch [10171/21340], Train Loss: 3.3563, Train Perplexity: 28.6816\n",
            "Epoch [1/3], Batch [10172/21340], Train Loss: 3.2377, Train Perplexity: 25.4740\n",
            "Epoch [1/3], Batch [10173/21340], Train Loss: 3.3243, Train Perplexity: 27.7806\n",
            "Epoch [1/3], Batch [10174/21340], Train Loss: 3.1854, Train Perplexity: 24.1769\n",
            "Epoch [1/3], Batch [10175/21340], Train Loss: 3.3010, Train Perplexity: 27.1398\n",
            "Epoch [1/3], Batch [10176/21340], Train Loss: 3.2396, Train Perplexity: 25.5231\n",
            "Epoch [1/3], Batch [10177/21340], Train Loss: 3.2627, Train Perplexity: 26.1212\n",
            "Epoch [1/3], Batch [10178/21340], Train Loss: 3.2034, Train Perplexity: 24.6157\n",
            "Epoch [1/3], Batch [10179/21340], Train Loss: 3.1960, Train Perplexity: 24.4344\n",
            "Epoch [1/3], Batch [10180/21340], Train Loss: 3.2465, Train Perplexity: 25.7010\n",
            "Epoch [1/3], Batch [10181/21340], Train Loss: 3.2062, Train Perplexity: 24.6844\n",
            "Epoch [1/3], Batch [10182/21340], Train Loss: 3.2043, Train Perplexity: 24.6380\n",
            "Epoch [1/3], Batch [10183/21340], Train Loss: 3.2196, Train Perplexity: 25.0181\n",
            "Epoch [1/3], Batch [10184/21340], Train Loss: 3.2801, Train Perplexity: 26.5783\n",
            "Epoch [1/3], Batch [10185/21340], Train Loss: 3.2161, Train Perplexity: 24.9309\n",
            "Epoch [1/3], Batch [10186/21340], Train Loss: 3.1354, Train Perplexity: 22.9984\n",
            "Epoch [1/3], Batch [10187/21340], Train Loss: 3.1945, Train Perplexity: 24.3975\n",
            "Epoch [1/3], Batch [10188/21340], Train Loss: 3.3017, Train Perplexity: 27.1585\n",
            "Epoch [1/3], Batch [10189/21340], Train Loss: 3.2173, Train Perplexity: 24.9596\n",
            "Epoch [1/3], Batch [10190/21340], Train Loss: 3.2399, Train Perplexity: 25.5314\n",
            "Epoch [1/3], Batch [10191/21340], Train Loss: 3.1774, Train Perplexity: 23.9839\n",
            "Epoch [1/3], Batch [10192/21340], Train Loss: 3.2703, Train Perplexity: 26.3184\n",
            "Epoch [1/3], Batch [10193/21340], Train Loss: 3.1950, Train Perplexity: 24.4103\n",
            "Epoch [1/3], Batch [10194/21340], Train Loss: 3.1482, Train Perplexity: 23.2938\n",
            "Epoch [1/3], Batch [10195/21340], Train Loss: 3.1963, Train Perplexity: 24.4411\n",
            "Epoch [1/3], Batch [10196/21340], Train Loss: 3.2035, Train Perplexity: 24.6178\n",
            "Epoch [1/3], Batch [10197/21340], Train Loss: 3.3412, Train Perplexity: 28.2536\n",
            "Epoch [1/3], Batch [10198/21340], Train Loss: 3.2201, Train Perplexity: 25.0308\n",
            "Epoch [1/3], Batch [10199/21340], Train Loss: 3.2797, Train Perplexity: 26.5688\n",
            "Epoch [1/3], Batch [10200/21340], Train Loss: 3.2670, Train Perplexity: 26.2334\n",
            "Epoch [1/3], Batch [10201/21340], Train Loss: 3.2022, Train Perplexity: 24.5859\n",
            "Epoch [1/3], Batch [10202/21340], Train Loss: 3.3363, Train Perplexity: 28.1154\n",
            "Epoch [1/3], Batch [10203/21340], Train Loss: 3.1724, Train Perplexity: 23.8643\n",
            "Epoch [1/3], Batch [10204/21340], Train Loss: 3.2814, Train Perplexity: 26.6135\n",
            "Epoch [1/3], Batch [10205/21340], Train Loss: 3.2830, Train Perplexity: 26.6547\n",
            "Epoch [1/3], Batch [10206/21340], Train Loss: 3.2928, Train Perplexity: 26.9183\n",
            "Epoch [1/3], Batch [10207/21340], Train Loss: 3.2263, Train Perplexity: 25.1874\n",
            "Epoch [1/3], Batch [10208/21340], Train Loss: 3.2135, Train Perplexity: 24.8667\n",
            "Epoch [1/3], Batch [10209/21340], Train Loss: 3.3173, Train Perplexity: 27.5870\n",
            "Epoch [1/3], Batch [10210/21340], Train Loss: 3.2447, Train Perplexity: 25.6545\n",
            "Epoch [1/3], Batch [10211/21340], Train Loss: 3.1543, Train Perplexity: 23.4362\n",
            "Epoch [1/3], Batch [10212/21340], Train Loss: 3.3199, Train Perplexity: 27.6564\n",
            "Epoch [1/3], Batch [10213/21340], Train Loss: 3.2402, Train Perplexity: 25.5384\n",
            "Epoch [1/3], Batch [10214/21340], Train Loss: 3.2965, Train Perplexity: 27.0183\n",
            "Epoch [1/3], Batch [10215/21340], Train Loss: 3.2402, Train Perplexity: 25.5392\n",
            "Epoch [1/3], Batch [10216/21340], Train Loss: 3.2181, Train Perplexity: 24.9813\n",
            "Epoch [1/3], Batch [10217/21340], Train Loss: 3.3450, Train Perplexity: 28.3603\n",
            "Epoch [1/3], Batch [10218/21340], Train Loss: 3.2137, Train Perplexity: 24.8714\n",
            "Epoch [1/3], Batch [10219/21340], Train Loss: 3.2007, Train Perplexity: 24.5502\n",
            "Epoch [1/3], Batch [10220/21340], Train Loss: 3.2016, Train Perplexity: 24.5724\n",
            "Epoch [1/3], Batch [10221/21340], Train Loss: 3.2656, Train Perplexity: 26.1964\n",
            "Epoch [1/3], Batch [10222/21340], Train Loss: 3.2659, Train Perplexity: 26.2041\n",
            "Epoch [1/3], Batch [10223/21340], Train Loss: 3.2740, Train Perplexity: 26.4163\n",
            "Epoch [1/3], Batch [10224/21340], Train Loss: 3.3292, Train Perplexity: 27.9164\n",
            "Epoch [1/3], Batch [10225/21340], Train Loss: 3.3167, Train Perplexity: 27.5688\n",
            "Epoch [1/3], Batch [10226/21340], Train Loss: 3.3000, Train Perplexity: 27.1126\n",
            "Epoch [1/3], Batch [10227/21340], Train Loss: 3.2329, Train Perplexity: 25.3524\n",
            "Epoch [1/3], Batch [10228/21340], Train Loss: 3.2789, Train Perplexity: 26.5454\n",
            "Epoch [1/3], Batch [10229/21340], Train Loss: 3.2100, Train Perplexity: 24.7801\n",
            "Epoch [1/3], Batch [10230/21340], Train Loss: 3.2395, Train Perplexity: 25.5210\n",
            "Epoch [1/3], Batch [10231/21340], Train Loss: 3.2016, Train Perplexity: 24.5710\n",
            "Epoch [1/3], Batch [10232/21340], Train Loss: 3.2357, Train Perplexity: 25.4231\n",
            "Epoch [1/3], Batch [10233/21340], Train Loss: 3.3416, Train Perplexity: 28.2640\n",
            "Epoch [1/3], Batch [10234/21340], Train Loss: 3.3291, Train Perplexity: 27.9142\n",
            "Epoch [1/3], Batch [10235/21340], Train Loss: 3.2561, Train Perplexity: 25.9483\n",
            "Epoch [1/3], Batch [10236/21340], Train Loss: 3.2168, Train Perplexity: 24.9489\n",
            "Epoch [1/3], Batch [10237/21340], Train Loss: 3.1913, Train Perplexity: 24.3203\n",
            "Epoch [1/3], Batch [10238/21340], Train Loss: 3.2031, Train Perplexity: 24.6087\n",
            "Epoch [1/3], Batch [10239/21340], Train Loss: 3.1770, Train Perplexity: 23.9759\n",
            "Epoch [1/3], Batch [10240/21340], Train Loss: 3.2033, Train Perplexity: 24.6138\n",
            "Epoch [1/3], Batch [10241/21340], Train Loss: 3.2211, Train Perplexity: 25.0567\n",
            "Epoch [1/3], Batch [10242/21340], Train Loss: 3.2021, Train Perplexity: 24.5843\n",
            "Epoch [1/3], Batch [10243/21340], Train Loss: 3.3211, Train Perplexity: 27.6898\n",
            "Epoch [1/3], Batch [10244/21340], Train Loss: 3.2538, Train Perplexity: 25.8887\n",
            "Epoch [1/3], Batch [10245/21340], Train Loss: 3.2626, Train Perplexity: 26.1184\n",
            "Epoch [1/3], Batch [10246/21340], Train Loss: 3.3421, Train Perplexity: 28.2796\n",
            "Epoch [1/3], Batch [10247/21340], Train Loss: 3.2705, Train Perplexity: 26.3254\n",
            "Epoch [1/3], Batch [10248/21340], Train Loss: 3.2492, Train Perplexity: 25.7691\n",
            "Epoch [1/3], Batch [10249/21340], Train Loss: 3.3335, Train Perplexity: 28.0358\n",
            "Epoch [1/3], Batch [10250/21340], Train Loss: 3.2396, Train Perplexity: 25.5238\n",
            "Epoch [1/3], Batch [10251/21340], Train Loss: 3.1839, Train Perplexity: 24.1407\n",
            "Epoch [1/3], Batch [10252/21340], Train Loss: 3.2250, Train Perplexity: 25.1531\n",
            "Epoch [1/3], Batch [10253/21340], Train Loss: 3.2680, Train Perplexity: 26.2597\n",
            "Epoch [1/3], Batch [10254/21340], Train Loss: 3.2026, Train Perplexity: 24.5971\n",
            "Epoch [1/3], Batch [10255/21340], Train Loss: 3.2293, Train Perplexity: 25.2628\n",
            "Epoch [1/3], Batch [10256/21340], Train Loss: 3.1934, Train Perplexity: 24.3705\n",
            "Epoch [1/3], Batch [10257/21340], Train Loss: 3.2228, Train Perplexity: 25.0973\n",
            "Epoch [1/3], Batch [10258/21340], Train Loss: 3.2930, Train Perplexity: 26.9243\n",
            "Epoch [1/3], Batch [10259/21340], Train Loss: 3.2265, Train Perplexity: 25.1921\n",
            "Epoch [1/3], Batch [10260/21340], Train Loss: 3.2746, Train Perplexity: 26.4318\n",
            "Epoch [1/3], Batch [10261/21340], Train Loss: 3.2828, Train Perplexity: 26.6505\n",
            "Epoch [1/3], Batch [10262/21340], Train Loss: 3.1296, Train Perplexity: 22.8651\n",
            "Epoch [1/3], Batch [10263/21340], Train Loss: 3.2392, Train Perplexity: 25.5136\n",
            "Epoch [1/3], Batch [10264/21340], Train Loss: 3.3119, Train Perplexity: 27.4366\n",
            "Epoch [1/3], Batch [10265/21340], Train Loss: 3.1691, Train Perplexity: 23.7850\n",
            "Epoch [1/3], Batch [10266/21340], Train Loss: 3.2451, Train Perplexity: 25.6634\n",
            "Epoch [1/3], Batch [10267/21340], Train Loss: 3.1606, Train Perplexity: 23.5840\n",
            "Epoch [1/3], Batch [10268/21340], Train Loss: 3.3240, Train Perplexity: 27.7725\n",
            "Epoch [1/3], Batch [10269/21340], Train Loss: 3.2290, Train Perplexity: 25.2549\n",
            "Epoch [1/3], Batch [10270/21340], Train Loss: 3.2707, Train Perplexity: 26.3310\n",
            "Epoch [1/3], Batch [10271/21340], Train Loss: 3.2362, Train Perplexity: 25.4363\n",
            "Epoch [1/3], Batch [10272/21340], Train Loss: 3.3471, Train Perplexity: 28.4189\n",
            "Epoch [1/3], Batch [10273/21340], Train Loss: 3.3468, Train Perplexity: 28.4105\n",
            "Epoch [1/3], Batch [10274/21340], Train Loss: 3.2727, Train Perplexity: 26.3833\n",
            "Epoch [1/3], Batch [10275/21340], Train Loss: 3.2631, Train Perplexity: 26.1296\n",
            "Epoch [1/3], Batch [10276/21340], Train Loss: 3.2108, Train Perplexity: 24.7998\n",
            "Epoch [1/3], Batch [10277/21340], Train Loss: 3.2170, Train Perplexity: 24.9520\n",
            "Epoch [1/3], Batch [10278/21340], Train Loss: 3.2775, Train Perplexity: 26.5094\n",
            "Epoch [1/3], Batch [10279/21340], Train Loss: 3.1804, Train Perplexity: 24.0558\n",
            "Epoch [1/3], Batch [10280/21340], Train Loss: 3.2563, Train Perplexity: 25.9531\n",
            "Epoch [1/3], Batch [10281/21340], Train Loss: 3.2159, Train Perplexity: 24.9245\n",
            "Epoch [1/3], Batch [10282/21340], Train Loss: 3.1440, Train Perplexity: 23.1969\n",
            "Epoch [1/3], Batch [10283/21340], Train Loss: 3.2505, Train Perplexity: 25.8042\n",
            "Epoch [1/3], Batch [10284/21340], Train Loss: 3.2848, Train Perplexity: 26.7031\n",
            "Epoch [1/3], Batch [10285/21340], Train Loss: 3.2132, Train Perplexity: 24.8591\n",
            "Epoch [1/3], Batch [10286/21340], Train Loss: 3.2890, Train Perplexity: 26.8170\n",
            "Epoch [1/3], Batch [10287/21340], Train Loss: 3.3116, Train Perplexity: 27.4299\n",
            "Epoch [1/3], Batch [10288/21340], Train Loss: 3.2215, Train Perplexity: 25.0654\n",
            "Epoch [1/3], Batch [10289/21340], Train Loss: 3.2293, Train Perplexity: 25.2613\n",
            "Epoch [1/3], Batch [10290/21340], Train Loss: 3.4487, Train Perplexity: 31.4597\n",
            "Epoch [1/3], Batch [10291/21340], Train Loss: 3.1908, Train Perplexity: 24.3089\n",
            "Epoch [1/3], Batch [10292/21340], Train Loss: 3.2439, Train Perplexity: 25.6344\n",
            "Epoch [1/3], Batch [10293/21340], Train Loss: 3.2574, Train Perplexity: 25.9829\n",
            "Epoch [1/3], Batch [10294/21340], Train Loss: 3.3201, Train Perplexity: 27.6624\n",
            "Epoch [1/3], Batch [10295/21340], Train Loss: 3.2225, Train Perplexity: 25.0914\n",
            "Epoch [1/3], Batch [10296/21340], Train Loss: 3.2814, Train Perplexity: 26.6123\n",
            "Epoch [1/3], Batch [10297/21340], Train Loss: 3.2212, Train Perplexity: 25.0573\n",
            "Epoch [1/3], Batch [10298/21340], Train Loss: 3.2165, Train Perplexity: 24.9415\n",
            "Epoch [1/3], Batch [10299/21340], Train Loss: 3.3253, Train Perplexity: 27.8086\n",
            "Epoch [1/3], Batch [10300/21340], Train Loss: 3.2353, Train Perplexity: 25.4138\n",
            "Epoch [1/3], Batch [10301/21340], Train Loss: 3.2146, Train Perplexity: 24.8940\n",
            "Epoch [1/3], Batch [10302/21340], Train Loss: 3.1646, Train Perplexity: 23.6800\n",
            "Epoch [1/3], Batch [10303/21340], Train Loss: 3.3440, Train Perplexity: 28.3309\n",
            "Epoch [1/3], Batch [10304/21340], Train Loss: 3.4334, Train Perplexity: 30.9811\n",
            "Epoch [1/3], Batch [10305/21340], Train Loss: 3.2410, Train Perplexity: 25.5605\n",
            "Epoch [1/3], Batch [10306/21340], Train Loss: 3.3062, Train Perplexity: 27.2811\n",
            "Epoch [1/3], Batch [10307/21340], Train Loss: 3.3582, Train Perplexity: 28.7371\n",
            "Epoch [1/3], Batch [10308/21340], Train Loss: 3.1709, Train Perplexity: 23.8298\n",
            "Epoch [1/3], Batch [10309/21340], Train Loss: 3.2471, Train Perplexity: 25.7169\n",
            "Epoch [1/3], Batch [10310/21340], Train Loss: 3.3439, Train Perplexity: 28.3298\n",
            "Epoch [1/3], Batch [10311/21340], Train Loss: 3.1729, Train Perplexity: 23.8764\n",
            "Epoch [1/3], Batch [10312/21340], Train Loss: 3.1950, Train Perplexity: 24.4094\n",
            "Epoch [1/3], Batch [10313/21340], Train Loss: 3.2217, Train Perplexity: 25.0716\n",
            "Epoch [1/3], Batch [10314/21340], Train Loss: 3.1668, Train Perplexity: 23.7318\n",
            "Epoch [1/3], Batch [10315/21340], Train Loss: 3.2152, Train Perplexity: 24.9080\n",
            "Epoch [1/3], Batch [10316/21340], Train Loss: 3.2036, Train Perplexity: 24.6213\n",
            "Epoch [1/3], Batch [10317/21340], Train Loss: 3.1838, Train Perplexity: 24.1379\n",
            "Epoch [1/3], Batch [10318/21340], Train Loss: 3.2676, Train Perplexity: 26.2492\n",
            "Epoch [1/3], Batch [10319/21340], Train Loss: 3.3332, Train Perplexity: 28.0278\n",
            "Epoch [1/3], Batch [10320/21340], Train Loss: 3.3101, Train Perplexity: 27.3869\n",
            "Epoch [1/3], Batch [10321/21340], Train Loss: 3.1971, Train Perplexity: 24.4626\n",
            "Epoch [1/3], Batch [10322/21340], Train Loss: 3.1736, Train Perplexity: 23.8927\n",
            "Epoch [1/3], Batch [10323/21340], Train Loss: 3.2989, Train Perplexity: 27.0827\n",
            "Epoch [1/3], Batch [10324/21340], Train Loss: 3.2852, Train Perplexity: 26.7149\n",
            "Epoch [1/3], Batch [10325/21340], Train Loss: 3.1737, Train Perplexity: 23.8947\n",
            "Epoch [1/3], Batch [10326/21340], Train Loss: 3.1955, Train Perplexity: 24.4216\n",
            "Epoch [1/3], Batch [10327/21340], Train Loss: 3.1193, Train Perplexity: 22.6299\n",
            "Epoch [1/3], Batch [10328/21340], Train Loss: 3.1881, Train Perplexity: 24.2428\n",
            "Epoch [1/3], Batch [10329/21340], Train Loss: 3.2619, Train Perplexity: 26.0986\n",
            "Epoch [1/3], Batch [10330/21340], Train Loss: 3.2196, Train Perplexity: 25.0176\n",
            "Epoch [1/3], Batch [10331/21340], Train Loss: 3.2471, Train Perplexity: 25.7155\n",
            "Epoch [1/3], Batch [10332/21340], Train Loss: 3.2357, Train Perplexity: 25.4235\n",
            "Epoch [1/3], Batch [10333/21340], Train Loss: 3.2971, Train Perplexity: 27.0331\n",
            "Epoch [1/3], Batch [10334/21340], Train Loss: 3.1626, Train Perplexity: 23.6309\n",
            "Epoch [1/3], Batch [10335/21340], Train Loss: 3.4072, Train Perplexity: 30.1811\n",
            "Epoch [1/3], Batch [10336/21340], Train Loss: 3.1499, Train Perplexity: 23.3345\n",
            "Epoch [1/3], Batch [10337/21340], Train Loss: 3.1709, Train Perplexity: 23.8301\n",
            "Epoch [1/3], Batch [10338/21340], Train Loss: 3.2905, Train Perplexity: 26.8552\n",
            "Epoch [1/3], Batch [10339/21340], Train Loss: 3.2785, Train Perplexity: 26.5361\n",
            "Epoch [1/3], Batch [10340/21340], Train Loss: 3.2653, Train Perplexity: 26.1874\n",
            "Epoch [1/3], Batch [10341/21340], Train Loss: 3.3166, Train Perplexity: 27.5652\n",
            "Epoch [1/3], Batch [10342/21340], Train Loss: 3.1920, Train Perplexity: 24.3378\n",
            "Epoch [1/3], Batch [10343/21340], Train Loss: 3.2074, Train Perplexity: 24.7157\n",
            "Epoch [1/3], Batch [10344/21340], Train Loss: 3.1118, Train Perplexity: 22.4623\n",
            "Epoch [1/3], Batch [10345/21340], Train Loss: 3.2418, Train Perplexity: 25.5800\n",
            "Epoch [1/3], Batch [10346/21340], Train Loss: 3.2778, Train Perplexity: 26.5169\n",
            "Epoch [1/3], Batch [10347/21340], Train Loss: 3.1823, Train Perplexity: 24.1026\n",
            "Epoch [1/3], Batch [10348/21340], Train Loss: 3.2183, Train Perplexity: 24.9857\n",
            "Epoch [1/3], Batch [10349/21340], Train Loss: 3.3147, Train Perplexity: 27.5147\n",
            "Epoch [1/3], Batch [10350/21340], Train Loss: 3.2176, Train Perplexity: 24.9671\n",
            "Epoch [1/3], Batch [10351/21340], Train Loss: 3.2164, Train Perplexity: 24.9391\n",
            "Epoch [1/3], Batch [10352/21340], Train Loss: 3.2907, Train Perplexity: 26.8604\n",
            "Epoch [1/3], Batch [10353/21340], Train Loss: 3.3082, Train Perplexity: 27.3352\n",
            "Epoch [1/3], Batch [10354/21340], Train Loss: 3.3047, Train Perplexity: 27.2415\n",
            "Epoch [1/3], Batch [10355/21340], Train Loss: 3.2189, Train Perplexity: 25.0011\n",
            "Epoch [1/3], Batch [10356/21340], Train Loss: 3.2776, Train Perplexity: 26.5114\n",
            "Epoch [1/3], Batch [10357/21340], Train Loss: 3.2064, Train Perplexity: 24.6910\n",
            "Epoch [1/3], Batch [10358/21340], Train Loss: 3.3275, Train Perplexity: 27.8681\n",
            "Epoch [1/3], Batch [10359/21340], Train Loss: 3.2731, Train Perplexity: 26.3926\n",
            "Epoch [1/3], Batch [10360/21340], Train Loss: 3.1724, Train Perplexity: 23.8656\n",
            "Epoch [1/3], Batch [10361/21340], Train Loss: 3.3817, Train Perplexity: 29.4216\n",
            "Epoch [1/3], Batch [10362/21340], Train Loss: 3.2068, Train Perplexity: 24.7005\n",
            "Epoch [1/3], Batch [10363/21340], Train Loss: 3.2517, Train Perplexity: 25.8337\n",
            "Epoch [1/3], Batch [10364/21340], Train Loss: 3.2809, Train Perplexity: 26.5990\n",
            "Epoch [1/3], Batch [10365/21340], Train Loss: 3.2686, Train Perplexity: 26.2745\n",
            "Epoch [1/3], Batch [10366/21340], Train Loss: 3.3682, Train Perplexity: 29.0271\n",
            "Epoch [1/3], Batch [10367/21340], Train Loss: 3.3507, Train Perplexity: 28.5233\n",
            "Epoch [1/3], Batch [10368/21340], Train Loss: 3.2996, Train Perplexity: 27.1006\n",
            "Epoch [1/3], Batch [10369/21340], Train Loss: 3.2389, Train Perplexity: 25.5051\n",
            "Epoch [1/3], Batch [10370/21340], Train Loss: 3.3194, Train Perplexity: 27.6425\n",
            "Epoch [1/3], Batch [10371/21340], Train Loss: 3.2479, Train Perplexity: 25.7370\n",
            "Epoch [1/3], Batch [10372/21340], Train Loss: 3.3555, Train Perplexity: 28.6597\n",
            "Epoch [1/3], Batch [10373/21340], Train Loss: 3.2764, Train Perplexity: 26.4798\n",
            "Epoch [1/3], Batch [10374/21340], Train Loss: 3.1938, Train Perplexity: 24.3799\n",
            "Epoch [1/3], Batch [10375/21340], Train Loss: 3.2421, Train Perplexity: 25.5875\n",
            "Epoch [1/3], Batch [10376/21340], Train Loss: 3.2007, Train Perplexity: 24.5486\n",
            "Epoch [1/3], Batch [10377/21340], Train Loss: 3.1334, Train Perplexity: 22.9510\n",
            "Epoch [1/3], Batch [10378/21340], Train Loss: 3.2489, Train Perplexity: 25.7622\n",
            "Epoch [1/3], Batch [10379/21340], Train Loss: 3.2336, Train Perplexity: 25.3703\n",
            "Epoch [1/3], Batch [10380/21340], Train Loss: 3.1910, Train Perplexity: 24.3120\n",
            "Epoch [1/3], Batch [10381/21340], Train Loss: 3.1822, Train Perplexity: 24.0996\n",
            "Epoch [1/3], Batch [10382/21340], Train Loss: 3.2237, Train Perplexity: 25.1204\n",
            "Epoch [1/3], Batch [10383/21340], Train Loss: 3.2223, Train Perplexity: 25.0861\n",
            "Epoch [1/3], Batch [10384/21340], Train Loss: 3.3175, Train Perplexity: 27.5917\n",
            "Epoch [1/3], Batch [10385/21340], Train Loss: 3.2494, Train Perplexity: 25.7759\n",
            "Epoch [1/3], Batch [10386/21340], Train Loss: 3.2176, Train Perplexity: 24.9676\n",
            "Epoch [1/3], Batch [10387/21340], Train Loss: 3.2406, Train Perplexity: 25.5490\n",
            "Epoch [1/3], Batch [10388/21340], Train Loss: 3.1763, Train Perplexity: 23.9579\n",
            "Epoch [1/3], Batch [10389/21340], Train Loss: 3.2504, Train Perplexity: 25.8007\n",
            "Epoch [1/3], Batch [10390/21340], Train Loss: 3.1627, Train Perplexity: 23.6351\n",
            "Epoch [1/3], Batch [10391/21340], Train Loss: 3.2705, Train Perplexity: 26.3258\n",
            "Epoch [1/3], Batch [10392/21340], Train Loss: 3.3706, Train Perplexity: 29.0945\n",
            "Epoch [1/3], Batch [10393/21340], Train Loss: 3.3591, Train Perplexity: 28.7637\n",
            "Epoch [1/3], Batch [10394/21340], Train Loss: 3.3199, Train Perplexity: 27.6582\n",
            "Epoch [1/3], Batch [10395/21340], Train Loss: 3.3177, Train Perplexity: 27.5957\n",
            "Epoch [1/3], Batch [10396/21340], Train Loss: 3.2171, Train Perplexity: 24.9563\n",
            "Epoch [1/3], Batch [10397/21340], Train Loss: 3.3134, Train Perplexity: 27.4772\n",
            "Epoch [1/3], Batch [10398/21340], Train Loss: 3.2889, Train Perplexity: 26.8135\n",
            "Epoch [1/3], Batch [10399/21340], Train Loss: 3.2253, Train Perplexity: 25.1604\n",
            "Epoch [1/3], Batch [10400/21340], Train Loss: 3.3491, Train Perplexity: 28.4766\n",
            "Epoch [1/3], Batch [10401/21340], Train Loss: 3.2532, Train Perplexity: 25.8741\n",
            "Epoch [1/3], Batch [10402/21340], Train Loss: 3.1534, Train Perplexity: 23.4162\n",
            "Epoch [1/3], Batch [10403/21340], Train Loss: 3.2896, Train Perplexity: 26.8325\n",
            "Epoch [1/3], Batch [10404/21340], Train Loss: 3.2129, Train Perplexity: 24.8511\n",
            "Epoch [1/3], Batch [10405/21340], Train Loss: 3.2175, Train Perplexity: 24.9657\n",
            "Epoch [1/3], Batch [10406/21340], Train Loss: 3.2192, Train Perplexity: 25.0075\n",
            "Epoch [1/3], Batch [10407/21340], Train Loss: 3.2309, Train Perplexity: 25.3027\n",
            "Epoch [1/3], Batch [10408/21340], Train Loss: 3.1454, Train Perplexity: 23.2294\n",
            "Epoch [1/3], Batch [10409/21340], Train Loss: 3.1955, Train Perplexity: 24.4232\n",
            "Epoch [1/3], Batch [10410/21340], Train Loss: 3.3082, Train Perplexity: 27.3355\n",
            "Epoch [1/3], Batch [10411/21340], Train Loss: 3.1483, Train Perplexity: 23.2973\n",
            "Epoch [1/3], Batch [10412/21340], Train Loss: 3.1260, Train Perplexity: 22.7823\n",
            "Epoch [1/3], Batch [10413/21340], Train Loss: 3.3331, Train Perplexity: 28.0248\n",
            "Epoch [1/3], Batch [10414/21340], Train Loss: 3.2056, Train Perplexity: 24.6708\n",
            "Epoch [1/3], Batch [10415/21340], Train Loss: 3.2538, Train Perplexity: 25.8876\n",
            "Epoch [1/3], Batch [10416/21340], Train Loss: 3.2396, Train Perplexity: 25.5239\n",
            "Epoch [1/3], Batch [10417/21340], Train Loss: 3.2233, Train Perplexity: 25.1111\n",
            "Epoch [1/3], Batch [10418/21340], Train Loss: 3.2579, Train Perplexity: 25.9958\n",
            "Epoch [1/3], Batch [10419/21340], Train Loss: 3.2293, Train Perplexity: 25.2628\n",
            "Epoch [1/3], Batch [10420/21340], Train Loss: 3.2724, Train Perplexity: 26.3758\n",
            "Epoch [1/3], Batch [10421/21340], Train Loss: 3.2782, Train Perplexity: 26.5281\n",
            "Epoch [1/3], Batch [10422/21340], Train Loss: 3.2855, Train Perplexity: 26.7235\n",
            "Epoch [1/3], Batch [10423/21340], Train Loss: 3.3298, Train Perplexity: 27.9332\n",
            "Epoch [1/3], Batch [10424/21340], Train Loss: 3.2520, Train Perplexity: 25.8431\n",
            "Epoch [1/3], Batch [10425/21340], Train Loss: 3.2772, Train Perplexity: 26.5005\n",
            "Epoch [1/3], Batch [10426/21340], Train Loss: 3.3141, Train Perplexity: 27.4975\n",
            "Epoch [1/3], Batch [10427/21340], Train Loss: 3.2954, Train Perplexity: 26.9871\n",
            "Epoch [1/3], Batch [10428/21340], Train Loss: 3.2133, Train Perplexity: 24.8611\n",
            "Epoch [1/3], Batch [10429/21340], Train Loss: 3.2697, Train Perplexity: 26.3024\n",
            "Epoch [1/3], Batch [10430/21340], Train Loss: 3.3204, Train Perplexity: 27.6719\n",
            "Epoch [1/3], Batch [10431/21340], Train Loss: 3.1933, Train Perplexity: 24.3688\n",
            "Epoch [1/3], Batch [10432/21340], Train Loss: 3.3735, Train Perplexity: 29.1808\n",
            "Epoch [1/3], Batch [10433/21340], Train Loss: 3.2873, Train Perplexity: 26.7705\n",
            "Epoch [1/3], Batch [10434/21340], Train Loss: 3.2052, Train Perplexity: 24.6612\n",
            "Epoch [1/3], Batch [10435/21340], Train Loss: 3.5089, Train Perplexity: 33.4126\n",
            "Epoch [1/3], Batch [10436/21340], Train Loss: 3.3219, Train Perplexity: 27.7143\n",
            "Epoch [1/3], Batch [10437/21340], Train Loss: 3.2677, Train Perplexity: 26.2504\n",
            "Epoch [1/3], Batch [10438/21340], Train Loss: 3.1983, Train Perplexity: 24.4899\n",
            "Epoch [1/3], Batch [10439/21340], Train Loss: 3.2675, Train Perplexity: 26.2457\n",
            "Epoch [1/3], Batch [10440/21340], Train Loss: 3.2249, Train Perplexity: 25.1502\n",
            "Epoch [1/3], Batch [10441/21340], Train Loss: 3.2905, Train Perplexity: 26.8571\n",
            "Epoch [1/3], Batch [10442/21340], Train Loss: 3.1959, Train Perplexity: 24.4324\n",
            "Epoch [1/3], Batch [10443/21340], Train Loss: 3.2090, Train Perplexity: 24.7546\n",
            "Epoch [1/3], Batch [10444/21340], Train Loss: 3.2517, Train Perplexity: 25.8349\n",
            "Epoch [1/3], Batch [10445/21340], Train Loss: 3.3256, Train Perplexity: 27.8166\n",
            "Epoch [1/3], Batch [10446/21340], Train Loss: 3.2432, Train Perplexity: 25.6160\n",
            "Epoch [1/3], Batch [10447/21340], Train Loss: 3.1988, Train Perplexity: 24.5020\n",
            "Epoch [1/3], Batch [10448/21340], Train Loss: 3.2563, Train Perplexity: 25.9522\n",
            "Epoch [1/3], Batch [10449/21340], Train Loss: 3.2042, Train Perplexity: 24.6354\n",
            "Epoch [1/3], Batch [10450/21340], Train Loss: 3.2784, Train Perplexity: 26.5324\n",
            "Epoch [1/3], Batch [10451/21340], Train Loss: 3.2568, Train Perplexity: 25.9654\n",
            "Epoch [1/3], Batch [10452/21340], Train Loss: 3.1947, Train Perplexity: 24.4022\n",
            "Epoch [1/3], Batch [10453/21340], Train Loss: 3.3335, Train Perplexity: 28.0369\n",
            "Epoch [1/3], Batch [10454/21340], Train Loss: 3.2150, Train Perplexity: 24.9032\n",
            "Epoch [1/3], Batch [10455/21340], Train Loss: 3.2071, Train Perplexity: 24.7061\n",
            "Epoch [1/3], Batch [10456/21340], Train Loss: 3.1449, Train Perplexity: 23.2170\n",
            "Epoch [1/3], Batch [10457/21340], Train Loss: 3.1684, Train Perplexity: 23.7692\n",
            "Epoch [1/3], Batch [10458/21340], Train Loss: 3.2561, Train Perplexity: 25.9482\n",
            "Epoch [1/3], Batch [10459/21340], Train Loss: 3.1798, Train Perplexity: 24.0416\n",
            "Epoch [1/3], Batch [10460/21340], Train Loss: 3.3837, Train Perplexity: 29.4800\n",
            "Epoch [1/3], Batch [10461/21340], Train Loss: 3.2451, Train Perplexity: 25.6632\n",
            "Epoch [1/3], Batch [10462/21340], Train Loss: 3.2818, Train Perplexity: 26.6224\n",
            "Epoch [1/3], Batch [10463/21340], Train Loss: 3.2106, Train Perplexity: 24.7935\n",
            "Epoch [1/3], Batch [10464/21340], Train Loss: 3.2851, Train Perplexity: 26.7124\n",
            "Epoch [1/3], Batch [10465/21340], Train Loss: 3.2418, Train Perplexity: 25.5795\n",
            "Epoch [1/3], Batch [10466/21340], Train Loss: 3.2989, Train Perplexity: 27.0816\n",
            "Epoch [1/3], Batch [10467/21340], Train Loss: 3.2274, Train Perplexity: 25.2134\n",
            "Epoch [1/3], Batch [10468/21340], Train Loss: 3.2210, Train Perplexity: 25.0527\n",
            "Epoch [1/3], Batch [10469/21340], Train Loss: 3.2549, Train Perplexity: 25.9182\n",
            "Epoch [1/3], Batch [10470/21340], Train Loss: 3.2256, Train Perplexity: 25.1676\n",
            "Epoch [1/3], Batch [10471/21340], Train Loss: 3.4343, Train Perplexity: 31.0091\n",
            "Epoch [1/3], Batch [10472/21340], Train Loss: 3.2207, Train Perplexity: 25.0464\n",
            "Epoch [1/3], Batch [10473/21340], Train Loss: 3.2489, Train Perplexity: 25.7614\n",
            "Epoch [1/3], Batch [10474/21340], Train Loss: 3.1611, Train Perplexity: 23.5962\n",
            "Epoch [1/3], Batch [10475/21340], Train Loss: 3.2338, Train Perplexity: 25.3748\n",
            "Epoch [1/3], Batch [10476/21340], Train Loss: 3.2134, Train Perplexity: 24.8624\n",
            "Epoch [1/3], Batch [10477/21340], Train Loss: 3.1523, Train Perplexity: 23.3908\n",
            "Epoch [1/3], Batch [10478/21340], Train Loss: 3.3537, Train Perplexity: 28.6077\n",
            "Epoch [1/3], Batch [10479/21340], Train Loss: 3.1918, Train Perplexity: 24.3323\n",
            "Epoch [1/3], Batch [10480/21340], Train Loss: 3.1936, Train Perplexity: 24.3766\n",
            "Epoch [1/3], Batch [10481/21340], Train Loss: 3.3585, Train Perplexity: 28.7460\n",
            "Epoch [1/3], Batch [10482/21340], Train Loss: 3.1760, Train Perplexity: 23.9519\n",
            "Epoch [1/3], Batch [10483/21340], Train Loss: 3.2091, Train Perplexity: 24.7560\n",
            "Epoch [1/3], Batch [10484/21340], Train Loss: 3.2363, Train Perplexity: 25.4390\n",
            "Epoch [1/3], Batch [10485/21340], Train Loss: 3.1806, Train Perplexity: 24.0606\n",
            "Epoch [1/3], Batch [10486/21340], Train Loss: 3.1796, Train Perplexity: 24.0367\n",
            "Epoch [1/3], Batch [10487/21340], Train Loss: 3.3206, Train Perplexity: 27.6774\n",
            "Epoch [1/3], Batch [10488/21340], Train Loss: 3.1921, Train Perplexity: 24.3404\n",
            "Epoch [1/3], Batch [10489/21340], Train Loss: 3.2254, Train Perplexity: 25.1636\n",
            "Epoch [1/3], Batch [10490/21340], Train Loss: 3.3000, Train Perplexity: 27.1119\n",
            "Epoch [1/3], Batch [10491/21340], Train Loss: 3.1548, Train Perplexity: 23.4485\n",
            "Epoch [1/3], Batch [10492/21340], Train Loss: 3.3022, Train Perplexity: 27.1724\n",
            "Epoch [1/3], Batch [10493/21340], Train Loss: 3.2579, Train Perplexity: 25.9944\n",
            "Epoch [1/3], Batch [10494/21340], Train Loss: 3.2725, Train Perplexity: 26.3781\n",
            "Epoch [1/3], Batch [10495/21340], Train Loss: 3.3310, Train Perplexity: 27.9655\n",
            "Epoch [1/3], Batch [10496/21340], Train Loss: 3.1730, Train Perplexity: 23.8791\n",
            "Epoch [1/3], Batch [10497/21340], Train Loss: 3.2236, Train Perplexity: 25.1195\n",
            "Epoch [1/3], Batch [10498/21340], Train Loss: 3.2081, Train Perplexity: 24.7310\n",
            "Epoch [1/3], Batch [10499/21340], Train Loss: 3.2202, Train Perplexity: 25.0322\n",
            "Epoch [1/3], Batch [10500/21340], Train Loss: 3.3817, Train Perplexity: 29.4210\n",
            "Epoch [1/3], Batch [10501/21340], Train Loss: 3.2550, Train Perplexity: 25.9188\n",
            "Epoch [1/3], Batch [10502/21340], Train Loss: 3.2586, Train Perplexity: 26.0128\n",
            "Epoch [1/3], Batch [10503/21340], Train Loss: 3.3953, Train Perplexity: 29.8244\n",
            "Epoch [1/3], Batch [10504/21340], Train Loss: 3.1898, Train Perplexity: 24.2838\n",
            "Epoch [1/3], Batch [10505/21340], Train Loss: 3.2800, Train Perplexity: 26.5759\n",
            "Epoch [1/3], Batch [10506/21340], Train Loss: 3.2534, Train Perplexity: 25.8791\n",
            "Epoch [1/3], Batch [10507/21340], Train Loss: 3.3162, Train Perplexity: 27.5566\n",
            "Epoch [1/3], Batch [10508/21340], Train Loss: 3.2384, Train Perplexity: 25.4932\n",
            "Epoch [1/3], Batch [10509/21340], Train Loss: 3.2301, Train Perplexity: 25.2823\n",
            "Epoch [1/3], Batch [10510/21340], Train Loss: 3.2264, Train Perplexity: 25.1888\n",
            "Epoch [1/3], Batch [10511/21340], Train Loss: 3.2012, Train Perplexity: 24.5611\n",
            "Epoch [1/3], Batch [10512/21340], Train Loss: 3.2258, Train Perplexity: 25.1737\n",
            "Epoch [1/3], Batch [10513/21340], Train Loss: 3.2877, Train Perplexity: 26.7802\n",
            "Epoch [1/3], Batch [10514/21340], Train Loss: 3.2296, Train Perplexity: 25.2699\n",
            "Epoch [1/3], Batch [10515/21340], Train Loss: 3.2239, Train Perplexity: 25.1252\n",
            "Epoch [1/3], Batch [10516/21340], Train Loss: 3.2179, Train Perplexity: 24.9763\n",
            "Epoch [1/3], Batch [10517/21340], Train Loss: 3.2166, Train Perplexity: 24.9428\n",
            "Epoch [1/3], Batch [10518/21340], Train Loss: 3.2386, Train Perplexity: 25.4975\n",
            "Epoch [1/3], Batch [10519/21340], Train Loss: 3.1375, Train Perplexity: 23.0465\n",
            "Epoch [1/3], Batch [10520/21340], Train Loss: 3.3609, Train Perplexity: 28.8159\n",
            "Epoch [1/3], Batch [10521/21340], Train Loss: 3.2309, Train Perplexity: 25.3020\n",
            "Epoch [1/3], Batch [10522/21340], Train Loss: 3.2122, Train Perplexity: 24.8345\n",
            "Epoch [1/3], Batch [10523/21340], Train Loss: 3.2908, Train Perplexity: 26.8652\n",
            "Epoch [1/3], Batch [10524/21340], Train Loss: 3.1978, Train Perplexity: 24.4785\n",
            "Epoch [1/3], Batch [10525/21340], Train Loss: 3.3259, Train Perplexity: 27.8243\n",
            "Epoch [1/3], Batch [10526/21340], Train Loss: 3.1768, Train Perplexity: 23.9697\n",
            "Epoch [1/3], Batch [10527/21340], Train Loss: 3.2447, Train Perplexity: 25.6536\n",
            "Epoch [1/3], Batch [10528/21340], Train Loss: 3.2529, Train Perplexity: 25.8655\n",
            "Epoch [1/3], Batch [10529/21340], Train Loss: 3.2590, Train Perplexity: 26.0235\n",
            "Epoch [1/3], Batch [10530/21340], Train Loss: 3.4290, Train Perplexity: 30.8465\n",
            "Epoch [1/3], Batch [10531/21340], Train Loss: 3.2240, Train Perplexity: 25.1279\n",
            "Epoch [1/3], Batch [10532/21340], Train Loss: 3.3212, Train Perplexity: 27.6948\n",
            "Epoch [1/3], Batch [10533/21340], Train Loss: 3.1746, Train Perplexity: 23.9174\n",
            "Epoch [1/3], Batch [10534/21340], Train Loss: 3.2770, Train Perplexity: 26.4970\n",
            "Epoch [1/3], Batch [10535/21340], Train Loss: 3.2194, Train Perplexity: 25.0127\n",
            "Epoch [1/3], Batch [10536/21340], Train Loss: 3.1700, Train Perplexity: 23.8082\n",
            "Epoch [1/3], Batch [10537/21340], Train Loss: 3.2371, Train Perplexity: 25.4595\n",
            "Epoch [1/3], Batch [10538/21340], Train Loss: 3.2515, Train Perplexity: 25.8279\n",
            "Epoch [1/3], Batch [10539/21340], Train Loss: 3.2093, Train Perplexity: 24.7614\n",
            "Epoch [1/3], Batch [10540/21340], Train Loss: 3.2535, Train Perplexity: 25.8820\n",
            "Epoch [1/3], Batch [10541/21340], Train Loss: 3.2321, Train Perplexity: 25.3337\n",
            "Epoch [1/3], Batch [10542/21340], Train Loss: 3.2427, Train Perplexity: 25.6025\n",
            "Epoch [1/3], Batch [10543/21340], Train Loss: 3.1713, Train Perplexity: 23.8377\n",
            "Epoch [1/3], Batch [10544/21340], Train Loss: 3.2422, Train Perplexity: 25.5909\n",
            "Epoch [1/3], Batch [10545/21340], Train Loss: 3.1814, Train Perplexity: 24.0805\n",
            "Epoch [1/3], Batch [10546/21340], Train Loss: 3.3177, Train Perplexity: 27.5960\n",
            "Epoch [1/3], Batch [10547/21340], Train Loss: 3.1835, Train Perplexity: 24.1320\n",
            "Epoch [1/3], Batch [10548/21340], Train Loss: 3.1822, Train Perplexity: 24.0997\n",
            "Epoch [1/3], Batch [10549/21340], Train Loss: 3.3198, Train Perplexity: 27.6559\n",
            "Epoch [1/3], Batch [10550/21340], Train Loss: 3.1908, Train Perplexity: 24.3077\n",
            "Epoch [1/3], Batch [10551/21340], Train Loss: 3.2732, Train Perplexity: 26.3960\n",
            "Epoch [1/3], Batch [10552/21340], Train Loss: 3.2858, Train Perplexity: 26.7291\n",
            "Epoch [1/3], Batch [10553/21340], Train Loss: 3.2900, Train Perplexity: 26.8420\n",
            "Epoch [1/3], Batch [10554/21340], Train Loss: 3.1957, Train Perplexity: 24.4278\n",
            "Epoch [1/3], Batch [10555/21340], Train Loss: 3.2776, Train Perplexity: 26.5122\n",
            "Epoch [1/3], Batch [10556/21340], Train Loss: 3.3553, Train Perplexity: 28.6555\n",
            "Epoch [1/3], Batch [10557/21340], Train Loss: 3.2308, Train Perplexity: 25.3002\n",
            "Epoch [1/3], Batch [10558/21340], Train Loss: 3.2686, Train Perplexity: 26.2745\n",
            "Epoch [1/3], Batch [10559/21340], Train Loss: 3.2650, Train Perplexity: 26.1800\n",
            "Epoch [1/3], Batch [10560/21340], Train Loss: 3.1882, Train Perplexity: 24.2455\n",
            "Epoch [1/3], Batch [10561/21340], Train Loss: 3.2766, Train Perplexity: 26.4849\n",
            "Epoch [1/3], Batch [10562/21340], Train Loss: 3.2286, Train Perplexity: 25.2442\n",
            "Epoch [1/3], Batch [10563/21340], Train Loss: 3.1963, Train Perplexity: 24.4422\n",
            "Epoch [1/3], Batch [10564/21340], Train Loss: 3.2033, Train Perplexity: 24.6125\n",
            "Epoch [1/3], Batch [10565/21340], Train Loss: 3.2056, Train Perplexity: 24.6699\n",
            "Epoch [1/3], Batch [10566/21340], Train Loss: 3.2439, Train Perplexity: 25.6324\n",
            "Epoch [1/3], Batch [10567/21340], Train Loss: 3.2452, Train Perplexity: 25.6660\n",
            "Epoch [1/3], Batch [10568/21340], Train Loss: 3.2499, Train Perplexity: 25.7878\n",
            "Epoch [1/3], Batch [10569/21340], Train Loss: 3.1804, Train Perplexity: 24.0562\n",
            "Epoch [1/3], Batch [10570/21340], Train Loss: 3.2621, Train Perplexity: 26.1054\n",
            "Epoch [1/3], Batch [10571/21340], Train Loss: 3.2348, Train Perplexity: 25.4015\n",
            "Epoch [1/3], Batch [10572/21340], Train Loss: 3.2655, Train Perplexity: 26.1939\n",
            "Epoch [1/3], Batch [10573/21340], Train Loss: 3.2164, Train Perplexity: 24.9384\n",
            "Epoch [1/3], Batch [10574/21340], Train Loss: 3.2070, Train Perplexity: 24.7060\n",
            "Epoch [1/3], Batch [10575/21340], Train Loss: 3.2679, Train Perplexity: 26.2572\n",
            "Epoch [1/3], Batch [10576/21340], Train Loss: 3.2545, Train Perplexity: 25.9077\n",
            "Epoch [1/3], Batch [10577/21340], Train Loss: 3.2113, Train Perplexity: 24.8123\n",
            "Epoch [1/3], Batch [10578/21340], Train Loss: 3.1350, Train Perplexity: 22.9895\n",
            "Epoch [1/3], Batch [10579/21340], Train Loss: 3.3048, Train Perplexity: 27.2443\n",
            "Epoch [1/3], Batch [10580/21340], Train Loss: 3.1585, Train Perplexity: 23.5362\n",
            "Epoch [1/3], Batch [10581/21340], Train Loss: 3.2422, Train Perplexity: 25.5899\n",
            "Epoch [1/3], Batch [10582/21340], Train Loss: 3.2313, Train Perplexity: 25.3122\n",
            "Epoch [1/3], Batch [10583/21340], Train Loss: 3.1809, Train Perplexity: 24.0693\n",
            "Epoch [1/3], Batch [10584/21340], Train Loss: 3.2333, Train Perplexity: 25.3629\n",
            "Epoch [1/3], Batch [10585/21340], Train Loss: 3.1268, Train Perplexity: 22.8009\n",
            "Epoch [1/3], Batch [10586/21340], Train Loss: 3.2552, Train Perplexity: 25.9252\n",
            "Epoch [1/3], Batch [10587/21340], Train Loss: 3.3750, Train Perplexity: 29.2231\n",
            "Epoch [1/3], Batch [10588/21340], Train Loss: 3.3006, Train Perplexity: 27.1296\n",
            "Epoch [1/3], Batch [10589/21340], Train Loss: 3.2617, Train Perplexity: 26.0929\n",
            "Epoch [1/3], Batch [10590/21340], Train Loss: 3.3834, Train Perplexity: 29.4714\n",
            "Epoch [1/3], Batch [10591/21340], Train Loss: 3.2032, Train Perplexity: 24.6123\n",
            "Epoch [1/3], Batch [10592/21340], Train Loss: 3.1120, Train Perplexity: 22.4665\n",
            "Epoch [1/3], Batch [10593/21340], Train Loss: 3.3113, Train Perplexity: 27.4214\n",
            "Epoch [1/3], Batch [10594/21340], Train Loss: 3.3259, Train Perplexity: 27.8241\n",
            "Epoch [1/3], Batch [10595/21340], Train Loss: 3.2470, Train Perplexity: 25.7137\n",
            "Epoch [1/3], Batch [10596/21340], Train Loss: 3.3862, Train Perplexity: 29.5520\n",
            "Epoch [1/3], Batch [10597/21340], Train Loss: 3.1818, Train Perplexity: 24.0900\n",
            "Epoch [1/3], Batch [10598/21340], Train Loss: 3.1331, Train Perplexity: 22.9452\n",
            "Epoch [1/3], Batch [10599/21340], Train Loss: 3.3619, Train Perplexity: 28.8450\n",
            "Epoch [1/3], Batch [10600/21340], Train Loss: 3.2363, Train Perplexity: 25.4384\n",
            "Epoch [1/3], Batch [10601/21340], Train Loss: 3.2553, Train Perplexity: 25.9271\n",
            "Epoch [1/3], Batch [10602/21340], Train Loss: 3.2198, Train Perplexity: 25.0240\n",
            "Epoch [1/3], Batch [10603/21340], Train Loss: 3.2051, Train Perplexity: 24.6574\n",
            "Epoch [1/3], Batch [10604/21340], Train Loss: 3.4245, Train Perplexity: 30.7078\n",
            "Epoch [1/3], Batch [10605/21340], Train Loss: 3.3376, Train Perplexity: 28.1529\n",
            "Epoch [1/3], Batch [10606/21340], Train Loss: 3.3305, Train Perplexity: 27.9523\n",
            "Epoch [1/3], Batch [10607/21340], Train Loss: 3.2863, Train Perplexity: 26.7430\n",
            "Epoch [1/3], Batch [10608/21340], Train Loss: 3.2833, Train Perplexity: 26.6631\n",
            "Epoch [1/3], Batch [10609/21340], Train Loss: 3.2361, Train Perplexity: 25.4348\n",
            "Epoch [1/3], Batch [10610/21340], Train Loss: 3.2636, Train Perplexity: 26.1422\n",
            "Epoch [1/3], Batch [10611/21340], Train Loss: 3.2933, Train Perplexity: 26.9305\n",
            "Epoch [1/3], Batch [10612/21340], Train Loss: 3.2352, Train Perplexity: 25.4112\n",
            "Epoch [1/3], Batch [10613/21340], Train Loss: 3.2903, Train Perplexity: 26.8503\n",
            "Epoch [1/3], Batch [10614/21340], Train Loss: 3.2744, Train Perplexity: 26.4264\n",
            "Epoch [1/3], Batch [10615/21340], Train Loss: 3.2174, Train Perplexity: 24.9619\n",
            "Epoch [1/3], Batch [10616/21340], Train Loss: 3.3016, Train Perplexity: 27.1573\n",
            "Epoch [1/3], Batch [10617/21340], Train Loss: 3.2907, Train Perplexity: 26.8609\n",
            "Epoch [1/3], Batch [10618/21340], Train Loss: 3.4483, Train Perplexity: 31.4457\n",
            "Epoch [1/3], Batch [10619/21340], Train Loss: 3.2290, Train Perplexity: 25.2556\n",
            "Epoch [1/3], Batch [10620/21340], Train Loss: 3.3195, Train Perplexity: 27.6459\n",
            "Epoch [1/3], Batch [10621/21340], Train Loss: 3.2241, Train Perplexity: 25.1302\n",
            "Epoch [1/3], Batch [10622/21340], Train Loss: 3.2234, Train Perplexity: 25.1135\n",
            "Epoch [1/3], Batch [10623/21340], Train Loss: 3.2313, Train Perplexity: 25.3115\n",
            "Epoch [1/3], Batch [10624/21340], Train Loss: 3.2829, Train Perplexity: 26.6527\n",
            "Epoch [1/3], Batch [10625/21340], Train Loss: 3.1911, Train Perplexity: 24.3158\n",
            "Epoch [1/3], Batch [10626/21340], Train Loss: 3.2923, Train Perplexity: 26.9059\n",
            "Epoch [1/3], Batch [10627/21340], Train Loss: 3.2659, Train Perplexity: 26.2034\n",
            "Epoch [1/3], Batch [10628/21340], Train Loss: 3.1807, Train Perplexity: 24.0636\n",
            "Epoch [1/3], Batch [10629/21340], Train Loss: 3.1765, Train Perplexity: 23.9624\n",
            "Epoch [1/3], Batch [10630/21340], Train Loss: 3.3401, Train Perplexity: 28.2227\n",
            "Epoch [1/3], Batch [10631/21340], Train Loss: 3.2812, Train Perplexity: 26.6072\n",
            "Epoch [1/3], Batch [10632/21340], Train Loss: 3.2651, Train Perplexity: 26.1823\n",
            "Epoch [1/3], Batch [10633/21340], Train Loss: 3.3342, Train Perplexity: 28.0549\n",
            "Epoch [1/3], Batch [10634/21340], Train Loss: 3.2287, Train Perplexity: 25.2479\n",
            "Epoch [1/3], Batch [10635/21340], Train Loss: 3.1843, Train Perplexity: 24.1496\n",
            "Epoch [1/3], Batch [10636/21340], Train Loss: 3.2775, Train Perplexity: 26.5105\n",
            "Epoch [1/3], Batch [10637/21340], Train Loss: 3.2948, Train Perplexity: 26.9728\n",
            "Epoch [1/3], Batch [10638/21340], Train Loss: 3.2311, Train Perplexity: 25.3082\n",
            "Epoch [1/3], Batch [10639/21340], Train Loss: 3.1789, Train Perplexity: 24.0200\n",
            "Epoch [1/3], Batch [10640/21340], Train Loss: 3.2108, Train Perplexity: 24.7986\n",
            "Epoch [1/3], Batch [10641/21340], Train Loss: 3.2306, Train Perplexity: 25.2946\n",
            "Epoch [1/3], Batch [10642/21340], Train Loss: 3.1106, Train Perplexity: 22.4340\n",
            "Epoch [1/3], Batch [10643/21340], Train Loss: 3.3901, Train Perplexity: 29.6695\n",
            "Epoch [1/3], Batch [10644/21340], Train Loss: 3.2252, Train Perplexity: 25.1582\n",
            "Epoch [1/3], Batch [10645/21340], Train Loss: 3.3026, Train Perplexity: 27.1845\n",
            "Epoch [1/3], Batch [10646/21340], Train Loss: 3.2558, Train Perplexity: 25.9416\n",
            "Epoch [1/3], Batch [10647/21340], Train Loss: 3.2648, Train Perplexity: 26.1751\n",
            "Epoch [1/3], Batch [10648/21340], Train Loss: 3.2119, Train Perplexity: 24.8252\n",
            "Epoch [1/3], Batch [10649/21340], Train Loss: 3.1602, Train Perplexity: 23.5756\n",
            "Epoch [1/3], Batch [10650/21340], Train Loss: 3.2350, Train Perplexity: 25.4069\n",
            "Epoch [1/3], Batch [10651/21340], Train Loss: 3.2552, Train Perplexity: 25.9255\n",
            "Epoch [1/3], Batch [10652/21340], Train Loss: 3.3244, Train Perplexity: 27.7827\n",
            "Epoch [1/3], Batch [10653/21340], Train Loss: 3.0989, Train Perplexity: 22.1733\n",
            "Epoch [1/3], Batch [10654/21340], Train Loss: 3.1968, Train Perplexity: 24.4539\n",
            "Epoch [1/3], Batch [10655/21340], Train Loss: 3.1902, Train Perplexity: 24.2932\n",
            "Epoch [1/3], Batch [10656/21340], Train Loss: 3.2599, Train Perplexity: 26.0465\n",
            "Epoch [1/3], Batch [10657/21340], Train Loss: 3.2972, Train Perplexity: 27.0355\n",
            "Epoch [1/3], Batch [10658/21340], Train Loss: 3.3679, Train Perplexity: 29.0188\n",
            "Epoch [1/3], Batch [10659/21340], Train Loss: 3.2122, Train Perplexity: 24.8347\n",
            "Epoch [1/3], Batch [10660/21340], Train Loss: 3.3140, Train Perplexity: 27.4940\n",
            "Epoch [1/3], Batch [10661/21340], Train Loss: 3.3273, Train Perplexity: 27.8637\n",
            "Epoch [1/3], Batch [10662/21340], Train Loss: 3.2377, Train Perplexity: 25.4739\n",
            "Epoch [1/3], Batch [10663/21340], Train Loss: 3.1983, Train Perplexity: 24.4908\n",
            "Epoch [1/3], Batch [10664/21340], Train Loss: 3.2248, Train Perplexity: 25.1482\n",
            "Epoch [1/3], Batch [10665/21340], Train Loss: 3.2295, Train Perplexity: 25.2661\n",
            "Epoch [1/3], Batch [10666/21340], Train Loss: 3.2086, Train Perplexity: 24.7453\n",
            "Epoch [1/3], Batch [10667/21340], Train Loss: 3.2085, Train Perplexity: 24.7420\n",
            "Epoch [1/3], Batch [10668/21340], Train Loss: 3.2927, Train Perplexity: 26.9160\n",
            "Epoch [1/3], Batch [10669/21340], Train Loss: 3.2506, Train Perplexity: 25.8061\n",
            "Epoch [1/3], Batch [10670/21340], Train Loss: 3.2582, Train Perplexity: 26.0035\n",
            "Epoch [1/3], Batch [10671/21340], Train Loss: 3.2566, Train Perplexity: 25.9600\n",
            "Epoch [1/3], Batch [10672/21340], Train Loss: 3.2682, Train Perplexity: 26.2644\n",
            "Epoch [1/3], Batch [10673/21340], Train Loss: 3.2600, Train Perplexity: 26.0498\n",
            "Epoch [1/3], Batch [10674/21340], Train Loss: 3.2574, Train Perplexity: 25.9826\n",
            "Epoch [1/3], Batch [10675/21340], Train Loss: 3.2492, Train Perplexity: 25.7706\n",
            "Epoch [1/3], Batch [10676/21340], Train Loss: 3.2964, Train Perplexity: 27.0151\n",
            "Epoch [1/3], Batch [10677/21340], Train Loss: 3.1561, Train Perplexity: 23.4779\n",
            "Epoch [1/3], Batch [10678/21340], Train Loss: 3.3045, Train Perplexity: 27.2337\n",
            "Epoch [1/3], Batch [10679/21340], Train Loss: 3.2551, Train Perplexity: 25.9221\n",
            "Epoch [1/3], Batch [10680/21340], Train Loss: 3.2040, Train Perplexity: 24.6298\n",
            "Epoch [1/3], Batch [10681/21340], Train Loss: 3.1713, Train Perplexity: 23.8376\n",
            "Epoch [1/3], Batch [10682/21340], Train Loss: 3.3511, Train Perplexity: 28.5334\n",
            "Epoch [1/3], Batch [10683/21340], Train Loss: 3.2312, Train Perplexity: 25.3099\n",
            "Epoch [1/3], Batch [10684/21340], Train Loss: 3.2831, Train Perplexity: 26.6590\n",
            "Epoch [1/3], Batch [10685/21340], Train Loss: 3.3791, Train Perplexity: 29.3455\n",
            "Epoch [1/3], Batch [10686/21340], Train Loss: 3.2741, Train Perplexity: 26.4191\n",
            "Epoch [1/3], Batch [10687/21340], Train Loss: 3.2841, Train Perplexity: 26.6843\n",
            "Epoch [1/3], Batch [10688/21340], Train Loss: 3.2695, Train Perplexity: 26.2991\n",
            "Epoch [1/3], Batch [10689/21340], Train Loss: 3.2917, Train Perplexity: 26.8884\n",
            "Epoch [1/3], Batch [10690/21340], Train Loss: 3.2566, Train Perplexity: 25.9615\n",
            "Epoch [1/3], Batch [10691/21340], Train Loss: 3.2583, Train Perplexity: 26.0055\n",
            "Epoch [1/3], Batch [10692/21340], Train Loss: 3.3127, Train Perplexity: 27.4588\n",
            "Epoch [1/3], Batch [10693/21340], Train Loss: 3.3175, Train Perplexity: 27.5922\n",
            "Epoch [1/3], Batch [10694/21340], Train Loss: 3.2404, Train Perplexity: 25.5429\n",
            "Epoch [1/3], Batch [10695/21340], Train Loss: 3.2444, Train Perplexity: 25.6464\n",
            "Epoch [1/3], Batch [10696/21340], Train Loss: 3.2257, Train Perplexity: 25.1710\n",
            "Epoch [1/3], Batch [10697/21340], Train Loss: 3.2266, Train Perplexity: 25.1936\n",
            "Epoch [1/3], Batch [10698/21340], Train Loss: 3.3676, Train Perplexity: 29.0088\n",
            "Epoch [1/3], Batch [10699/21340], Train Loss: 3.2378, Train Perplexity: 25.4774\n",
            "Epoch [1/3], Batch [10700/21340], Train Loss: 3.1954, Train Perplexity: 24.4187\n",
            "Epoch [1/3], Batch [10701/21340], Train Loss: 3.3088, Train Perplexity: 27.3519\n",
            "Epoch [1/3], Batch [10702/21340], Train Loss: 3.2126, Train Perplexity: 24.8440\n",
            "Epoch [1/3], Batch [10703/21340], Train Loss: 3.2580, Train Perplexity: 25.9974\n",
            "Epoch [1/3], Batch [10704/21340], Train Loss: 3.2549, Train Perplexity: 25.9174\n",
            "Epoch [1/3], Batch [10705/21340], Train Loss: 3.2483, Train Perplexity: 25.7470\n",
            "Epoch [1/3], Batch [10706/21340], Train Loss: 3.2521, Train Perplexity: 25.8450\n",
            "Epoch [1/3], Batch [10707/21340], Train Loss: 3.1881, Train Perplexity: 24.2414\n",
            "Epoch [1/3], Batch [10708/21340], Train Loss: 3.1760, Train Perplexity: 23.9513\n",
            "Epoch [1/3], Batch [10709/21340], Train Loss: 3.2889, Train Perplexity: 26.8140\n",
            "Epoch [1/3], Batch [10710/21340], Train Loss: 3.2514, Train Perplexity: 25.8260\n",
            "Epoch [1/3], Batch [10711/21340], Train Loss: 3.2156, Train Perplexity: 24.9188\n",
            "Epoch [1/3], Batch [10712/21340], Train Loss: 3.2012, Train Perplexity: 24.5616\n",
            "Epoch [1/3], Batch [10713/21340], Train Loss: 3.2287, Train Perplexity: 25.2462\n",
            "Epoch [1/3], Batch [10714/21340], Train Loss: 3.2083, Train Perplexity: 24.7371\n",
            "Epoch [1/3], Batch [10715/21340], Train Loss: 3.2203, Train Perplexity: 25.0349\n",
            "Epoch [1/3], Batch [10716/21340], Train Loss: 3.2646, Train Perplexity: 26.1696\n",
            "Epoch [1/3], Batch [10717/21340], Train Loss: 3.2797, Train Perplexity: 26.5666\n",
            "Epoch [1/3], Batch [10718/21340], Train Loss: 3.2407, Train Perplexity: 25.5526\n",
            "Epoch [1/3], Batch [10719/21340], Train Loss: 3.3444, Train Perplexity: 28.3438\n",
            "Epoch [1/3], Batch [10720/21340], Train Loss: 3.4918, Train Perplexity: 32.8452\n",
            "Epoch [1/3], Batch [10721/21340], Train Loss: 3.2587, Train Perplexity: 26.0146\n",
            "Epoch [1/3], Batch [10722/21340], Train Loss: 3.2565, Train Perplexity: 25.9579\n",
            "Epoch [1/3], Batch [10723/21340], Train Loss: 3.2540, Train Perplexity: 25.8948\n",
            "Epoch [1/3], Batch [10724/21340], Train Loss: 3.5616, Train Perplexity: 35.2207\n",
            "Epoch [1/3], Batch [10725/21340], Train Loss: 3.2811, Train Perplexity: 26.6048\n",
            "Epoch [1/3], Batch [10726/21340], Train Loss: 3.2685, Train Perplexity: 26.2719\n",
            "Epoch [1/3], Batch [10727/21340], Train Loss: 3.2526, Train Perplexity: 25.8588\n",
            "Epoch [1/3], Batch [10728/21340], Train Loss: 3.2538, Train Perplexity: 25.8880\n",
            "Epoch [1/3], Batch [10729/21340], Train Loss: 3.2463, Train Perplexity: 25.6954\n",
            "Epoch [1/3], Batch [10730/21340], Train Loss: 3.3781, Train Perplexity: 29.3165\n",
            "Epoch [1/3], Batch [10731/21340], Train Loss: 3.2132, Train Perplexity: 24.8593\n",
            "Epoch [1/3], Batch [10732/21340], Train Loss: 3.2442, Train Perplexity: 25.6415\n",
            "Epoch [1/3], Batch [10733/21340], Train Loss: 3.3373, Train Perplexity: 28.1431\n",
            "Epoch [1/3], Batch [10734/21340], Train Loss: 3.2356, Train Perplexity: 25.4218\n",
            "Epoch [1/3], Batch [10735/21340], Train Loss: 3.2990, Train Perplexity: 27.0852\n",
            "Epoch [1/3], Batch [10736/21340], Train Loss: 3.2393, Train Perplexity: 25.5159\n",
            "Epoch [1/3], Batch [10737/21340], Train Loss: 3.1408, Train Perplexity: 23.1214\n",
            "Epoch [1/3], Batch [10738/21340], Train Loss: 3.1391, Train Perplexity: 23.0830\n",
            "Epoch [1/3], Batch [10739/21340], Train Loss: 3.2873, Train Perplexity: 26.7692\n",
            "Epoch [1/3], Batch [10740/21340], Train Loss: 3.2502, Train Perplexity: 25.7963\n",
            "Epoch [1/3], Batch [10741/21340], Train Loss: 3.2394, Train Perplexity: 25.5174\n",
            "Epoch [1/3], Batch [10742/21340], Train Loss: 3.2699, Train Perplexity: 26.3088\n",
            "Epoch [1/3], Batch [10743/21340], Train Loss: 3.3224, Train Perplexity: 27.7268\n",
            "Epoch [1/3], Batch [10744/21340], Train Loss: 3.1571, Train Perplexity: 23.5013\n",
            "Epoch [1/3], Batch [10745/21340], Train Loss: 3.6047, Train Perplexity: 36.7691\n",
            "Epoch [1/3], Batch [10746/21340], Train Loss: 3.2115, Train Perplexity: 24.8168\n",
            "Epoch [1/3], Batch [10747/21340], Train Loss: 3.3530, Train Perplexity: 28.5898\n",
            "Epoch [1/3], Batch [10748/21340], Train Loss: 3.3094, Train Perplexity: 27.3699\n",
            "Epoch [1/3], Batch [10749/21340], Train Loss: 3.2689, Train Perplexity: 26.2834\n",
            "Epoch [1/3], Batch [10750/21340], Train Loss: 3.2641, Train Perplexity: 26.1575\n",
            "Epoch [1/3], Batch [10751/21340], Train Loss: 3.2434, Train Perplexity: 25.6219\n",
            "Epoch [1/3], Batch [10752/21340], Train Loss: 3.2723, Train Perplexity: 26.3717\n",
            "Epoch [1/3], Batch [10753/21340], Train Loss: 3.2836, Train Perplexity: 26.6726\n",
            "Epoch [1/3], Batch [10754/21340], Train Loss: 3.1427, Train Perplexity: 23.1652\n",
            "Epoch [1/3], Batch [10755/21340], Train Loss: 3.2303, Train Perplexity: 25.2871\n",
            "Epoch [1/3], Batch [10756/21340], Train Loss: 3.2246, Train Perplexity: 25.1445\n",
            "Epoch [1/3], Batch [10757/21340], Train Loss: 3.2873, Train Perplexity: 26.7701\n",
            "Epoch [1/3], Batch [10758/21340], Train Loss: 3.3651, Train Perplexity: 28.9374\n",
            "Epoch [1/3], Batch [10759/21340], Train Loss: 3.2196, Train Perplexity: 25.0187\n",
            "Epoch [1/3], Batch [10760/21340], Train Loss: 3.3175, Train Perplexity: 27.5917\n",
            "Epoch [1/3], Batch [10761/21340], Train Loss: 3.2302, Train Perplexity: 25.2855\n",
            "Epoch [1/3], Batch [10762/21340], Train Loss: 3.1590, Train Perplexity: 23.5476\n",
            "Epoch [1/3], Batch [10763/21340], Train Loss: 3.0884, Train Perplexity: 21.9426\n",
            "Epoch [1/3], Batch [10764/21340], Train Loss: 3.2082, Train Perplexity: 24.7339\n",
            "Epoch [1/3], Batch [10765/21340], Train Loss: 3.2202, Train Perplexity: 25.0329\n",
            "Epoch [1/3], Batch [10766/21340], Train Loss: 3.3761, Train Perplexity: 29.2573\n",
            "Epoch [1/3], Batch [10767/21340], Train Loss: 3.2188, Train Perplexity: 24.9993\n",
            "Epoch [1/3], Batch [10768/21340], Train Loss: 3.2061, Train Perplexity: 24.6817\n",
            "Epoch [1/3], Batch [10769/21340], Train Loss: 3.2253, Train Perplexity: 25.1600\n",
            "Epoch [1/3], Batch [10770/21340], Train Loss: 3.1831, Train Perplexity: 24.1209\n",
            "Epoch [1/3], Batch [10771/21340], Train Loss: 3.2204, Train Perplexity: 25.0390\n",
            "Epoch [1/3], Batch [10772/21340], Train Loss: 3.2027, Train Perplexity: 24.5987\n",
            "Epoch [1/3], Batch [10773/21340], Train Loss: 3.2400, Train Perplexity: 25.5334\n",
            "Epoch [1/3], Batch [10774/21340], Train Loss: 3.2957, Train Perplexity: 26.9956\n",
            "Epoch [1/3], Batch [10775/21340], Train Loss: 3.3018, Train Perplexity: 27.1624\n",
            "Epoch [1/3], Batch [10776/21340], Train Loss: 3.2973, Train Perplexity: 27.0402\n",
            "Epoch [1/3], Batch [10777/21340], Train Loss: 3.1905, Train Perplexity: 24.3002\n",
            "Epoch [1/3], Batch [10778/21340], Train Loss: 3.1627, Train Perplexity: 23.6335\n",
            "Epoch [1/3], Batch [10779/21340], Train Loss: 3.2666, Train Perplexity: 26.2222\n",
            "Epoch [1/3], Batch [10780/21340], Train Loss: 3.1613, Train Perplexity: 23.6010\n",
            "Epoch [1/3], Batch [10781/21340], Train Loss: 3.3242, Train Perplexity: 27.7755\n",
            "Epoch [1/3], Batch [10782/21340], Train Loss: 3.3856, Train Perplexity: 29.5350\n",
            "Epoch [1/3], Batch [10783/21340], Train Loss: 3.2962, Train Perplexity: 27.0099\n",
            "Epoch [1/3], Batch [10784/21340], Train Loss: 3.2883, Train Perplexity: 26.7981\n",
            "Epoch [1/3], Batch [10785/21340], Train Loss: 3.1900, Train Perplexity: 24.2872\n",
            "Epoch [1/3], Batch [10786/21340], Train Loss: 3.2172, Train Perplexity: 24.9582\n",
            "Epoch [1/3], Batch [10787/21340], Train Loss: 3.3205, Train Perplexity: 27.6728\n",
            "Epoch [1/3], Batch [10788/21340], Train Loss: 3.2547, Train Perplexity: 25.9126\n",
            "Epoch [1/3], Batch [10789/21340], Train Loss: 3.2397, Train Perplexity: 25.5270\n",
            "Epoch [1/3], Batch [10790/21340], Train Loss: 3.1702, Train Perplexity: 23.8116\n",
            "Epoch [1/3], Batch [10791/21340], Train Loss: 3.3294, Train Perplexity: 27.9218\n",
            "Epoch [1/3], Batch [10792/21340], Train Loss: 3.2649, Train Perplexity: 26.1784\n",
            "Epoch [1/3], Batch [10793/21340], Train Loss: 3.2321, Train Perplexity: 25.3326\n",
            "Epoch [1/3], Batch [10794/21340], Train Loss: 3.2194, Train Perplexity: 25.0134\n",
            "Epoch [1/3], Batch [10795/21340], Train Loss: 3.2486, Train Perplexity: 25.7535\n",
            "Epoch [1/3], Batch [10796/21340], Train Loss: 3.2679, Train Perplexity: 26.2561\n",
            "Epoch [1/3], Batch [10797/21340], Train Loss: 3.1711, Train Perplexity: 23.8344\n",
            "Epoch [1/3], Batch [10798/21340], Train Loss: 3.1835, Train Perplexity: 24.1315\n",
            "Epoch [1/3], Batch [10799/21340], Train Loss: 3.2810, Train Perplexity: 26.6019\n",
            "Epoch [1/3], Batch [10800/21340], Train Loss: 3.2342, Train Perplexity: 25.3872\n",
            "Epoch [1/3], Batch [10801/21340], Train Loss: 3.3412, Train Perplexity: 28.2532\n",
            "Epoch [1/3], Batch [10802/21340], Train Loss: 3.1362, Train Perplexity: 23.0151\n",
            "Epoch [1/3], Batch [10803/21340], Train Loss: 3.3517, Train Perplexity: 28.5518\n",
            "Epoch [1/3], Batch [10804/21340], Train Loss: 3.2038, Train Perplexity: 24.6269\n",
            "Epoch [1/3], Batch [10805/21340], Train Loss: 3.1593, Train Perplexity: 23.5543\n",
            "Epoch [1/3], Batch [10806/21340], Train Loss: 3.1380, Train Perplexity: 23.0573\n",
            "Epoch [1/3], Batch [10807/21340], Train Loss: 3.1761, Train Perplexity: 23.9523\n",
            "Epoch [1/3], Batch [10808/21340], Train Loss: 3.2423, Train Perplexity: 25.5938\n",
            "Epoch [1/3], Batch [10809/21340], Train Loss: 3.2485, Train Perplexity: 25.7519\n",
            "Epoch [1/3], Batch [10810/21340], Train Loss: 3.1967, Train Perplexity: 24.4520\n",
            "Epoch [1/3], Batch [10811/21340], Train Loss: 3.3231, Train Perplexity: 27.7475\n",
            "Epoch [1/3], Batch [10812/21340], Train Loss: 3.2874, Train Perplexity: 26.7727\n",
            "Epoch [1/3], Batch [10813/21340], Train Loss: 3.2340, Train Perplexity: 25.3816\n",
            "Epoch [1/3], Batch [10814/21340], Train Loss: 3.1788, Train Perplexity: 24.0168\n",
            "Epoch [1/3], Batch [10815/21340], Train Loss: 3.1981, Train Perplexity: 24.4864\n",
            "Epoch [1/3], Batch [10816/21340], Train Loss: 3.1980, Train Perplexity: 24.4824\n",
            "Epoch [1/3], Batch [10817/21340], Train Loss: 3.2439, Train Perplexity: 25.6329\n",
            "Epoch [1/3], Batch [10818/21340], Train Loss: 3.1934, Train Perplexity: 24.3711\n",
            "Epoch [1/3], Batch [10819/21340], Train Loss: 3.1675, Train Perplexity: 23.7472\n",
            "Epoch [1/3], Batch [10820/21340], Train Loss: 3.2729, Train Perplexity: 26.3873\n",
            "Epoch [1/3], Batch [10821/21340], Train Loss: 3.2418, Train Perplexity: 25.5799\n",
            "Epoch [1/3], Batch [10822/21340], Train Loss: 3.3011, Train Perplexity: 27.1413\n",
            "Epoch [1/3], Batch [10823/21340], Train Loss: 3.2384, Train Perplexity: 25.4927\n",
            "Epoch [1/3], Batch [10824/21340], Train Loss: 3.1977, Train Perplexity: 24.4773\n",
            "Epoch [1/3], Batch [10825/21340], Train Loss: 3.2803, Train Perplexity: 26.5844\n",
            "Epoch [1/3], Batch [10826/21340], Train Loss: 3.2289, Train Perplexity: 25.2525\n",
            "Epoch [1/3], Batch [10827/21340], Train Loss: 3.2968, Train Perplexity: 27.0262\n",
            "Epoch [1/3], Batch [10828/21340], Train Loss: 3.3789, Train Perplexity: 29.3385\n",
            "Epoch [1/3], Batch [10829/21340], Train Loss: 3.2839, Train Perplexity: 26.6809\n",
            "Epoch [1/3], Batch [10830/21340], Train Loss: 3.1703, Train Perplexity: 23.8139\n",
            "Epoch [1/3], Batch [10831/21340], Train Loss: 3.2299, Train Perplexity: 25.2769\n",
            "Epoch [1/3], Batch [10832/21340], Train Loss: 3.3236, Train Perplexity: 27.7603\n",
            "Epoch [1/3], Batch [10833/21340], Train Loss: 3.2154, Train Perplexity: 24.9129\n",
            "Epoch [1/3], Batch [10834/21340], Train Loss: 3.2191, Train Perplexity: 25.0054\n",
            "Epoch [1/3], Batch [10835/21340], Train Loss: 3.3363, Train Perplexity: 28.1147\n",
            "Epoch [1/3], Batch [10836/21340], Train Loss: 3.1422, Train Perplexity: 23.1542\n",
            "Epoch [1/3], Batch [10837/21340], Train Loss: 3.4647, Train Perplexity: 31.9665\n",
            "Epoch [1/3], Batch [10838/21340], Train Loss: 3.2100, Train Perplexity: 24.7798\n",
            "Epoch [1/3], Batch [10839/21340], Train Loss: 3.2830, Train Perplexity: 26.6559\n",
            "Epoch [1/3], Batch [10840/21340], Train Loss: 3.2565, Train Perplexity: 25.9581\n",
            "Epoch [1/3], Batch [10841/21340], Train Loss: 3.2487, Train Perplexity: 25.7568\n",
            "Epoch [1/3], Batch [10842/21340], Train Loss: 3.2710, Train Perplexity: 26.3380\n",
            "Epoch [1/3], Batch [10843/21340], Train Loss: 3.3154, Train Perplexity: 27.5346\n",
            "Epoch [1/3], Batch [10844/21340], Train Loss: 3.2053, Train Perplexity: 24.6618\n",
            "Epoch [1/3], Batch [10845/21340], Train Loss: 3.3127, Train Perplexity: 27.4605\n",
            "Epoch [1/3], Batch [10846/21340], Train Loss: 3.2105, Train Perplexity: 24.7915\n",
            "Epoch [1/3], Batch [10847/21340], Train Loss: 3.2265, Train Perplexity: 25.1910\n",
            "Epoch [1/3], Batch [10848/21340], Train Loss: 3.1915, Train Perplexity: 24.3252\n",
            "Epoch [1/3], Batch [10849/21340], Train Loss: 3.4037, Train Perplexity: 30.0765\n",
            "Epoch [1/3], Batch [10850/21340], Train Loss: 3.1571, Train Perplexity: 23.5031\n",
            "Epoch [1/3], Batch [10851/21340], Train Loss: 3.3365, Train Perplexity: 28.1212\n",
            "Epoch [1/3], Batch [10852/21340], Train Loss: 3.2444, Train Perplexity: 25.6461\n",
            "Epoch [1/3], Batch [10853/21340], Train Loss: 3.2281, Train Perplexity: 25.2307\n",
            "Epoch [1/3], Batch [10854/21340], Train Loss: 3.2728, Train Perplexity: 26.3846\n",
            "Epoch [1/3], Batch [10855/21340], Train Loss: 3.2962, Train Perplexity: 27.0103\n",
            "Epoch [1/3], Batch [10856/21340], Train Loss: 3.2042, Train Perplexity: 24.6361\n",
            "Epoch [1/3], Batch [10857/21340], Train Loss: 3.2725, Train Perplexity: 26.3769\n",
            "Epoch [1/3], Batch [10858/21340], Train Loss: 3.2310, Train Perplexity: 25.3049\n",
            "Epoch [1/3], Batch [10859/21340], Train Loss: 3.2827, Train Perplexity: 26.6471\n",
            "Epoch [1/3], Batch [10860/21340], Train Loss: 3.3590, Train Perplexity: 28.7601\n",
            "Epoch [1/3], Batch [10861/21340], Train Loss: 3.3242, Train Perplexity: 27.7767\n",
            "Epoch [1/3], Batch [10862/21340], Train Loss: 3.3349, Train Perplexity: 28.0748\n",
            "Epoch [1/3], Batch [10863/21340], Train Loss: 3.1878, Train Perplexity: 24.2348\n",
            "Epoch [1/3], Batch [10864/21340], Train Loss: 3.2104, Train Perplexity: 24.7892\n",
            "Epoch [1/3], Batch [10865/21340], Train Loss: 3.1675, Train Perplexity: 23.7480\n",
            "Epoch [1/3], Batch [10866/21340], Train Loss: 3.3220, Train Perplexity: 27.7147\n",
            "Epoch [1/3], Batch [10867/21340], Train Loss: 3.2305, Train Perplexity: 25.2911\n",
            "Epoch [1/3], Batch [10868/21340], Train Loss: 3.2110, Train Perplexity: 24.8042\n",
            "Epoch [1/3], Batch [10869/21340], Train Loss: 3.1955, Train Perplexity: 24.4215\n",
            "Epoch [1/3], Batch [10870/21340], Train Loss: 3.2920, Train Perplexity: 26.8956\n",
            "Epoch [1/3], Batch [10871/21340], Train Loss: 3.1917, Train Perplexity: 24.3299\n",
            "Epoch [1/3], Batch [10872/21340], Train Loss: 3.1931, Train Perplexity: 24.3643\n",
            "Epoch [1/3], Batch [10873/21340], Train Loss: 3.1398, Train Perplexity: 23.1004\n",
            "Epoch [1/3], Batch [10874/21340], Train Loss: 3.1555, Train Perplexity: 23.4653\n",
            "Epoch [1/3], Batch [10875/21340], Train Loss: 3.3596, Train Perplexity: 28.7788\n",
            "Epoch [1/3], Batch [10876/21340], Train Loss: 3.2645, Train Perplexity: 26.1662\n",
            "Epoch [1/3], Batch [10877/21340], Train Loss: 3.2401, Train Perplexity: 25.5372\n",
            "Epoch [1/3], Batch [10878/21340], Train Loss: 3.2760, Train Perplexity: 26.4686\n",
            "Epoch [1/3], Batch [10879/21340], Train Loss: 3.2197, Train Perplexity: 25.0204\n",
            "Epoch [1/3], Batch [10880/21340], Train Loss: 3.3207, Train Perplexity: 27.6784\n",
            "Epoch [1/3], Batch [10881/21340], Train Loss: 3.2976, Train Perplexity: 27.0471\n",
            "Epoch [1/3], Batch [10882/21340], Train Loss: 3.3578, Train Perplexity: 28.7248\n",
            "Epoch [1/3], Batch [10883/21340], Train Loss: 3.2673, Train Perplexity: 26.2396\n",
            "Epoch [1/3], Batch [10884/21340], Train Loss: 3.1905, Train Perplexity: 24.2995\n",
            "Epoch [1/3], Batch [10885/21340], Train Loss: 3.5366, Train Perplexity: 34.3487\n",
            "Epoch [1/3], Batch [10886/21340], Train Loss: 3.2763, Train Perplexity: 26.4774\n",
            "Epoch [1/3], Batch [10887/21340], Train Loss: 3.2144, Train Perplexity: 24.8894\n",
            "Epoch [1/3], Batch [10888/21340], Train Loss: 3.2224, Train Perplexity: 25.0889\n",
            "Epoch [1/3], Batch [10889/21340], Train Loss: 3.2010, Train Perplexity: 24.5562\n",
            "Epoch [1/3], Batch [10890/21340], Train Loss: 3.1842, Train Perplexity: 24.1483\n",
            "Epoch [1/3], Batch [10891/21340], Train Loss: 3.1510, Train Perplexity: 23.3595\n",
            "Epoch [1/3], Batch [10892/21340], Train Loss: 3.2770, Train Perplexity: 26.4960\n",
            "Epoch [1/3], Batch [10893/21340], Train Loss: 3.2968, Train Perplexity: 27.0248\n",
            "Epoch [1/3], Batch [10894/21340], Train Loss: 3.2165, Train Perplexity: 24.9401\n",
            "Epoch [1/3], Batch [10895/21340], Train Loss: 3.2620, Train Perplexity: 26.1019\n",
            "Epoch [1/3], Batch [10896/21340], Train Loss: 3.2258, Train Perplexity: 25.1728\n",
            "Epoch [1/3], Batch [10897/21340], Train Loss: 3.2459, Train Perplexity: 25.6857\n",
            "Epoch [1/3], Batch [10898/21340], Train Loss: 3.1698, Train Perplexity: 23.8023\n",
            "Epoch [1/3], Batch [10899/21340], Train Loss: 3.2004, Train Perplexity: 24.5420\n",
            "Epoch [1/3], Batch [10900/21340], Train Loss: 3.2758, Train Perplexity: 26.4632\n",
            "Epoch [1/3], Batch [10901/21340], Train Loss: 3.2880, Train Perplexity: 26.7896\n",
            "Epoch [1/3], Batch [10902/21340], Train Loss: 3.1667, Train Perplexity: 23.7300\n",
            "Epoch [1/3], Batch [10903/21340], Train Loss: 3.2018, Train Perplexity: 24.5755\n",
            "Epoch [1/3], Batch [10904/21340], Train Loss: 3.2192, Train Perplexity: 25.0091\n",
            "Epoch [1/3], Batch [10905/21340], Train Loss: 3.1567, Train Perplexity: 23.4924\n",
            "Epoch [1/3], Batch [10906/21340], Train Loss: 3.2595, Train Perplexity: 26.0370\n",
            "Epoch [1/3], Batch [10907/21340], Train Loss: 3.2731, Train Perplexity: 26.3925\n",
            "Epoch [1/3], Batch [10908/21340], Train Loss: 3.2407, Train Perplexity: 25.5524\n",
            "Epoch [1/3], Batch [10909/21340], Train Loss: 3.2150, Train Perplexity: 24.9034\n",
            "Epoch [1/3], Batch [10910/21340], Train Loss: 3.1892, Train Perplexity: 24.2683\n",
            "Epoch [1/3], Batch [10911/21340], Train Loss: 3.3630, Train Perplexity: 28.8759\n",
            "Epoch [1/3], Batch [10912/21340], Train Loss: 3.2536, Train Perplexity: 25.8828\n",
            "Epoch [1/3], Batch [10913/21340], Train Loss: 3.2497, Train Perplexity: 25.7816\n",
            "Epoch [1/3], Batch [10914/21340], Train Loss: 3.2576, Train Perplexity: 25.9883\n",
            "Epoch [1/3], Batch [10915/21340], Train Loss: 3.1586, Train Perplexity: 23.5369\n",
            "Epoch [1/3], Batch [10916/21340], Train Loss: 3.2542, Train Perplexity: 25.8988\n",
            "Epoch [1/3], Batch [10917/21340], Train Loss: 3.2768, Train Perplexity: 26.4911\n",
            "Epoch [1/3], Batch [10918/21340], Train Loss: 3.2436, Train Perplexity: 25.6266\n",
            "Epoch [1/3], Batch [10919/21340], Train Loss: 3.2086, Train Perplexity: 24.7437\n",
            "Epoch [1/3], Batch [10920/21340], Train Loss: 3.3002, Train Perplexity: 27.1171\n",
            "Epoch [1/3], Batch [10921/21340], Train Loss: 3.2736, Train Perplexity: 26.4052\n",
            "Epoch [1/3], Batch [10922/21340], Train Loss: 3.1732, Train Perplexity: 23.8846\n",
            "Epoch [1/3], Batch [10923/21340], Train Loss: 3.1810, Train Perplexity: 24.0710\n",
            "Epoch [1/3], Batch [10924/21340], Train Loss: 3.2486, Train Perplexity: 25.7555\n",
            "Epoch [1/3], Batch [10925/21340], Train Loss: 3.3580, Train Perplexity: 28.7323\n",
            "Epoch [1/3], Batch [10926/21340], Train Loss: 3.1961, Train Perplexity: 24.4366\n",
            "Epoch [1/3], Batch [10927/21340], Train Loss: 3.2613, Train Perplexity: 26.0845\n",
            "Epoch [1/3], Batch [10928/21340], Train Loss: 3.5147, Train Perplexity: 33.6065\n",
            "Epoch [1/3], Batch [10929/21340], Train Loss: 3.2077, Train Perplexity: 24.7225\n",
            "Epoch [1/3], Batch [10930/21340], Train Loss: 3.2378, Train Perplexity: 25.4781\n",
            "Epoch [1/3], Batch [10931/21340], Train Loss: 3.2251, Train Perplexity: 25.1561\n",
            "Epoch [1/3], Batch [10932/21340], Train Loss: 3.2282, Train Perplexity: 25.2348\n",
            "Epoch [1/3], Batch [10933/21340], Train Loss: 3.2260, Train Perplexity: 25.1781\n",
            "Epoch [1/3], Batch [10934/21340], Train Loss: 3.2187, Train Perplexity: 24.9959\n",
            "Epoch [1/3], Batch [10935/21340], Train Loss: 3.2539, Train Perplexity: 25.8906\n",
            "Epoch [1/3], Batch [10936/21340], Train Loss: 3.4156, Train Perplexity: 30.4350\n",
            "Epoch [1/3], Batch [10937/21340], Train Loss: 3.2058, Train Perplexity: 24.6741\n",
            "Epoch [1/3], Batch [10938/21340], Train Loss: 3.1764, Train Perplexity: 23.9611\n",
            "Epoch [1/3], Batch [10939/21340], Train Loss: 3.1698, Train Perplexity: 23.8027\n",
            "Epoch [1/3], Batch [10940/21340], Train Loss: 3.3784, Train Perplexity: 29.3244\n",
            "Epoch [1/3], Batch [10941/21340], Train Loss: 3.2096, Train Perplexity: 24.7686\n",
            "Epoch [1/3], Batch [10942/21340], Train Loss: 3.4992, Train Perplexity: 33.0885\n",
            "Epoch [1/3], Batch [10943/21340], Train Loss: 3.2090, Train Perplexity: 24.7534\n",
            "Epoch [1/3], Batch [10944/21340], Train Loss: 3.2569, Train Perplexity: 25.9682\n",
            "Epoch [1/3], Batch [10945/21340], Train Loss: 3.1887, Train Perplexity: 24.2559\n",
            "Epoch [1/3], Batch [10946/21340], Train Loss: 3.3044, Train Perplexity: 27.2318\n",
            "Epoch [1/3], Batch [10947/21340], Train Loss: 3.1786, Train Perplexity: 24.0138\n",
            "Epoch [1/3], Batch [10948/21340], Train Loss: 3.2465, Train Perplexity: 25.7012\n",
            "Epoch [1/3], Batch [10949/21340], Train Loss: 3.1837, Train Perplexity: 24.1347\n",
            "Epoch [1/3], Batch [10950/21340], Train Loss: 3.1997, Train Perplexity: 24.5240\n",
            "Epoch [1/3], Batch [10951/21340], Train Loss: 3.2151, Train Perplexity: 24.9068\n",
            "Epoch [1/3], Batch [10952/21340], Train Loss: 3.2838, Train Perplexity: 26.6769\n",
            "Epoch [1/3], Batch [10953/21340], Train Loss: 3.3069, Train Perplexity: 27.2998\n",
            "Epoch [1/3], Batch [10954/21340], Train Loss: 3.1529, Train Perplexity: 23.4038\n",
            "Epoch [1/3], Batch [10955/21340], Train Loss: 3.2546, Train Perplexity: 25.9089\n",
            "Epoch [1/3], Batch [10956/21340], Train Loss: 3.1650, Train Perplexity: 23.6889\n",
            "Epoch [1/3], Batch [10957/21340], Train Loss: 3.1988, Train Perplexity: 24.5020\n",
            "Epoch [1/3], Batch [10958/21340], Train Loss: 3.2245, Train Perplexity: 25.1400\n",
            "Epoch [1/3], Batch [10959/21340], Train Loss: 3.2214, Train Perplexity: 25.0627\n",
            "Epoch [1/3], Batch [10960/21340], Train Loss: 3.3085, Train Perplexity: 27.3429\n",
            "Epoch [1/3], Batch [10961/21340], Train Loss: 3.4678, Train Perplexity: 32.0669\n",
            "Epoch [1/3], Batch [10962/21340], Train Loss: 3.2089, Train Perplexity: 24.7511\n",
            "Epoch [1/3], Batch [10963/21340], Train Loss: 3.5318, Train Perplexity: 34.1842\n",
            "Epoch [1/3], Batch [10964/21340], Train Loss: 3.2554, Train Perplexity: 25.9299\n",
            "Epoch [1/3], Batch [10965/21340], Train Loss: 3.2448, Train Perplexity: 25.6562\n",
            "Epoch [1/3], Batch [10966/21340], Train Loss: 3.2170, Train Perplexity: 24.9535\n",
            "Epoch [1/3], Batch [10967/21340], Train Loss: 3.3002, Train Perplexity: 27.1172\n",
            "Epoch [1/3], Batch [10968/21340], Train Loss: 3.3441, Train Perplexity: 28.3341\n",
            "Epoch [1/3], Batch [10969/21340], Train Loss: 3.3594, Train Perplexity: 28.7726\n",
            "Epoch [1/3], Batch [10970/21340], Train Loss: 3.3059, Train Perplexity: 27.2723\n",
            "Epoch [1/3], Batch [10971/21340], Train Loss: 3.3014, Train Perplexity: 27.1505\n",
            "Epoch [1/3], Batch [10972/21340], Train Loss: 3.2563, Train Perplexity: 25.9531\n",
            "Epoch [1/3], Batch [10973/21340], Train Loss: 3.3522, Train Perplexity: 28.5658\n",
            "Epoch [1/3], Batch [10974/21340], Train Loss: 3.2729, Train Perplexity: 26.3891\n",
            "Epoch [1/3], Batch [10975/21340], Train Loss: 3.1907, Train Perplexity: 24.3063\n",
            "Epoch [1/3], Batch [10976/21340], Train Loss: 3.2135, Train Perplexity: 24.8669\n",
            "Epoch [1/3], Batch [10977/21340], Train Loss: 3.1518, Train Perplexity: 23.3773\n",
            "Epoch [1/3], Batch [10978/21340], Train Loss: 3.2201, Train Perplexity: 25.0296\n",
            "Epoch [1/3], Batch [10979/21340], Train Loss: 3.2152, Train Perplexity: 24.9083\n",
            "Epoch [1/3], Batch [10980/21340], Train Loss: 3.1843, Train Perplexity: 24.1496\n",
            "Epoch [1/3], Batch [10981/21340], Train Loss: 3.2640, Train Perplexity: 26.1546\n",
            "Epoch [1/3], Batch [10982/21340], Train Loss: 3.2259, Train Perplexity: 25.1766\n",
            "Epoch [1/3], Batch [10983/21340], Train Loss: 3.1705, Train Perplexity: 23.8195\n",
            "Epoch [1/3], Batch [10984/21340], Train Loss: 3.2599, Train Perplexity: 26.0461\n",
            "Epoch [1/3], Batch [10985/21340], Train Loss: 3.2980, Train Perplexity: 27.0587\n",
            "Epoch [1/3], Batch [10986/21340], Train Loss: 3.2064, Train Perplexity: 24.6908\n",
            "Epoch [1/3], Batch [10987/21340], Train Loss: 3.2138, Train Perplexity: 24.8731\n",
            "Epoch [1/3], Batch [10988/21340], Train Loss: 3.1947, Train Perplexity: 24.4025\n",
            "Epoch [1/3], Batch [10989/21340], Train Loss: 3.2268, Train Perplexity: 25.1977\n",
            "Epoch [1/3], Batch [10990/21340], Train Loss: 3.1113, Train Perplexity: 22.4510\n",
            "Epoch [1/3], Batch [10991/21340], Train Loss: 3.2472, Train Perplexity: 25.7184\n",
            "Epoch [1/3], Batch [10992/21340], Train Loss: 3.3272, Train Perplexity: 27.8591\n",
            "Epoch [1/3], Batch [10993/21340], Train Loss: 3.2744, Train Perplexity: 26.4282\n",
            "Epoch [1/3], Batch [10994/21340], Train Loss: 3.1815, Train Perplexity: 24.0820\n",
            "Epoch [1/3], Batch [10995/21340], Train Loss: 3.2457, Train Perplexity: 25.6798\n",
            "Epoch [1/3], Batch [10996/21340], Train Loss: 3.2558, Train Perplexity: 25.9411\n",
            "Epoch [1/3], Batch [10997/21340], Train Loss: 3.3072, Train Perplexity: 27.3095\n",
            "Epoch [1/3], Batch [10998/21340], Train Loss: 3.1968, Train Perplexity: 24.4546\n",
            "Epoch [1/3], Batch [10999/21340], Train Loss: 3.2081, Train Perplexity: 24.7317\n",
            "Epoch [1/3], Batch [11000/21340], Train Loss: 3.2648, Train Perplexity: 26.1752\n",
            "Epoch [1/3], Batch [11001/21340], Train Loss: 3.2186, Train Perplexity: 24.9939\n",
            "Epoch [1/3], Batch [11002/21340], Train Loss: 3.3354, Train Perplexity: 28.0901\n",
            "Epoch [1/3], Batch [11003/21340], Train Loss: 3.2796, Train Perplexity: 26.5641\n",
            "Epoch [1/3], Batch [11004/21340], Train Loss: 3.2973, Train Perplexity: 27.0385\n",
            "Epoch [1/3], Batch [11005/21340], Train Loss: 3.3623, Train Perplexity: 28.8565\n",
            "Epoch [1/3], Batch [11006/21340], Train Loss: 3.2504, Train Perplexity: 25.8005\n",
            "Epoch [1/3], Batch [11007/21340], Train Loss: 3.2778, Train Perplexity: 26.5171\n",
            "Epoch [1/3], Batch [11008/21340], Train Loss: 3.2557, Train Perplexity: 25.9384\n",
            "Epoch [1/3], Batch [11009/21340], Train Loss: 3.6437, Train Perplexity: 38.2329\n",
            "Epoch [1/3], Batch [11010/21340], Train Loss: 3.2160, Train Perplexity: 24.9282\n",
            "Epoch [1/3], Batch [11011/21340], Train Loss: 3.3002, Train Perplexity: 27.1175\n",
            "Epoch [1/3], Batch [11012/21340], Train Loss: 3.2729, Train Perplexity: 26.3889\n",
            "Epoch [1/3], Batch [11013/21340], Train Loss: 3.2473, Train Perplexity: 25.7213\n",
            "Epoch [1/3], Batch [11014/21340], Train Loss: 3.2314, Train Perplexity: 25.3163\n",
            "Epoch [1/3], Batch [11015/21340], Train Loss: 3.3167, Train Perplexity: 27.5691\n",
            "Epoch [1/3], Batch [11016/21340], Train Loss: 3.3716, Train Perplexity: 29.1260\n",
            "Epoch [1/3], Batch [11017/21340], Train Loss: 3.2790, Train Perplexity: 26.5492\n",
            "Epoch [1/3], Batch [11018/21340], Train Loss: 3.4027, Train Perplexity: 30.0457\n",
            "Epoch [1/3], Batch [11019/21340], Train Loss: 3.1909, Train Perplexity: 24.3099\n",
            "Epoch [1/3], Batch [11020/21340], Train Loss: 3.2462, Train Perplexity: 25.6923\n",
            "Epoch [1/3], Batch [11021/21340], Train Loss: 3.2967, Train Perplexity: 27.0240\n",
            "Epoch [1/3], Batch [11022/21340], Train Loss: 3.2330, Train Perplexity: 25.3547\n",
            "Epoch [1/3], Batch [11023/21340], Train Loss: 3.2720, Train Perplexity: 26.3641\n",
            "Epoch [1/3], Batch [11024/21340], Train Loss: 3.2327, Train Perplexity: 25.3475\n",
            "Epoch [1/3], Batch [11025/21340], Train Loss: 3.2307, Train Perplexity: 25.2984\n",
            "Epoch [1/3], Batch [11026/21340], Train Loss: 3.2130, Train Perplexity: 24.8531\n",
            "Epoch [1/3], Batch [11027/21340], Train Loss: 3.4572, Train Perplexity: 31.7277\n",
            "Epoch [1/3], Batch [11028/21340], Train Loss: 3.2239, Train Perplexity: 25.1248\n",
            "Epoch [1/3], Batch [11029/21340], Train Loss: 3.3227, Train Perplexity: 27.7345\n",
            "Epoch [1/3], Batch [11030/21340], Train Loss: 3.5237, Train Perplexity: 33.9097\n",
            "Epoch [1/3], Batch [11031/21340], Train Loss: 3.2327, Train Perplexity: 25.3479\n",
            "Epoch [1/3], Batch [11032/21340], Train Loss: 3.1517, Train Perplexity: 23.3765\n",
            "Epoch [1/3], Batch [11033/21340], Train Loss: 3.2791, Train Perplexity: 26.5525\n",
            "Epoch [1/3], Batch [11034/21340], Train Loss: 3.2327, Train Perplexity: 25.3472\n",
            "Epoch [1/3], Batch [11035/21340], Train Loss: 3.2535, Train Perplexity: 25.8800\n",
            "Epoch [1/3], Batch [11036/21340], Train Loss: 3.2519, Train Perplexity: 25.8387\n",
            "Epoch [1/3], Batch [11037/21340], Train Loss: 3.3195, Train Perplexity: 27.6478\n",
            "Epoch [1/3], Batch [11038/21340], Train Loss: 3.3074, Train Perplexity: 27.3139\n",
            "Epoch [1/3], Batch [11039/21340], Train Loss: 3.2903, Train Perplexity: 26.8519\n",
            "Epoch [1/3], Batch [11040/21340], Train Loss: 3.2011, Train Perplexity: 24.5598\n",
            "Epoch [1/3], Batch [11041/21340], Train Loss: 3.2194, Train Perplexity: 25.0143\n",
            "Epoch [1/3], Batch [11042/21340], Train Loss: 3.4426, Train Perplexity: 31.2694\n",
            "Epoch [1/3], Batch [11043/21340], Train Loss: 3.4280, Train Perplexity: 30.8153\n",
            "Epoch [1/3], Batch [11044/21340], Train Loss: 3.2719, Train Perplexity: 26.3614\n",
            "Epoch [1/3], Batch [11045/21340], Train Loss: 3.2452, Train Perplexity: 25.6678\n",
            "Epoch [1/3], Batch [11046/21340], Train Loss: 3.2785, Train Perplexity: 26.5356\n",
            "Epoch [1/3], Batch [11047/21340], Train Loss: 3.2037, Train Perplexity: 24.6227\n",
            "Epoch [1/3], Batch [11048/21340], Train Loss: 3.3345, Train Perplexity: 28.0643\n",
            "Epoch [1/3], Batch [11049/21340], Train Loss: 3.2116, Train Perplexity: 24.8198\n",
            "Epoch [1/3], Batch [11050/21340], Train Loss: 3.2232, Train Perplexity: 25.1086\n",
            "Epoch [1/3], Batch [11051/21340], Train Loss: 3.3432, Train Perplexity: 28.3100\n",
            "Epoch [1/3], Batch [11052/21340], Train Loss: 3.2776, Train Perplexity: 26.5128\n",
            "Epoch [1/3], Batch [11053/21340], Train Loss: 3.2054, Train Perplexity: 24.6662\n",
            "Epoch [1/3], Batch [11054/21340], Train Loss: 3.1548, Train Perplexity: 23.4476\n",
            "Epoch [1/3], Batch [11055/21340], Train Loss: 3.1838, Train Perplexity: 24.1392\n",
            "Epoch [1/3], Batch [11056/21340], Train Loss: 3.2115, Train Perplexity: 24.8175\n",
            "Epoch [1/3], Batch [11057/21340], Train Loss: 3.1982, Train Perplexity: 24.4877\n",
            "Epoch [1/3], Batch [11058/21340], Train Loss: 3.2022, Train Perplexity: 24.5859\n",
            "Epoch [1/3], Batch [11059/21340], Train Loss: 3.2913, Train Perplexity: 26.8774\n",
            "Epoch [1/3], Batch [11060/21340], Train Loss: 3.3020, Train Perplexity: 27.1676\n",
            "Epoch [1/3], Batch [11061/21340], Train Loss: 3.2228, Train Perplexity: 25.0973\n",
            "Epoch [1/3], Batch [11062/21340], Train Loss: 3.3290, Train Perplexity: 27.9096\n",
            "Epoch [1/3], Batch [11063/21340], Train Loss: 3.2249, Train Perplexity: 25.1508\n",
            "Epoch [1/3], Batch [11064/21340], Train Loss: 3.1780, Train Perplexity: 23.9990\n",
            "Epoch [1/3], Batch [11065/21340], Train Loss: 3.3018, Train Perplexity: 27.1613\n",
            "Epoch [1/3], Batch [11066/21340], Train Loss: 3.3270, Train Perplexity: 27.8554\n",
            "Epoch [1/3], Batch [11067/21340], Train Loss: 3.2154, Train Perplexity: 24.9134\n",
            "Epoch [1/3], Batch [11068/21340], Train Loss: 3.2542, Train Perplexity: 25.8998\n",
            "Epoch [1/3], Batch [11069/21340], Train Loss: 3.1573, Train Perplexity: 23.5063\n",
            "Epoch [1/3], Batch [11070/21340], Train Loss: 3.2868, Train Perplexity: 26.7569\n",
            "Epoch [1/3], Batch [11071/21340], Train Loss: 3.1872, Train Perplexity: 24.2205\n",
            "Epoch [1/3], Batch [11072/21340], Train Loss: 3.3910, Train Perplexity: 29.6963\n",
            "Epoch [1/3], Batch [11073/21340], Train Loss: 3.1747, Train Perplexity: 23.9186\n",
            "Epoch [1/3], Batch [11074/21340], Train Loss: 3.2172, Train Perplexity: 24.9586\n",
            "Epoch [1/3], Batch [11075/21340], Train Loss: 3.2219, Train Perplexity: 25.0767\n",
            "Epoch [1/3], Batch [11076/21340], Train Loss: 3.2943, Train Perplexity: 26.9586\n",
            "Epoch [1/3], Batch [11077/21340], Train Loss: 3.2868, Train Perplexity: 26.7560\n",
            "Epoch [1/3], Batch [11078/21340], Train Loss: 3.1693, Train Perplexity: 23.7918\n",
            "Epoch [1/3], Batch [11079/21340], Train Loss: 3.1704, Train Perplexity: 23.8167\n",
            "Epoch [1/3], Batch [11080/21340], Train Loss: 3.2455, Train Perplexity: 25.6739\n",
            "Epoch [1/3], Batch [11081/21340], Train Loss: 3.2124, Train Perplexity: 24.8380\n",
            "Epoch [1/3], Batch [11082/21340], Train Loss: 3.2464, Train Perplexity: 25.6988\n",
            "Epoch [1/3], Batch [11083/21340], Train Loss: 3.2603, Train Perplexity: 26.0576\n",
            "Epoch [1/3], Batch [11084/21340], Train Loss: 3.2731, Train Perplexity: 26.3942\n",
            "Epoch [1/3], Batch [11085/21340], Train Loss: 3.2769, Train Perplexity: 26.4926\n",
            "Epoch [1/3], Batch [11086/21340], Train Loss: 3.1370, Train Perplexity: 23.0343\n",
            "Epoch [1/3], Batch [11087/21340], Train Loss: 3.1955, Train Perplexity: 24.4234\n",
            "Epoch [1/3], Batch [11088/21340], Train Loss: 3.3353, Train Perplexity: 28.0857\n",
            "Epoch [1/3], Batch [11089/21340], Train Loss: 3.2708, Train Perplexity: 26.3321\n",
            "Epoch [1/3], Batch [11090/21340], Train Loss: 3.2610, Train Perplexity: 26.0764\n",
            "Epoch [1/3], Batch [11091/21340], Train Loss: 3.4578, Train Perplexity: 31.7476\n",
            "Epoch [1/3], Batch [11092/21340], Train Loss: 3.2692, Train Perplexity: 26.2906\n",
            "Epoch [1/3], Batch [11093/21340], Train Loss: 3.2552, Train Perplexity: 25.9259\n",
            "Epoch [1/3], Batch [11094/21340], Train Loss: 3.2196, Train Perplexity: 25.0184\n",
            "Epoch [1/3], Batch [11095/21340], Train Loss: 3.3019, Train Perplexity: 27.1636\n",
            "Epoch [1/3], Batch [11096/21340], Train Loss: 3.2364, Train Perplexity: 25.4418\n",
            "Epoch [1/3], Batch [11097/21340], Train Loss: 3.1819, Train Perplexity: 24.0931\n",
            "Epoch [1/3], Batch [11098/21340], Train Loss: 3.4028, Train Perplexity: 30.0496\n",
            "Epoch [1/3], Batch [11099/21340], Train Loss: 3.2140, Train Perplexity: 24.8772\n",
            "Epoch [1/3], Batch [11100/21340], Train Loss: 3.2254, Train Perplexity: 25.1634\n",
            "Epoch [1/3], Batch [11101/21340], Train Loss: 3.3114, Train Perplexity: 27.4245\n",
            "Epoch [1/3], Batch [11102/21340], Train Loss: 3.2305, Train Perplexity: 25.2919\n",
            "Epoch [1/3], Batch [11103/21340], Train Loss: 3.2367, Train Perplexity: 25.4489\n",
            "Epoch [1/3], Batch [11104/21340], Train Loss: 3.2506, Train Perplexity: 25.8070\n",
            "Epoch [1/3], Batch [11105/21340], Train Loss: 3.2614, Train Perplexity: 26.0870\n",
            "Epoch [1/3], Batch [11106/21340], Train Loss: 3.2752, Train Perplexity: 26.4498\n",
            "Epoch [1/3], Batch [11107/21340], Train Loss: 3.1947, Train Perplexity: 24.4017\n",
            "Epoch [1/3], Batch [11108/21340], Train Loss: 3.2444, Train Perplexity: 25.6467\n",
            "Epoch [1/3], Batch [11109/21340], Train Loss: 3.2994, Train Perplexity: 27.0954\n",
            "Epoch [1/3], Batch [11110/21340], Train Loss: 3.2599, Train Perplexity: 26.0466\n",
            "Epoch [1/3], Batch [11111/21340], Train Loss: 3.1964, Train Perplexity: 24.4432\n",
            "Epoch [1/3], Batch [11112/21340], Train Loss: 3.4723, Train Perplexity: 32.2095\n",
            "Epoch [1/3], Batch [11113/21340], Train Loss: 3.1842, Train Perplexity: 24.1475\n",
            "Epoch [1/3], Batch [11114/21340], Train Loss: 3.1646, Train Perplexity: 23.6788\n",
            "Epoch [1/3], Batch [11115/21340], Train Loss: 3.2707, Train Perplexity: 26.3286\n",
            "Epoch [1/3], Batch [11116/21340], Train Loss: 3.1895, Train Perplexity: 24.2759\n",
            "Epoch [1/3], Batch [11117/21340], Train Loss: 3.3113, Train Perplexity: 27.4213\n",
            "Epoch [1/3], Batch [11118/21340], Train Loss: 3.2388, Train Perplexity: 25.5028\n",
            "Epoch [1/3], Batch [11119/21340], Train Loss: 3.2193, Train Perplexity: 25.0099\n",
            "Epoch [1/3], Batch [11120/21340], Train Loss: 3.1996, Train Perplexity: 24.5232\n",
            "Epoch [1/3], Batch [11121/21340], Train Loss: 3.1801, Train Perplexity: 24.0503\n",
            "Epoch [1/3], Batch [11122/21340], Train Loss: 3.2596, Train Perplexity: 26.0385\n",
            "Epoch [1/3], Batch [11123/21340], Train Loss: 3.2218, Train Perplexity: 25.0730\n",
            "Epoch [1/3], Batch [11124/21340], Train Loss: 3.1598, Train Perplexity: 23.5662\n",
            "Epoch [1/3], Batch [11125/21340], Train Loss: 3.2788, Train Perplexity: 26.5448\n",
            "Epoch [1/3], Batch [11126/21340], Train Loss: 3.1719, Train Perplexity: 23.8518\n",
            "Epoch [1/3], Batch [11127/21340], Train Loss: 3.2765, Train Perplexity: 26.4816\n",
            "Epoch [1/3], Batch [11128/21340], Train Loss: 3.2351, Train Perplexity: 25.4099\n",
            "Epoch [1/3], Batch [11129/21340], Train Loss: 3.2333, Train Perplexity: 25.3630\n",
            "Epoch [1/3], Batch [11130/21340], Train Loss: 3.1479, Train Perplexity: 23.2880\n",
            "Epoch [1/3], Batch [11131/21340], Train Loss: 3.2260, Train Perplexity: 25.1782\n",
            "Epoch [1/3], Batch [11132/21340], Train Loss: 3.2386, Train Perplexity: 25.4979\n",
            "Epoch [1/3], Batch [11133/21340], Train Loss: 3.2696, Train Perplexity: 26.3005\n",
            "Epoch [1/3], Batch [11134/21340], Train Loss: 3.2493, Train Perplexity: 25.7733\n",
            "Epoch [1/3], Batch [11135/21340], Train Loss: 3.1747, Train Perplexity: 23.9204\n",
            "Epoch [1/3], Batch [11136/21340], Train Loss: 3.4433, Train Perplexity: 31.2908\n",
            "Epoch [1/3], Batch [11137/21340], Train Loss: 3.1783, Train Perplexity: 24.0059\n",
            "Epoch [1/3], Batch [11138/21340], Train Loss: 3.2145, Train Perplexity: 24.8914\n",
            "Epoch [1/3], Batch [11139/21340], Train Loss: 3.2182, Train Perplexity: 24.9836\n",
            "Epoch [1/3], Batch [11140/21340], Train Loss: 3.2882, Train Perplexity: 26.7951\n",
            "Epoch [1/3], Batch [11141/21340], Train Loss: 3.2896, Train Perplexity: 26.8320\n",
            "Epoch [1/3], Batch [11142/21340], Train Loss: 3.1982, Train Perplexity: 24.4875\n",
            "Epoch [1/3], Batch [11143/21340], Train Loss: 3.2326, Train Perplexity: 25.3454\n",
            "Epoch [1/3], Batch [11144/21340], Train Loss: 3.2341, Train Perplexity: 25.3832\n",
            "Epoch [1/3], Batch [11145/21340], Train Loss: 3.1922, Train Perplexity: 24.3408\n",
            "Epoch [1/3], Batch [11146/21340], Train Loss: 3.1669, Train Perplexity: 23.7342\n",
            "Epoch [1/3], Batch [11147/21340], Train Loss: 3.2246, Train Perplexity: 25.1423\n",
            "Epoch [1/3], Batch [11148/21340], Train Loss: 3.1407, Train Perplexity: 23.1201\n",
            "Epoch [1/3], Batch [11149/21340], Train Loss: 3.1541, Train Perplexity: 23.4316\n",
            "Epoch [1/3], Batch [11150/21340], Train Loss: 3.1304, Train Perplexity: 22.8823\n",
            "Epoch [1/3], Batch [11151/21340], Train Loss: 3.4814, Train Perplexity: 32.5060\n",
            "Epoch [1/3], Batch [11152/21340], Train Loss: 3.2486, Train Perplexity: 25.7541\n",
            "Epoch [1/3], Batch [11153/21340], Train Loss: 3.3615, Train Perplexity: 28.8328\n",
            "Epoch [1/3], Batch [11154/21340], Train Loss: 3.2589, Train Perplexity: 26.0217\n",
            "Epoch [1/3], Batch [11155/21340], Train Loss: 3.2639, Train Perplexity: 26.1521\n",
            "Epoch [1/3], Batch [11156/21340], Train Loss: 3.2143, Train Perplexity: 24.8870\n",
            "Epoch [1/3], Batch [11157/21340], Train Loss: 3.3036, Train Perplexity: 27.2095\n",
            "Epoch [1/3], Batch [11158/21340], Train Loss: 3.3071, Train Perplexity: 27.3065\n",
            "Epoch [1/3], Batch [11159/21340], Train Loss: 3.1639, Train Perplexity: 23.6624\n",
            "Epoch [1/3], Batch [11160/21340], Train Loss: 3.2744, Train Perplexity: 26.4263\n",
            "Epoch [1/3], Batch [11161/21340], Train Loss: 3.1724, Train Perplexity: 23.8650\n",
            "Epoch [1/3], Batch [11162/21340], Train Loss: 3.2834, Train Perplexity: 26.6668\n",
            "Epoch [1/3], Batch [11163/21340], Train Loss: 3.2147, Train Perplexity: 24.8964\n",
            "Epoch [1/3], Batch [11164/21340], Train Loss: 3.3619, Train Perplexity: 28.8446\n",
            "Epoch [1/3], Batch [11165/21340], Train Loss: 3.1948, Train Perplexity: 24.4042\n",
            "Epoch [1/3], Batch [11166/21340], Train Loss: 3.2175, Train Perplexity: 24.9646\n",
            "Epoch [1/3], Batch [11167/21340], Train Loss: 3.1773, Train Perplexity: 23.9823\n",
            "Epoch [1/3], Batch [11168/21340], Train Loss: 3.2501, Train Perplexity: 25.7942\n",
            "Epoch [1/3], Batch [11169/21340], Train Loss: 3.2020, Train Perplexity: 24.5824\n",
            "Epoch [1/3], Batch [11170/21340], Train Loss: 3.2647, Train Perplexity: 26.1726\n",
            "Epoch [1/3], Batch [11171/21340], Train Loss: 3.2838, Train Perplexity: 26.6760\n",
            "Epoch [1/3], Batch [11172/21340], Train Loss: 3.1884, Train Perplexity: 24.2506\n",
            "Epoch [1/3], Batch [11173/21340], Train Loss: 3.2123, Train Perplexity: 24.8353\n",
            "Epoch [1/3], Batch [11174/21340], Train Loss: 3.1824, Train Perplexity: 24.1044\n",
            "Epoch [1/3], Batch [11175/21340], Train Loss: 3.2487, Train Perplexity: 25.7568\n",
            "Epoch [1/3], Batch [11176/21340], Train Loss: 3.2194, Train Perplexity: 25.0139\n",
            "Epoch [1/3], Batch [11177/21340], Train Loss: 3.2168, Train Perplexity: 24.9482\n",
            "Epoch [1/3], Batch [11178/21340], Train Loss: 3.2772, Train Perplexity: 26.5011\n",
            "Epoch [1/3], Batch [11179/21340], Train Loss: 3.2253, Train Perplexity: 25.1613\n",
            "Epoch [1/3], Batch [11180/21340], Train Loss: 3.1635, Train Perplexity: 23.6540\n",
            "Epoch [1/3], Batch [11181/21340], Train Loss: 3.2999, Train Perplexity: 27.1104\n",
            "Epoch [1/3], Batch [11182/21340], Train Loss: 3.2700, Train Perplexity: 26.3124\n",
            "Epoch [1/3], Batch [11183/21340], Train Loss: 3.2018, Train Perplexity: 24.5776\n",
            "Epoch [1/3], Batch [11184/21340], Train Loss: 3.2579, Train Perplexity: 25.9950\n",
            "Epoch [1/3], Batch [11185/21340], Train Loss: 3.2240, Train Perplexity: 25.1286\n",
            "Epoch [1/3], Batch [11186/21340], Train Loss: 3.1497, Train Perplexity: 23.3282\n",
            "Epoch [1/3], Batch [11187/21340], Train Loss: 3.1774, Train Perplexity: 23.9832\n",
            "Epoch [1/3], Batch [11188/21340], Train Loss: 3.2315, Train Perplexity: 25.3183\n",
            "Epoch [1/3], Batch [11189/21340], Train Loss: 3.1386, Train Perplexity: 23.0705\n",
            "Epoch [1/3], Batch [11190/21340], Train Loss: 3.2770, Train Perplexity: 26.4950\n",
            "Epoch [1/3], Batch [11191/21340], Train Loss: 3.3248, Train Perplexity: 27.7939\n",
            "Epoch [1/3], Batch [11192/21340], Train Loss: 3.2277, Train Perplexity: 25.2220\n",
            "Epoch [1/3], Batch [11193/21340], Train Loss: 3.2142, Train Perplexity: 24.8841\n",
            "Epoch [1/3], Batch [11194/21340], Train Loss: 3.2358, Train Perplexity: 25.4261\n",
            "Epoch [1/3], Batch [11195/21340], Train Loss: 3.2698, Train Perplexity: 26.3066\n",
            "Epoch [1/3], Batch [11196/21340], Train Loss: 3.3114, Train Perplexity: 27.4242\n",
            "Epoch [1/3], Batch [11197/21340], Train Loss: 3.2587, Train Perplexity: 26.0167\n",
            "Epoch [1/3], Batch [11198/21340], Train Loss: 3.3123, Train Perplexity: 27.4490\n",
            "Epoch [1/3], Batch [11199/21340], Train Loss: 3.3608, Train Perplexity: 28.8130\n",
            "Epoch [1/3], Batch [11200/21340], Train Loss: 3.3287, Train Perplexity: 27.9019\n",
            "Epoch [1/3], Batch [11201/21340], Train Loss: 3.2160, Train Perplexity: 24.9272\n",
            "Epoch [1/3], Batch [11202/21340], Train Loss: 3.2410, Train Perplexity: 25.5593\n",
            "Epoch [1/3], Batch [11203/21340], Train Loss: 3.3011, Train Perplexity: 27.1416\n",
            "Epoch [1/3], Batch [11204/21340], Train Loss: 3.2845, Train Perplexity: 26.6953\n",
            "Epoch [1/3], Batch [11205/21340], Train Loss: 3.2428, Train Perplexity: 25.6050\n",
            "Epoch [1/3], Batch [11206/21340], Train Loss: 3.3366, Train Perplexity: 28.1224\n",
            "Epoch [1/3], Batch [11207/21340], Train Loss: 3.2326, Train Perplexity: 25.3460\n",
            "Epoch [1/3], Batch [11208/21340], Train Loss: 3.3183, Train Perplexity: 27.6123\n",
            "Epoch [1/3], Batch [11209/21340], Train Loss: 3.2663, Train Perplexity: 26.2142\n",
            "Epoch [1/3], Batch [11210/21340], Train Loss: 3.3201, Train Perplexity: 27.6641\n",
            "Epoch [1/3], Batch [11211/21340], Train Loss: 3.1814, Train Perplexity: 24.0816\n",
            "Epoch [1/3], Batch [11212/21340], Train Loss: 3.2937, Train Perplexity: 26.9435\n",
            "Epoch [1/3], Batch [11213/21340], Train Loss: 3.2528, Train Perplexity: 25.8632\n",
            "Epoch [1/3], Batch [11214/21340], Train Loss: 3.2557, Train Perplexity: 25.9376\n",
            "Epoch [1/3], Batch [11215/21340], Train Loss: 3.2216, Train Perplexity: 25.0679\n",
            "Epoch [1/3], Batch [11216/21340], Train Loss: 3.2137, Train Perplexity: 24.8709\n",
            "Epoch [1/3], Batch [11217/21340], Train Loss: 3.3195, Train Perplexity: 27.6472\n",
            "Epoch [1/3], Batch [11218/21340], Train Loss: 3.3335, Train Perplexity: 28.0364\n",
            "Epoch [1/3], Batch [11219/21340], Train Loss: 3.1459, Train Perplexity: 23.2395\n",
            "Epoch [1/3], Batch [11220/21340], Train Loss: 3.2364, Train Perplexity: 25.4427\n",
            "Epoch [1/3], Batch [11221/21340], Train Loss: 3.1654, Train Perplexity: 23.6989\n",
            "Epoch [1/3], Batch [11222/21340], Train Loss: 3.3050, Train Perplexity: 27.2498\n",
            "Epoch [1/3], Batch [11223/21340], Train Loss: 3.2471, Train Perplexity: 25.7154\n",
            "Epoch [1/3], Batch [11224/21340], Train Loss: 3.1845, Train Perplexity: 24.1556\n",
            "Epoch [1/3], Batch [11225/21340], Train Loss: 3.3614, Train Perplexity: 28.8301\n",
            "Epoch [1/3], Batch [11226/21340], Train Loss: 3.2892, Train Perplexity: 26.8225\n",
            "Epoch [1/3], Batch [11227/21340], Train Loss: 3.2693, Train Perplexity: 26.2930\n",
            "Epoch [1/3], Batch [11228/21340], Train Loss: 3.2313, Train Perplexity: 25.3133\n",
            "Epoch [1/3], Batch [11229/21340], Train Loss: 3.2685, Train Perplexity: 26.2727\n",
            "Epoch [1/3], Batch [11230/21340], Train Loss: 3.3043, Train Perplexity: 27.2294\n",
            "Epoch [1/3], Batch [11231/21340], Train Loss: 3.2165, Train Perplexity: 24.9409\n",
            "Epoch [1/3], Batch [11232/21340], Train Loss: 3.2873, Train Perplexity: 26.7701\n",
            "Epoch [1/3], Batch [11233/21340], Train Loss: 3.2808, Train Perplexity: 26.5977\n",
            "Epoch [1/3], Batch [11234/21340], Train Loss: 3.2897, Train Perplexity: 26.8344\n",
            "Epoch [1/3], Batch [11235/21340], Train Loss: 3.2060, Train Perplexity: 24.6795\n",
            "Epoch [1/3], Batch [11236/21340], Train Loss: 3.3447, Train Perplexity: 28.3517\n",
            "Epoch [1/3], Batch [11237/21340], Train Loss: 3.1557, Train Perplexity: 23.4690\n",
            "Epoch [1/3], Batch [11238/21340], Train Loss: 3.2369, Train Perplexity: 25.4556\n",
            "Epoch [1/3], Batch [11239/21340], Train Loss: 3.1639, Train Perplexity: 23.6627\n",
            "Epoch [1/3], Batch [11240/21340], Train Loss: 3.3477, Train Perplexity: 28.4360\n",
            "Epoch [1/3], Batch [11241/21340], Train Loss: 3.2885, Train Perplexity: 26.8026\n",
            "Epoch [1/3], Batch [11242/21340], Train Loss: 3.1845, Train Perplexity: 24.1545\n",
            "Epoch [1/3], Batch [11243/21340], Train Loss: 3.3559, Train Perplexity: 28.6727\n",
            "Epoch [1/3], Batch [11244/21340], Train Loss: 3.1246, Train Perplexity: 22.7507\n",
            "Epoch [1/3], Batch [11245/21340], Train Loss: 3.1643, Train Perplexity: 23.6713\n",
            "Epoch [1/3], Batch [11246/21340], Train Loss: 3.2831, Train Perplexity: 26.6579\n",
            "Epoch [1/3], Batch [11247/21340], Train Loss: 3.2309, Train Perplexity: 25.3035\n",
            "Epoch [1/3], Batch [11248/21340], Train Loss: 3.1804, Train Perplexity: 24.0555\n",
            "Epoch [1/3], Batch [11249/21340], Train Loss: 3.2473, Train Perplexity: 25.7212\n",
            "Epoch [1/3], Batch [11250/21340], Train Loss: 3.2695, Train Perplexity: 26.2975\n",
            "Epoch [1/3], Batch [11251/21340], Train Loss: 3.2677, Train Perplexity: 26.2499\n",
            "Epoch [1/3], Batch [11252/21340], Train Loss: 3.3093, Train Perplexity: 27.3653\n",
            "Epoch [1/3], Batch [11253/21340], Train Loss: 3.3322, Train Perplexity: 28.0000\n",
            "Epoch [1/3], Batch [11254/21340], Train Loss: 3.2107, Train Perplexity: 24.7971\n",
            "Epoch [1/3], Batch [11255/21340], Train Loss: 3.2262, Train Perplexity: 25.1836\n",
            "Epoch [1/3], Batch [11256/21340], Train Loss: 3.1693, Train Perplexity: 23.7899\n",
            "Epoch [1/3], Batch [11257/21340], Train Loss: 3.3864, Train Perplexity: 29.5594\n",
            "Epoch [1/3], Batch [11258/21340], Train Loss: 3.2379, Train Perplexity: 25.4789\n",
            "Epoch [1/3], Batch [11259/21340], Train Loss: 3.2193, Train Perplexity: 25.0111\n",
            "Epoch [1/3], Batch [11260/21340], Train Loss: 3.2600, Train Perplexity: 26.0500\n",
            "Epoch [1/3], Batch [11261/21340], Train Loss: 3.3041, Train Perplexity: 27.2245\n",
            "Epoch [1/3], Batch [11262/21340], Train Loss: 3.2394, Train Perplexity: 25.5184\n",
            "Epoch [1/3], Batch [11263/21340], Train Loss: 3.2312, Train Perplexity: 25.3104\n",
            "Epoch [1/3], Batch [11264/21340], Train Loss: 3.4531, Train Perplexity: 31.5973\n",
            "Epoch [1/3], Batch [11265/21340], Train Loss: 3.1955, Train Perplexity: 24.4218\n",
            "Epoch [1/3], Batch [11266/21340], Train Loss: 3.2574, Train Perplexity: 25.9816\n",
            "Epoch [1/3], Batch [11267/21340], Train Loss: 3.2432, Train Perplexity: 25.6160\n",
            "Epoch [1/3], Batch [11268/21340], Train Loss: 3.1436, Train Perplexity: 23.1883\n",
            "Epoch [1/3], Batch [11269/21340], Train Loss: 3.2412, Train Perplexity: 25.5635\n",
            "Epoch [1/3], Batch [11270/21340], Train Loss: 3.1595, Train Perplexity: 23.5596\n",
            "Epoch [1/3], Batch [11271/21340], Train Loss: 3.2996, Train Perplexity: 27.1011\n",
            "Epoch [1/3], Batch [11272/21340], Train Loss: 3.1580, Train Perplexity: 23.5247\n",
            "Epoch [1/3], Batch [11273/21340], Train Loss: 3.1962, Train Perplexity: 24.4386\n",
            "Epoch [1/3], Batch [11274/21340], Train Loss: 3.1505, Train Perplexity: 23.3484\n",
            "Epoch [1/3], Batch [11275/21340], Train Loss: 3.2111, Train Perplexity: 24.8071\n",
            "Epoch [1/3], Batch [11276/21340], Train Loss: 3.3483, Train Perplexity: 28.4531\n",
            "Epoch [1/3], Batch [11277/21340], Train Loss: 3.2521, Train Perplexity: 25.8454\n",
            "Epoch [1/3], Batch [11278/21340], Train Loss: 3.2098, Train Perplexity: 24.7750\n",
            "Epoch [1/3], Batch [11279/21340], Train Loss: 3.2632, Train Perplexity: 26.1334\n",
            "Epoch [1/3], Batch [11280/21340], Train Loss: 3.2647, Train Perplexity: 26.1717\n",
            "Epoch [1/3], Batch [11281/21340], Train Loss: 3.1674, Train Perplexity: 23.7450\n",
            "Epoch [1/3], Batch [11282/21340], Train Loss: 3.2611, Train Perplexity: 26.0782\n",
            "Epoch [1/3], Batch [11283/21340], Train Loss: 3.2557, Train Perplexity: 25.9377\n",
            "Epoch [1/3], Batch [11284/21340], Train Loss: 3.2123, Train Perplexity: 24.8356\n",
            "Epoch [1/3], Batch [11285/21340], Train Loss: 3.1464, Train Perplexity: 23.2528\n",
            "Epoch [1/3], Batch [11286/21340], Train Loss: 3.2467, Train Perplexity: 25.7047\n",
            "Epoch [1/3], Batch [11287/21340], Train Loss: 3.2591, Train Perplexity: 26.0263\n",
            "Epoch [1/3], Batch [11288/21340], Train Loss: 3.3355, Train Perplexity: 28.0936\n",
            "Epoch [1/3], Batch [11289/21340], Train Loss: 3.2455, Train Perplexity: 25.6741\n",
            "Epoch [1/3], Batch [11290/21340], Train Loss: 3.1622, Train Perplexity: 23.6236\n",
            "Epoch [1/3], Batch [11291/21340], Train Loss: 3.2120, Train Perplexity: 24.8287\n",
            "Epoch [1/3], Batch [11292/21340], Train Loss: 3.2745, Train Perplexity: 26.4294\n",
            "Epoch [1/3], Batch [11293/21340], Train Loss: 3.2613, Train Perplexity: 26.0846\n",
            "Epoch [1/3], Batch [11294/21340], Train Loss: 3.2120, Train Perplexity: 24.8297\n",
            "Epoch [1/3], Batch [11295/21340], Train Loss: 3.2366, Train Perplexity: 25.4465\n",
            "Epoch [1/3], Batch [11296/21340], Train Loss: 3.3428, Train Perplexity: 28.2971\n",
            "Epoch [1/3], Batch [11297/21340], Train Loss: 3.3428, Train Perplexity: 28.2991\n",
            "Epoch [1/3], Batch [11298/21340], Train Loss: 3.2083, Train Perplexity: 24.7377\n",
            "Epoch [1/3], Batch [11299/21340], Train Loss: 3.2137, Train Perplexity: 24.8709\n",
            "Epoch [1/3], Batch [11300/21340], Train Loss: 3.1429, Train Perplexity: 23.1720\n",
            "Epoch [1/3], Batch [11301/21340], Train Loss: 3.2115, Train Perplexity: 24.8151\n",
            "Epoch [1/3], Batch [11302/21340], Train Loss: 3.2114, Train Perplexity: 24.8131\n",
            "Epoch [1/3], Batch [11303/21340], Train Loss: 3.1195, Train Perplexity: 22.6343\n",
            "Epoch [1/3], Batch [11304/21340], Train Loss: 3.3301, Train Perplexity: 27.9400\n",
            "Epoch [1/3], Batch [11305/21340], Train Loss: 3.2760, Train Perplexity: 26.4697\n",
            "Epoch [1/3], Batch [11306/21340], Train Loss: 3.1921, Train Perplexity: 24.3383\n",
            "Epoch [1/3], Batch [11307/21340], Train Loss: 3.2874, Train Perplexity: 26.7730\n",
            "Epoch [1/3], Batch [11308/21340], Train Loss: 3.3524, Train Perplexity: 28.5707\n",
            "Epoch [1/3], Batch [11309/21340], Train Loss: 3.2293, Train Perplexity: 25.2620\n",
            "Epoch [1/3], Batch [11310/21340], Train Loss: 3.2704, Train Perplexity: 26.3218\n",
            "Epoch [1/3], Batch [11311/21340], Train Loss: 3.2379, Train Perplexity: 25.4810\n",
            "Epoch [1/3], Batch [11312/21340], Train Loss: 3.1928, Train Perplexity: 24.3567\n",
            "Epoch [1/3], Batch [11313/21340], Train Loss: 3.1715, Train Perplexity: 23.8436\n",
            "Epoch [1/3], Batch [11314/21340], Train Loss: 3.2175, Train Perplexity: 24.9659\n",
            "Epoch [1/3], Batch [11315/21340], Train Loss: 3.1690, Train Perplexity: 23.7844\n",
            "Epoch [1/3], Batch [11316/21340], Train Loss: 3.3561, Train Perplexity: 28.6773\n",
            "Epoch [1/3], Batch [11317/21340], Train Loss: 3.2780, Train Perplexity: 26.5222\n",
            "Epoch [1/3], Batch [11318/21340], Train Loss: 3.1650, Train Perplexity: 23.6899\n",
            "Epoch [1/3], Batch [11319/21340], Train Loss: 3.2879, Train Perplexity: 26.7878\n",
            "Epoch [1/3], Batch [11320/21340], Train Loss: 3.2053, Train Perplexity: 24.6628\n",
            "Epoch [1/3], Batch [11321/21340], Train Loss: 3.3328, Train Perplexity: 28.0158\n",
            "Epoch [1/3], Batch [11322/21340], Train Loss: 3.2219, Train Perplexity: 25.0745\n",
            "Epoch [1/3], Batch [11323/21340], Train Loss: 3.3163, Train Perplexity: 27.5571\n",
            "Epoch [1/3], Batch [11324/21340], Train Loss: 3.2181, Train Perplexity: 24.9813\n",
            "Epoch [1/3], Batch [11325/21340], Train Loss: 3.2787, Train Perplexity: 26.5414\n",
            "Epoch [1/3], Batch [11326/21340], Train Loss: 3.1519, Train Perplexity: 23.3801\n",
            "Epoch [1/3], Batch [11327/21340], Train Loss: 3.3352, Train Perplexity: 28.0850\n",
            "Epoch [1/3], Batch [11328/21340], Train Loss: 3.2679, Train Perplexity: 26.2559\n",
            "Epoch [1/3], Batch [11329/21340], Train Loss: 3.1433, Train Perplexity: 23.1808\n",
            "Epoch [1/3], Batch [11330/21340], Train Loss: 3.1882, Train Perplexity: 24.2456\n",
            "Epoch [1/3], Batch [11331/21340], Train Loss: 3.2349, Train Perplexity: 25.4029\n",
            "Epoch [1/3], Batch [11332/21340], Train Loss: 3.1710, Train Perplexity: 23.8303\n",
            "Epoch [1/3], Batch [11333/21340], Train Loss: 3.2804, Train Perplexity: 26.5854\n",
            "Epoch [1/3], Batch [11334/21340], Train Loss: 3.2188, Train Perplexity: 24.9989\n",
            "Epoch [1/3], Batch [11335/21340], Train Loss: 3.1840, Train Perplexity: 24.1441\n",
            "Epoch [1/3], Batch [11336/21340], Train Loss: 3.3670, Train Perplexity: 28.9902\n",
            "Epoch [1/3], Batch [11337/21340], Train Loss: 3.2233, Train Perplexity: 25.1111\n",
            "Epoch [1/3], Batch [11338/21340], Train Loss: 3.1748, Train Perplexity: 23.9230\n",
            "Epoch [1/3], Batch [11339/21340], Train Loss: 3.1885, Train Perplexity: 24.2521\n",
            "Epoch [1/3], Batch [11340/21340], Train Loss: 3.2014, Train Perplexity: 24.5660\n",
            "Epoch [1/3], Batch [11341/21340], Train Loss: 3.1446, Train Perplexity: 23.2100\n",
            "Epoch [1/3], Batch [11342/21340], Train Loss: 3.2019, Train Perplexity: 24.5803\n",
            "Epoch [1/3], Batch [11343/21340], Train Loss: 3.1751, Train Perplexity: 23.9286\n",
            "Epoch [1/3], Batch [11344/21340], Train Loss: 3.3136, Train Perplexity: 27.4850\n",
            "Epoch [1/3], Batch [11345/21340], Train Loss: 3.2261, Train Perplexity: 25.1822\n",
            "Epoch [1/3], Batch [11346/21340], Train Loss: 3.4600, Train Perplexity: 31.8157\n",
            "Epoch [1/3], Batch [11347/21340], Train Loss: 3.2463, Train Perplexity: 25.6958\n",
            "Epoch [1/3], Batch [11348/21340], Train Loss: 3.1647, Train Perplexity: 23.6813\n",
            "Epoch [1/3], Batch [11349/21340], Train Loss: 3.1640, Train Perplexity: 23.6659\n",
            "Epoch [1/3], Batch [11350/21340], Train Loss: 3.2790, Train Perplexity: 26.5505\n",
            "Epoch [1/3], Batch [11351/21340], Train Loss: 3.2961, Train Perplexity: 27.0061\n",
            "Epoch [1/3], Batch [11352/21340], Train Loss: 3.2324, Train Perplexity: 25.3393\n",
            "Epoch [1/3], Batch [11353/21340], Train Loss: 3.1962, Train Perplexity: 24.4391\n",
            "Epoch [1/3], Batch [11354/21340], Train Loss: 3.2154, Train Perplexity: 24.9129\n",
            "Epoch [1/3], Batch [11355/21340], Train Loss: 3.1352, Train Perplexity: 22.9931\n",
            "Epoch [1/3], Batch [11356/21340], Train Loss: 3.3360, Train Perplexity: 28.1053\n",
            "Epoch [1/3], Batch [11357/21340], Train Loss: 3.3516, Train Perplexity: 28.5472\n",
            "Epoch [1/3], Batch [11358/21340], Train Loss: 3.1986, Train Perplexity: 24.4980\n",
            "Epoch [1/3], Batch [11359/21340], Train Loss: 3.1468, Train Perplexity: 23.2607\n",
            "Epoch [1/3], Batch [11360/21340], Train Loss: 3.1994, Train Perplexity: 24.5170\n",
            "Epoch [1/3], Batch [11361/21340], Train Loss: 3.2163, Train Perplexity: 24.9366\n",
            "Epoch [1/3], Batch [11362/21340], Train Loss: 3.2770, Train Perplexity: 26.4957\n",
            "Epoch [1/3], Batch [11363/21340], Train Loss: 3.2695, Train Perplexity: 26.2990\n",
            "Epoch [1/3], Batch [11364/21340], Train Loss: 3.1978, Train Perplexity: 24.4797\n",
            "Epoch [1/3], Batch [11365/21340], Train Loss: 3.2551, Train Perplexity: 25.9215\n",
            "Epoch [1/3], Batch [11366/21340], Train Loss: 3.2909, Train Perplexity: 26.8682\n",
            "Epoch [1/3], Batch [11367/21340], Train Loss: 3.2185, Train Perplexity: 24.9900\n",
            "Epoch [1/3], Batch [11368/21340], Train Loss: 3.2756, Train Perplexity: 26.4596\n",
            "Epoch [1/3], Batch [11369/21340], Train Loss: 3.2981, Train Perplexity: 27.0624\n",
            "Epoch [1/3], Batch [11370/21340], Train Loss: 3.3352, Train Perplexity: 28.0835\n",
            "Epoch [1/3], Batch [11371/21340], Train Loss: 3.2511, Train Perplexity: 25.8193\n",
            "Epoch [1/3], Batch [11372/21340], Train Loss: 3.2034, Train Perplexity: 24.6169\n",
            "Epoch [1/3], Batch [11373/21340], Train Loss: 3.2065, Train Perplexity: 24.6936\n",
            "Epoch [1/3], Batch [11374/21340], Train Loss: 3.2075, Train Perplexity: 24.7167\n",
            "Epoch [1/3], Batch [11375/21340], Train Loss: 3.2980, Train Perplexity: 27.0587\n",
            "Epoch [1/3], Batch [11376/21340], Train Loss: 3.3742, Train Perplexity: 29.2009\n",
            "Epoch [1/3], Batch [11377/21340], Train Loss: 3.3492, Train Perplexity: 28.4786\n",
            "Epoch [1/3], Batch [11378/21340], Train Loss: 3.3733, Train Perplexity: 29.1742\n",
            "Epoch [1/3], Batch [11379/21340], Train Loss: 3.1774, Train Perplexity: 23.9848\n",
            "Epoch [1/3], Batch [11380/21340], Train Loss: 3.1958, Train Perplexity: 24.4306\n",
            "Epoch [1/3], Batch [11381/21340], Train Loss: 3.2251, Train Perplexity: 25.1555\n",
            "Epoch [1/3], Batch [11382/21340], Train Loss: 3.2776, Train Perplexity: 26.5125\n",
            "Epoch [1/3], Batch [11383/21340], Train Loss: 3.2063, Train Perplexity: 24.6887\n",
            "Epoch [1/3], Batch [11384/21340], Train Loss: 3.2137, Train Perplexity: 24.8698\n",
            "Epoch [1/3], Batch [11385/21340], Train Loss: 3.3623, Train Perplexity: 28.8548\n",
            "Epoch [1/3], Batch [11386/21340], Train Loss: 3.2079, Train Perplexity: 24.7272\n",
            "Epoch [1/3], Batch [11387/21340], Train Loss: 3.1792, Train Perplexity: 24.0265\n",
            "Epoch [1/3], Batch [11388/21340], Train Loss: 3.2480, Train Perplexity: 25.7385\n",
            "Epoch [1/3], Batch [11389/21340], Train Loss: 3.5152, Train Perplexity: 33.6218\n",
            "Epoch [1/3], Batch [11390/21340], Train Loss: 3.2057, Train Perplexity: 24.6733\n",
            "Epoch [1/3], Batch [11391/21340], Train Loss: 3.2876, Train Perplexity: 26.7796\n",
            "Epoch [1/3], Batch [11392/21340], Train Loss: 3.2648, Train Perplexity: 26.1746\n",
            "Epoch [1/3], Batch [11393/21340], Train Loss: 3.3997, Train Perplexity: 29.9538\n",
            "Epoch [1/3], Batch [11394/21340], Train Loss: 3.1719, Train Perplexity: 23.8537\n",
            "Epoch [1/3], Batch [11395/21340], Train Loss: 3.2876, Train Perplexity: 26.7798\n",
            "Epoch [1/3], Batch [11396/21340], Train Loss: 3.2292, Train Perplexity: 25.2599\n",
            "Epoch [1/3], Batch [11397/21340], Train Loss: 3.1568, Train Perplexity: 23.4951\n",
            "Epoch [1/3], Batch [11398/21340], Train Loss: 3.2141, Train Perplexity: 24.8816\n",
            "Epoch [1/3], Batch [11399/21340], Train Loss: 3.2277, Train Perplexity: 25.2208\n",
            "Epoch [1/3], Batch [11400/21340], Train Loss: 3.3030, Train Perplexity: 27.1953\n",
            "Epoch [1/3], Batch [11401/21340], Train Loss: 3.2715, Train Perplexity: 26.3507\n",
            "Epoch [1/3], Batch [11402/21340], Train Loss: 3.3817, Train Perplexity: 29.4205\n",
            "Epoch [1/3], Batch [11403/21340], Train Loss: 3.2686, Train Perplexity: 26.2743\n",
            "Epoch [1/3], Batch [11404/21340], Train Loss: 3.1017, Train Perplexity: 22.2363\n",
            "Epoch [1/3], Batch [11405/21340], Train Loss: 3.1793, Train Perplexity: 24.0311\n",
            "Epoch [1/3], Batch [11406/21340], Train Loss: 3.1449, Train Perplexity: 23.2169\n",
            "Epoch [1/3], Batch [11407/21340], Train Loss: 3.2583, Train Perplexity: 26.0046\n",
            "Epoch [1/3], Batch [11408/21340], Train Loss: 3.2230, Train Perplexity: 25.1026\n",
            "Epoch [1/3], Batch [11409/21340], Train Loss: 3.2927, Train Perplexity: 26.9157\n",
            "Epoch [1/3], Batch [11410/21340], Train Loss: 3.3168, Train Perplexity: 27.5721\n",
            "Epoch [1/3], Batch [11411/21340], Train Loss: 3.2371, Train Perplexity: 25.4594\n",
            "Epoch [1/3], Batch [11412/21340], Train Loss: 3.2283, Train Perplexity: 25.2366\n",
            "Epoch [1/3], Batch [11413/21340], Train Loss: 3.2319, Train Perplexity: 25.3288\n",
            "Epoch [1/3], Batch [11414/21340], Train Loss: 3.1140, Train Perplexity: 22.5110\n",
            "Epoch [1/3], Batch [11415/21340], Train Loss: 3.1830, Train Perplexity: 24.1193\n",
            "Epoch [1/3], Batch [11416/21340], Train Loss: 3.2140, Train Perplexity: 24.8793\n",
            "Epoch [1/3], Batch [11417/21340], Train Loss: 3.2710, Train Perplexity: 26.3366\n",
            "Epoch [1/3], Batch [11418/21340], Train Loss: 3.3134, Train Perplexity: 27.4794\n",
            "Epoch [1/3], Batch [11419/21340], Train Loss: 3.2126, Train Perplexity: 24.8444\n",
            "Epoch [1/3], Batch [11420/21340], Train Loss: 3.2266, Train Perplexity: 25.1949\n",
            "Epoch [1/3], Batch [11421/21340], Train Loss: 3.3206, Train Perplexity: 27.6757\n",
            "Epoch [1/3], Batch [11422/21340], Train Loss: 3.2645, Train Perplexity: 26.1676\n",
            "Epoch [1/3], Batch [11423/21340], Train Loss: 3.1468, Train Perplexity: 23.2619\n",
            "Epoch [1/3], Batch [11424/21340], Train Loss: 3.2854, Train Perplexity: 26.7193\n",
            "Epoch [1/3], Batch [11425/21340], Train Loss: 3.1990, Train Perplexity: 24.5070\n",
            "Epoch [1/3], Batch [11426/21340], Train Loss: 3.1558, Train Perplexity: 23.4712\n",
            "Epoch [1/3], Batch [11427/21340], Train Loss: 3.2773, Train Perplexity: 26.5051\n",
            "Epoch [1/3], Batch [11428/21340], Train Loss: 3.2447, Train Perplexity: 25.6540\n",
            "Epoch [1/3], Batch [11429/21340], Train Loss: 3.1426, Train Perplexity: 23.1650\n",
            "Epoch [1/3], Batch [11430/21340], Train Loss: 3.3304, Train Perplexity: 27.9486\n",
            "Epoch [1/3], Batch [11431/21340], Train Loss: 3.4040, Train Perplexity: 30.0850\n",
            "Epoch [1/3], Batch [11432/21340], Train Loss: 3.2287, Train Perplexity: 25.2474\n",
            "Epoch [1/3], Batch [11433/21340], Train Loss: 3.3217, Train Perplexity: 27.7070\n",
            "Epoch [1/3], Batch [11434/21340], Train Loss: 3.2389, Train Perplexity: 25.5060\n",
            "Epoch [1/3], Batch [11435/21340], Train Loss: 3.1473, Train Perplexity: 23.2736\n",
            "Epoch [1/3], Batch [11436/21340], Train Loss: 3.4047, Train Perplexity: 30.1054\n",
            "Epoch [1/3], Batch [11437/21340], Train Loss: 3.2747, Train Perplexity: 26.4360\n",
            "Epoch [1/3], Batch [11438/21340], Train Loss: 3.3047, Train Perplexity: 27.2407\n",
            "Epoch [1/3], Batch [11439/21340], Train Loss: 3.2946, Train Perplexity: 26.9678\n",
            "Epoch [1/3], Batch [11440/21340], Train Loss: 3.2070, Train Perplexity: 24.7060\n",
            "Epoch [1/3], Batch [11441/21340], Train Loss: 3.2475, Train Perplexity: 25.7262\n",
            "Epoch [1/3], Batch [11442/21340], Train Loss: 3.1158, Train Perplexity: 22.5520\n",
            "Epoch [1/3], Batch [11443/21340], Train Loss: 3.2010, Train Perplexity: 24.5576\n",
            "Epoch [1/3], Batch [11444/21340], Train Loss: 3.2216, Train Perplexity: 25.0671\n",
            "Epoch [1/3], Batch [11445/21340], Train Loss: 3.2294, Train Perplexity: 25.2644\n",
            "Epoch [1/3], Batch [11446/21340], Train Loss: 3.1571, Train Perplexity: 23.5020\n",
            "Epoch [1/3], Batch [11447/21340], Train Loss: 3.5161, Train Perplexity: 33.6544\n",
            "Epoch [1/3], Batch [11448/21340], Train Loss: 3.2930, Train Perplexity: 26.9245\n",
            "Epoch [1/3], Batch [11449/21340], Train Loss: 3.3041, Train Perplexity: 27.2241\n",
            "Epoch [1/3], Batch [11450/21340], Train Loss: 3.3151, Train Perplexity: 27.5260\n",
            "Epoch [1/3], Batch [11451/21340], Train Loss: 3.2074, Train Perplexity: 24.7154\n",
            "Epoch [1/3], Batch [11452/21340], Train Loss: 3.1519, Train Perplexity: 23.3810\n",
            "Epoch [1/3], Batch [11453/21340], Train Loss: 3.3995, Train Perplexity: 29.9495\n",
            "Epoch [1/3], Batch [11454/21340], Train Loss: 3.3227, Train Perplexity: 27.7340\n",
            "Epoch [1/3], Batch [11455/21340], Train Loss: 3.1987, Train Perplexity: 24.5005\n",
            "Epoch [1/3], Batch [11456/21340], Train Loss: 3.1099, Train Perplexity: 22.4187\n",
            "Epoch [1/3], Batch [11457/21340], Train Loss: 3.1853, Train Perplexity: 24.1747\n",
            "Epoch [1/3], Batch [11458/21340], Train Loss: 3.3361, Train Perplexity: 28.1080\n",
            "Epoch [1/3], Batch [11459/21340], Train Loss: 3.1878, Train Perplexity: 24.2356\n",
            "Epoch [1/3], Batch [11460/21340], Train Loss: 3.1900, Train Perplexity: 24.2881\n",
            "Epoch [1/3], Batch [11461/21340], Train Loss: 3.2319, Train Perplexity: 25.3274\n",
            "Epoch [1/3], Batch [11462/21340], Train Loss: 3.3020, Train Perplexity: 27.1670\n",
            "Epoch [1/3], Batch [11463/21340], Train Loss: 3.2917, Train Perplexity: 26.8892\n",
            "Epoch [1/3], Batch [11464/21340], Train Loss: 3.1774, Train Perplexity: 23.9841\n",
            "Epoch [1/3], Batch [11465/21340], Train Loss: 3.1910, Train Perplexity: 24.3126\n",
            "Epoch [1/3], Batch [11466/21340], Train Loss: 3.2307, Train Perplexity: 25.2978\n",
            "Epoch [1/3], Batch [11467/21340], Train Loss: 3.2262, Train Perplexity: 25.1848\n",
            "Epoch [1/3], Batch [11468/21340], Train Loss: 3.2005, Train Perplexity: 24.5451\n",
            "Epoch [1/3], Batch [11469/21340], Train Loss: 3.2251, Train Perplexity: 25.1550\n",
            "Epoch [1/3], Batch [11470/21340], Train Loss: 3.2461, Train Perplexity: 25.6895\n",
            "Epoch [1/3], Batch [11471/21340], Train Loss: 3.2992, Train Perplexity: 27.0910\n",
            "Epoch [1/3], Batch [11472/21340], Train Loss: 3.2650, Train Perplexity: 26.1798\n",
            "Epoch [1/3], Batch [11473/21340], Train Loss: 3.2479, Train Perplexity: 25.7367\n",
            "Epoch [1/3], Batch [11474/21340], Train Loss: 3.3371, Train Perplexity: 28.1360\n",
            "Epoch [1/3], Batch [11475/21340], Train Loss: 3.2962, Train Perplexity: 27.0085\n",
            "Epoch [1/3], Batch [11476/21340], Train Loss: 3.1826, Train Perplexity: 24.1098\n",
            "Epoch [1/3], Batch [11477/21340], Train Loss: 3.1487, Train Perplexity: 23.3060\n",
            "Epoch [1/3], Batch [11478/21340], Train Loss: 3.3353, Train Perplexity: 28.0872\n",
            "Epoch [1/3], Batch [11479/21340], Train Loss: 3.1738, Train Perplexity: 23.8984\n",
            "Epoch [1/3], Batch [11480/21340], Train Loss: 3.2419, Train Perplexity: 25.5825\n",
            "Epoch [1/3], Batch [11481/21340], Train Loss: 3.1919, Train Perplexity: 24.3354\n",
            "Epoch [1/3], Batch [11482/21340], Train Loss: 3.2848, Train Perplexity: 26.7048\n",
            "Epoch [1/3], Batch [11483/21340], Train Loss: 3.2032, Train Perplexity: 24.6109\n",
            "Epoch [1/3], Batch [11484/21340], Train Loss: 3.2862, Train Perplexity: 26.7410\n",
            "Epoch [1/3], Batch [11485/21340], Train Loss: 3.1795, Train Perplexity: 24.0343\n",
            "Epoch [1/3], Batch [11486/21340], Train Loss: 3.2842, Train Perplexity: 26.6866\n",
            "Epoch [1/3], Batch [11487/21340], Train Loss: 3.1449, Train Perplexity: 23.2174\n",
            "Epoch [1/3], Batch [11488/21340], Train Loss: 3.2015, Train Perplexity: 24.5698\n",
            "Epoch [1/3], Batch [11489/21340], Train Loss: 3.1970, Train Perplexity: 24.4579\n",
            "Epoch [1/3], Batch [11490/21340], Train Loss: 3.3044, Train Perplexity: 27.2330\n",
            "Epoch [1/3], Batch [11491/21340], Train Loss: 3.2388, Train Perplexity: 25.5035\n",
            "Epoch [1/3], Batch [11492/21340], Train Loss: 3.2698, Train Perplexity: 26.3057\n",
            "Epoch [1/3], Batch [11493/21340], Train Loss: 3.3410, Train Perplexity: 28.2472\n",
            "Epoch [1/3], Batch [11494/21340], Train Loss: 3.2842, Train Perplexity: 26.6871\n",
            "Epoch [1/3], Batch [11495/21340], Train Loss: 3.2254, Train Perplexity: 25.1638\n",
            "Epoch [1/3], Batch [11496/21340], Train Loss: 3.1918, Train Perplexity: 24.3329\n",
            "Epoch [1/3], Batch [11497/21340], Train Loss: 3.3062, Train Perplexity: 27.2811\n",
            "Epoch [1/3], Batch [11498/21340], Train Loss: 3.1620, Train Perplexity: 23.6189\n",
            "Epoch [1/3], Batch [11499/21340], Train Loss: 3.2444, Train Perplexity: 25.6455\n",
            "Epoch [1/3], Batch [11500/21340], Train Loss: 3.1711, Train Perplexity: 23.8344\n",
            "Epoch [1/3], Batch [11501/21340], Train Loss: 3.2828, Train Perplexity: 26.6493\n",
            "Epoch [1/3], Batch [11502/21340], Train Loss: 3.2212, Train Perplexity: 25.0578\n",
            "Epoch [1/3], Batch [11503/21340], Train Loss: 3.2285, Train Perplexity: 25.2422\n",
            "Epoch [1/3], Batch [11504/21340], Train Loss: 3.2651, Train Perplexity: 26.1819\n",
            "Epoch [1/3], Batch [11505/21340], Train Loss: 3.2467, Train Perplexity: 25.7057\n",
            "Epoch [1/3], Batch [11506/21340], Train Loss: 3.1954, Train Perplexity: 24.4202\n",
            "Epoch [1/3], Batch [11507/21340], Train Loss: 3.2658, Train Perplexity: 26.2019\n",
            "Epoch [1/3], Batch [11508/21340], Train Loss: 3.2919, Train Perplexity: 26.8933\n",
            "Epoch [1/3], Batch [11509/21340], Train Loss: 3.2066, Train Perplexity: 24.6960\n",
            "Epoch [1/3], Batch [11510/21340], Train Loss: 3.1873, Train Perplexity: 24.2241\n",
            "Epoch [1/3], Batch [11511/21340], Train Loss: 3.1738, Train Perplexity: 23.8970\n",
            "Epoch [1/3], Batch [11512/21340], Train Loss: 3.2511, Train Perplexity: 25.8194\n",
            "Epoch [1/3], Batch [11513/21340], Train Loss: 3.2911, Train Perplexity: 26.8721\n",
            "Epoch [1/3], Batch [11514/21340], Train Loss: 3.2036, Train Perplexity: 24.6213\n",
            "Epoch [1/3], Batch [11515/21340], Train Loss: 3.2308, Train Perplexity: 25.2996\n",
            "Epoch [1/3], Batch [11516/21340], Train Loss: 3.2184, Train Perplexity: 24.9871\n",
            "Epoch [1/3], Batch [11517/21340], Train Loss: 3.2325, Train Perplexity: 25.3423\n",
            "Epoch [1/3], Batch [11518/21340], Train Loss: 3.2322, Train Perplexity: 25.3351\n",
            "Epoch [1/3], Batch [11519/21340], Train Loss: 3.1798, Train Perplexity: 24.0422\n",
            "Epoch [1/3], Batch [11520/21340], Train Loss: 3.1997, Train Perplexity: 24.5254\n",
            "Epoch [1/3], Batch [11521/21340], Train Loss: 3.2862, Train Perplexity: 26.7399\n",
            "Epoch [1/3], Batch [11522/21340], Train Loss: 3.3978, Train Perplexity: 29.8993\n",
            "Epoch [1/3], Batch [11523/21340], Train Loss: 3.2203, Train Perplexity: 25.0357\n",
            "Epoch [1/3], Batch [11524/21340], Train Loss: 3.2476, Train Perplexity: 25.7278\n",
            "Epoch [1/3], Batch [11525/21340], Train Loss: 3.2126, Train Perplexity: 24.8431\n",
            "Epoch [1/3], Batch [11526/21340], Train Loss: 3.3919, Train Perplexity: 29.7222\n",
            "Epoch [1/3], Batch [11527/21340], Train Loss: 3.3971, Train Perplexity: 29.8765\n",
            "Epoch [1/3], Batch [11528/21340], Train Loss: 3.2464, Train Perplexity: 25.6972\n",
            "Epoch [1/3], Batch [11529/21340], Train Loss: 3.3125, Train Perplexity: 27.4537\n",
            "Epoch [1/3], Batch [11530/21340], Train Loss: 3.2269, Train Perplexity: 25.2023\n",
            "Epoch [1/3], Batch [11531/21340], Train Loss: 3.1834, Train Perplexity: 24.1279\n",
            "Epoch [1/3], Batch [11532/21340], Train Loss: 3.2378, Train Perplexity: 25.4787\n",
            "Epoch [1/3], Batch [11533/21340], Train Loss: 3.2997, Train Perplexity: 27.1051\n",
            "Epoch [1/3], Batch [11534/21340], Train Loss: 3.1152, Train Perplexity: 22.5386\n",
            "Epoch [1/3], Batch [11535/21340], Train Loss: 3.1978, Train Perplexity: 24.4789\n",
            "Epoch [1/3], Batch [11536/21340], Train Loss: 3.1754, Train Perplexity: 23.9369\n",
            "Epoch [1/3], Batch [11537/21340], Train Loss: 3.3737, Train Perplexity: 29.1852\n",
            "Epoch [1/3], Batch [11538/21340], Train Loss: 3.4305, Train Perplexity: 30.8907\n",
            "Epoch [1/3], Batch [11539/21340], Train Loss: 3.1828, Train Perplexity: 24.1143\n",
            "Epoch [1/3], Batch [11540/21340], Train Loss: 3.2743, Train Perplexity: 26.4244\n",
            "Epoch [1/3], Batch [11541/21340], Train Loss: 3.2337, Train Perplexity: 25.3727\n",
            "Epoch [1/3], Batch [11542/21340], Train Loss: 3.2197, Train Perplexity: 25.0198\n",
            "Epoch [1/3], Batch [11543/21340], Train Loss: 3.2154, Train Perplexity: 24.9134\n",
            "Epoch [1/3], Batch [11544/21340], Train Loss: 3.2754, Train Perplexity: 26.4548\n",
            "Epoch [1/3], Batch [11545/21340], Train Loss: 3.2293, Train Perplexity: 25.2622\n",
            "Epoch [1/3], Batch [11546/21340], Train Loss: 3.2872, Train Perplexity: 26.7689\n",
            "Epoch [1/3], Batch [11547/21340], Train Loss: 3.2467, Train Perplexity: 25.7065\n",
            "Epoch [1/3], Batch [11548/21340], Train Loss: 3.2631, Train Perplexity: 26.1295\n",
            "Epoch [1/3], Batch [11549/21340], Train Loss: 3.2626, Train Perplexity: 26.1175\n",
            "Epoch [1/3], Batch [11550/21340], Train Loss: 3.1264, Train Perplexity: 22.7911\n",
            "Epoch [1/3], Batch [11551/21340], Train Loss: 3.2353, Train Perplexity: 25.4141\n",
            "Epoch [1/3], Batch [11552/21340], Train Loss: 3.2632, Train Perplexity: 26.1333\n",
            "Epoch [1/3], Batch [11553/21340], Train Loss: 3.1975, Train Perplexity: 24.4720\n",
            "Epoch [1/3], Batch [11554/21340], Train Loss: 3.2255, Train Perplexity: 25.1666\n",
            "Epoch [1/3], Batch [11555/21340], Train Loss: 3.3261, Train Perplexity: 27.8298\n",
            "Epoch [1/3], Batch [11556/21340], Train Loss: 3.3184, Train Perplexity: 27.6161\n",
            "Epoch [1/3], Batch [11557/21340], Train Loss: 3.2938, Train Perplexity: 26.9460\n",
            "Epoch [1/3], Batch [11558/21340], Train Loss: 3.1876, Train Perplexity: 24.2313\n",
            "Epoch [1/3], Batch [11559/21340], Train Loss: 3.2232, Train Perplexity: 25.1075\n",
            "Epoch [1/3], Batch [11560/21340], Train Loss: 3.2446, Train Perplexity: 25.6508\n",
            "Epoch [1/3], Batch [11561/21340], Train Loss: 3.2908, Train Perplexity: 26.8640\n",
            "Epoch [1/3], Batch [11562/21340], Train Loss: 3.2856, Train Perplexity: 26.7262\n",
            "Epoch [1/3], Batch [11563/21340], Train Loss: 3.2225, Train Perplexity: 25.0917\n",
            "Epoch [1/3], Batch [11564/21340], Train Loss: 3.2194, Train Perplexity: 25.0138\n",
            "Epoch [1/3], Batch [11565/21340], Train Loss: 3.2717, Train Perplexity: 26.3555\n",
            "Epoch [1/3], Batch [11566/21340], Train Loss: 3.2218, Train Perplexity: 25.0730\n",
            "Epoch [1/3], Batch [11567/21340], Train Loss: 3.1515, Train Perplexity: 23.3700\n",
            "Epoch [1/3], Batch [11568/21340], Train Loss: 3.2602, Train Perplexity: 26.0549\n",
            "Epoch [1/3], Batch [11569/21340], Train Loss: 3.2632, Train Perplexity: 26.1325\n",
            "Epoch [1/3], Batch [11570/21340], Train Loss: 3.1849, Train Perplexity: 24.1645\n",
            "Epoch [1/3], Batch [11571/21340], Train Loss: 3.2143, Train Perplexity: 24.8866\n",
            "Epoch [1/3], Batch [11572/21340], Train Loss: 3.1989, Train Perplexity: 24.5062\n",
            "Epoch [1/3], Batch [11573/21340], Train Loss: 3.3771, Train Perplexity: 29.2853\n",
            "Epoch [1/3], Batch [11574/21340], Train Loss: 3.2234, Train Perplexity: 25.1144\n",
            "Epoch [1/3], Batch [11575/21340], Train Loss: 3.2964, Train Perplexity: 27.0148\n",
            "Epoch [1/3], Batch [11576/21340], Train Loss: 3.3220, Train Perplexity: 27.7153\n",
            "Epoch [1/3], Batch [11577/21340], Train Loss: 3.2470, Train Perplexity: 25.7137\n",
            "Epoch [1/3], Batch [11578/21340], Train Loss: 3.5434, Train Perplexity: 34.5827\n",
            "Epoch [1/3], Batch [11579/21340], Train Loss: 3.2856, Train Perplexity: 26.7260\n",
            "Epoch [1/3], Batch [11580/21340], Train Loss: 3.2464, Train Perplexity: 25.6971\n",
            "Epoch [1/3], Batch [11581/21340], Train Loss: 3.2028, Train Perplexity: 24.6008\n",
            "Epoch [1/3], Batch [11582/21340], Train Loss: 3.2820, Train Perplexity: 26.6287\n",
            "Epoch [1/3], Batch [11583/21340], Train Loss: 3.1561, Train Perplexity: 23.4782\n",
            "Epoch [1/3], Batch [11584/21340], Train Loss: 3.2210, Train Perplexity: 25.0529\n",
            "Epoch [1/3], Batch [11585/21340], Train Loss: 3.1759, Train Perplexity: 23.9489\n",
            "Epoch [1/3], Batch [11586/21340], Train Loss: 3.3364, Train Perplexity: 28.1178\n",
            "Epoch [1/3], Batch [11587/21340], Train Loss: 3.2311, Train Perplexity: 25.3066\n",
            "Epoch [1/3], Batch [11588/21340], Train Loss: 3.1255, Train Perplexity: 22.7711\n",
            "Epoch [1/3], Batch [11589/21340], Train Loss: 3.2085, Train Perplexity: 24.7421\n",
            "Epoch [1/3], Batch [11590/21340], Train Loss: 3.3257, Train Perplexity: 27.8185\n",
            "Epoch [1/3], Batch [11591/21340], Train Loss: 3.2970, Train Perplexity: 27.0313\n",
            "Epoch [1/3], Batch [11592/21340], Train Loss: 3.2530, Train Perplexity: 25.8668\n",
            "Epoch [1/3], Batch [11593/21340], Train Loss: 3.3022, Train Perplexity: 27.1729\n",
            "Epoch [1/3], Batch [11594/21340], Train Loss: 3.2554, Train Perplexity: 25.9298\n",
            "Epoch [1/3], Batch [11595/21340], Train Loss: 3.2827, Train Perplexity: 26.6479\n",
            "Epoch [1/3], Batch [11596/21340], Train Loss: 3.1420, Train Perplexity: 23.1495\n",
            "Epoch [1/3], Batch [11597/21340], Train Loss: 3.1734, Train Perplexity: 23.8897\n",
            "Epoch [1/3], Batch [11598/21340], Train Loss: 3.1835, Train Perplexity: 24.1316\n",
            "Epoch [1/3], Batch [11599/21340], Train Loss: 3.2581, Train Perplexity: 25.9994\n",
            "Epoch [1/3], Batch [11600/21340], Train Loss: 3.2955, Train Perplexity: 26.9903\n",
            "Epoch [1/3], Batch [11601/21340], Train Loss: 3.2236, Train Perplexity: 25.1185\n",
            "Epoch [1/3], Batch [11602/21340], Train Loss: 3.2325, Train Perplexity: 25.3439\n",
            "Epoch [1/3], Batch [11603/21340], Train Loss: 3.2950, Train Perplexity: 26.9775\n",
            "Epoch [1/3], Batch [11604/21340], Train Loss: 3.0912, Train Perplexity: 22.0043\n",
            "Epoch [1/3], Batch [11605/21340], Train Loss: 3.1718, Train Perplexity: 23.8502\n",
            "Epoch [1/3], Batch [11606/21340], Train Loss: 3.3209, Train Perplexity: 27.6858\n",
            "Epoch [1/3], Batch [11607/21340], Train Loss: 3.1843, Train Perplexity: 24.1507\n",
            "Epoch [1/3], Batch [11608/21340], Train Loss: 3.2892, Train Perplexity: 26.8220\n",
            "Epoch [1/3], Batch [11609/21340], Train Loss: 3.2237, Train Perplexity: 25.1199\n",
            "Epoch [1/3], Batch [11610/21340], Train Loss: 3.1979, Train Perplexity: 24.4807\n",
            "Epoch [1/3], Batch [11611/21340], Train Loss: 3.2739, Train Perplexity: 26.4132\n",
            "Epoch [1/3], Batch [11612/21340], Train Loss: 3.3951, Train Perplexity: 29.8173\n",
            "Epoch [1/3], Batch [11613/21340], Train Loss: 3.1741, Train Perplexity: 23.9041\n",
            "Epoch [1/3], Batch [11614/21340], Train Loss: 3.2200, Train Perplexity: 25.0291\n",
            "Epoch [1/3], Batch [11615/21340], Train Loss: 3.2865, Train Perplexity: 26.7484\n",
            "Epoch [1/3], Batch [11616/21340], Train Loss: 3.3055, Train Perplexity: 27.2635\n",
            "Epoch [1/3], Batch [11617/21340], Train Loss: 3.3009, Train Perplexity: 27.1371\n",
            "Epoch [1/3], Batch [11618/21340], Train Loss: 3.2990, Train Perplexity: 27.0867\n",
            "Epoch [1/3], Batch [11619/21340], Train Loss: 3.1884, Train Perplexity: 24.2499\n",
            "Epoch [1/3], Batch [11620/21340], Train Loss: 3.2815, Train Perplexity: 26.6164\n",
            "Epoch [1/3], Batch [11621/21340], Train Loss: 3.2013, Train Perplexity: 24.5646\n",
            "Epoch [1/3], Batch [11622/21340], Train Loss: 3.3043, Train Perplexity: 27.2293\n",
            "Epoch [1/3], Batch [11623/21340], Train Loss: 3.1556, Train Perplexity: 23.4672\n",
            "Epoch [1/3], Batch [11624/21340], Train Loss: 3.3076, Train Perplexity: 27.3195\n",
            "Epoch [1/3], Batch [11625/21340], Train Loss: 3.2517, Train Perplexity: 25.8332\n",
            "Epoch [1/3], Batch [11626/21340], Train Loss: 3.1689, Train Perplexity: 23.7824\n",
            "Epoch [1/3], Batch [11627/21340], Train Loss: 3.2386, Train Perplexity: 25.4982\n",
            "Epoch [1/3], Batch [11628/21340], Train Loss: 3.1522, Train Perplexity: 23.3880\n",
            "Epoch [1/3], Batch [11629/21340], Train Loss: 3.1832, Train Perplexity: 24.1241\n",
            "Epoch [1/3], Batch [11630/21340], Train Loss: 3.1888, Train Perplexity: 24.2589\n",
            "Epoch [1/3], Batch [11631/21340], Train Loss: 3.4007, Train Perplexity: 29.9849\n",
            "Epoch [1/3], Batch [11632/21340], Train Loss: 3.3094, Train Perplexity: 27.3696\n",
            "Epoch [1/3], Batch [11633/21340], Train Loss: 3.2306, Train Perplexity: 25.2940\n",
            "Epoch [1/3], Batch [11634/21340], Train Loss: 3.2647, Train Perplexity: 26.1732\n",
            "Epoch [1/3], Batch [11635/21340], Train Loss: 3.2352, Train Perplexity: 25.4106\n",
            "Epoch [1/3], Batch [11636/21340], Train Loss: 3.1752, Train Perplexity: 23.9327\n",
            "Epoch [1/3], Batch [11637/21340], Train Loss: 3.2487, Train Perplexity: 25.7562\n",
            "Epoch [1/3], Batch [11638/21340], Train Loss: 3.2315, Train Perplexity: 25.3176\n",
            "Epoch [1/3], Batch [11639/21340], Train Loss: 3.2970, Train Perplexity: 27.0308\n",
            "Epoch [1/3], Batch [11640/21340], Train Loss: 3.3737, Train Perplexity: 29.1870\n",
            "Epoch [1/3], Batch [11641/21340], Train Loss: 3.3091, Train Perplexity: 27.3604\n",
            "Epoch [1/3], Batch [11642/21340], Train Loss: 3.2162, Train Perplexity: 24.9338\n",
            "Epoch [1/3], Batch [11643/21340], Train Loss: 3.2211, Train Perplexity: 25.0549\n",
            "Epoch [1/3], Batch [11644/21340], Train Loss: 3.1638, Train Perplexity: 23.6605\n",
            "Epoch [1/3], Batch [11645/21340], Train Loss: 3.1885, Train Perplexity: 24.2527\n",
            "Epoch [1/3], Batch [11646/21340], Train Loss: 3.2681, Train Perplexity: 26.2609\n",
            "Epoch [1/3], Batch [11647/21340], Train Loss: 3.2334, Train Perplexity: 25.3645\n",
            "Epoch [1/3], Batch [11648/21340], Train Loss: 3.1549, Train Perplexity: 23.4507\n",
            "Epoch [1/3], Batch [11649/21340], Train Loss: 3.3714, Train Perplexity: 29.1195\n",
            "Epoch [1/3], Batch [11650/21340], Train Loss: 3.2622, Train Perplexity: 26.1082\n",
            "Epoch [1/3], Batch [11651/21340], Train Loss: 3.3451, Train Perplexity: 28.3640\n",
            "Epoch [1/3], Batch [11652/21340], Train Loss: 3.2035, Train Perplexity: 24.6177\n",
            "Epoch [1/3], Batch [11653/21340], Train Loss: 3.1538, Train Perplexity: 23.4259\n",
            "Epoch [1/3], Batch [11654/21340], Train Loss: 3.2465, Train Perplexity: 25.6991\n",
            "Epoch [1/3], Batch [11655/21340], Train Loss: 3.2222, Train Perplexity: 25.0825\n",
            "Epoch [1/3], Batch [11656/21340], Train Loss: 3.2200, Train Perplexity: 25.0276\n",
            "Epoch [1/3], Batch [11657/21340], Train Loss: 3.2910, Train Perplexity: 26.8684\n",
            "Epoch [1/3], Batch [11658/21340], Train Loss: 3.2690, Train Perplexity: 26.2858\n",
            "Epoch [1/3], Batch [11659/21340], Train Loss: 3.2601, Train Perplexity: 26.0530\n",
            "Epoch [1/3], Batch [11660/21340], Train Loss: 3.3131, Train Perplexity: 27.4710\n",
            "Epoch [1/3], Batch [11661/21340], Train Loss: 3.2211, Train Perplexity: 25.0566\n",
            "Epoch [1/3], Batch [11662/21340], Train Loss: 3.2236, Train Perplexity: 25.1178\n",
            "Epoch [1/3], Batch [11663/21340], Train Loss: 3.1773, Train Perplexity: 23.9831\n",
            "Epoch [1/3], Batch [11664/21340], Train Loss: 3.1816, Train Perplexity: 24.0863\n",
            "Epoch [1/3], Batch [11665/21340], Train Loss: 3.3025, Train Perplexity: 27.1794\n",
            "Epoch [1/3], Batch [11666/21340], Train Loss: 3.2092, Train Perplexity: 24.7584\n",
            "Epoch [1/3], Batch [11667/21340], Train Loss: 3.2266, Train Perplexity: 25.1936\n",
            "Epoch [1/3], Batch [11668/21340], Train Loss: 3.3255, Train Perplexity: 27.8116\n",
            "Epoch [1/3], Batch [11669/21340], Train Loss: 3.2415, Train Perplexity: 25.5718\n",
            "Epoch [1/3], Batch [11670/21340], Train Loss: 3.1658, Train Perplexity: 23.7083\n",
            "Epoch [1/3], Batch [11671/21340], Train Loss: 3.4758, Train Perplexity: 32.3228\n",
            "Epoch [1/3], Batch [11672/21340], Train Loss: 3.2803, Train Perplexity: 26.5827\n",
            "Epoch [1/3], Batch [11673/21340], Train Loss: 3.3280, Train Perplexity: 27.8827\n",
            "Epoch [1/3], Batch [11674/21340], Train Loss: 3.2242, Train Perplexity: 25.1340\n",
            "Epoch [1/3], Batch [11675/21340], Train Loss: 3.2773, Train Perplexity: 26.5038\n",
            "Epoch [1/3], Batch [11676/21340], Train Loss: 3.3504, Train Perplexity: 28.5145\n",
            "Epoch [1/3], Batch [11677/21340], Train Loss: 3.2885, Train Perplexity: 26.8033\n",
            "Epoch [1/3], Batch [11678/21340], Train Loss: 3.3104, Train Perplexity: 27.3954\n",
            "Epoch [1/3], Batch [11679/21340], Train Loss: 3.1924, Train Perplexity: 24.3468\n",
            "Epoch [1/3], Batch [11680/21340], Train Loss: 3.3539, Train Perplexity: 28.6141\n",
            "Epoch [1/3], Batch [11681/21340], Train Loss: 3.1853, Train Perplexity: 24.1755\n",
            "Epoch [1/3], Batch [11682/21340], Train Loss: 3.3527, Train Perplexity: 28.5799\n",
            "Epoch [1/3], Batch [11683/21340], Train Loss: 3.2491, Train Perplexity: 25.7683\n",
            "Epoch [1/3], Batch [11684/21340], Train Loss: 3.1871, Train Perplexity: 24.2186\n",
            "Epoch [1/3], Batch [11685/21340], Train Loss: 3.1986, Train Perplexity: 24.4994\n",
            "Epoch [1/3], Batch [11686/21340], Train Loss: 3.2007, Train Perplexity: 24.5497\n",
            "Epoch [1/3], Batch [11687/21340], Train Loss: 3.2383, Train Perplexity: 25.4908\n",
            "Epoch [1/3], Batch [11688/21340], Train Loss: 3.1671, Train Perplexity: 23.7384\n",
            "Epoch [1/3], Batch [11689/21340], Train Loss: 3.1565, Train Perplexity: 23.4885\n",
            "Epoch [1/3], Batch [11690/21340], Train Loss: 3.2910, Train Perplexity: 26.8708\n",
            "Epoch [1/3], Batch [11691/21340], Train Loss: 3.2811, Train Perplexity: 26.6056\n",
            "Epoch [1/3], Batch [11692/21340], Train Loss: 3.1857, Train Perplexity: 24.1849\n",
            "Epoch [1/3], Batch [11693/21340], Train Loss: 3.3058, Train Perplexity: 27.2711\n",
            "Epoch [1/3], Batch [11694/21340], Train Loss: 3.1884, Train Perplexity: 24.2489\n",
            "Epoch [1/3], Batch [11695/21340], Train Loss: 3.1683, Train Perplexity: 23.7675\n",
            "Epoch [1/3], Batch [11696/21340], Train Loss: 3.3297, Train Perplexity: 27.9293\n",
            "Epoch [1/3], Batch [11697/21340], Train Loss: 3.2492, Train Perplexity: 25.7695\n",
            "Epoch [1/3], Batch [11698/21340], Train Loss: 3.1955, Train Perplexity: 24.4220\n",
            "Epoch [1/3], Batch [11699/21340], Train Loss: 3.1882, Train Perplexity: 24.2439\n",
            "Epoch [1/3], Batch [11700/21340], Train Loss: 3.2078, Train Perplexity: 24.7253\n",
            "Epoch [1/3], Batch [11701/21340], Train Loss: 3.2642, Train Perplexity: 26.1584\n",
            "Epoch [1/3], Batch [11702/21340], Train Loss: 3.2379, Train Perplexity: 25.4808\n",
            "Epoch [1/3], Batch [11703/21340], Train Loss: 3.2758, Train Perplexity: 26.4652\n",
            "Epoch [1/3], Batch [11704/21340], Train Loss: 3.3182, Train Perplexity: 27.6093\n",
            "Epoch [1/3], Batch [11705/21340], Train Loss: 3.3315, Train Perplexity: 27.9791\n",
            "Epoch [1/3], Batch [11706/21340], Train Loss: 3.1910, Train Perplexity: 24.3126\n",
            "Epoch [1/3], Batch [11707/21340], Train Loss: 3.2613, Train Perplexity: 26.0843\n",
            "Epoch [1/3], Batch [11708/21340], Train Loss: 3.1560, Train Perplexity: 23.4754\n",
            "Epoch [1/3], Batch [11709/21340], Train Loss: 3.2940, Train Perplexity: 26.9510\n",
            "Epoch [1/3], Batch [11710/21340], Train Loss: 3.2869, Train Perplexity: 26.7595\n",
            "Epoch [1/3], Batch [11711/21340], Train Loss: 3.3146, Train Perplexity: 27.5113\n",
            "Epoch [1/3], Batch [11712/21340], Train Loss: 3.3008, Train Perplexity: 27.1351\n",
            "Epoch [1/3], Batch [11713/21340], Train Loss: 3.2397, Train Perplexity: 25.5257\n",
            "Epoch [1/3], Batch [11714/21340], Train Loss: 3.1978, Train Perplexity: 24.4785\n",
            "Epoch [1/3], Batch [11715/21340], Train Loss: 3.1798, Train Perplexity: 24.0416\n",
            "Epoch [1/3], Batch [11716/21340], Train Loss: 3.1741, Train Perplexity: 23.9063\n",
            "Epoch [1/3], Batch [11717/21340], Train Loss: 3.1936, Train Perplexity: 24.3755\n",
            "Epoch [1/3], Batch [11718/21340], Train Loss: 3.2180, Train Perplexity: 24.9773\n",
            "Epoch [1/3], Batch [11719/21340], Train Loss: 3.1448, Train Perplexity: 23.2154\n",
            "Epoch [1/3], Batch [11720/21340], Train Loss: 3.1893, Train Perplexity: 24.2714\n",
            "Epoch [1/3], Batch [11721/21340], Train Loss: 3.1772, Train Perplexity: 23.9784\n",
            "Epoch [1/3], Batch [11722/21340], Train Loss: 3.1814, Train Perplexity: 24.0809\n",
            "Epoch [1/3], Batch [11723/21340], Train Loss: 3.1654, Train Perplexity: 23.6972\n",
            "Epoch [1/3], Batch [11724/21340], Train Loss: 3.1912, Train Perplexity: 24.3173\n",
            "Epoch [1/3], Batch [11725/21340], Train Loss: 3.2306, Train Perplexity: 25.2942\n",
            "Epoch [1/3], Batch [11726/21340], Train Loss: 3.2279, Train Perplexity: 25.2269\n",
            "Epoch [1/3], Batch [11727/21340], Train Loss: 3.2823, Train Perplexity: 26.6363\n",
            "Epoch [1/3], Batch [11728/21340], Train Loss: 3.1471, Train Perplexity: 23.2692\n",
            "Epoch [1/3], Batch [11729/21340], Train Loss: 3.3308, Train Perplexity: 27.9619\n",
            "Epoch [1/3], Batch [11730/21340], Train Loss: 3.3120, Train Perplexity: 27.4410\n",
            "Epoch [1/3], Batch [11731/21340], Train Loss: 3.2786, Train Perplexity: 26.5381\n",
            "Epoch [1/3], Batch [11732/21340], Train Loss: 3.3047, Train Perplexity: 27.2407\n",
            "Epoch [1/3], Batch [11733/21340], Train Loss: 3.2659, Train Perplexity: 26.2050\n",
            "Epoch [1/3], Batch [11734/21340], Train Loss: 3.2693, Train Perplexity: 26.2919\n",
            "Epoch [1/3], Batch [11735/21340], Train Loss: 3.1842, Train Perplexity: 24.1483\n",
            "Epoch [1/3], Batch [11736/21340], Train Loss: 3.2747, Train Perplexity: 26.4353\n",
            "Epoch [1/3], Batch [11737/21340], Train Loss: 3.2086, Train Perplexity: 24.7450\n",
            "Epoch [1/3], Batch [11738/21340], Train Loss: 3.2414, Train Perplexity: 25.5692\n",
            "Epoch [1/3], Batch [11739/21340], Train Loss: 3.2172, Train Perplexity: 24.9587\n",
            "Epoch [1/3], Batch [11740/21340], Train Loss: 3.2810, Train Perplexity: 26.6014\n",
            "Epoch [1/3], Batch [11741/21340], Train Loss: 3.3019, Train Perplexity: 27.1633\n",
            "Epoch [1/3], Batch [11742/21340], Train Loss: 3.3314, Train Perplexity: 27.9784\n",
            "Epoch [1/3], Batch [11743/21340], Train Loss: 3.2894, Train Perplexity: 26.8264\n",
            "Epoch [1/3], Batch [11744/21340], Train Loss: 3.2000, Train Perplexity: 24.5332\n",
            "Epoch [1/3], Batch [11745/21340], Train Loss: 3.2517, Train Perplexity: 25.8349\n",
            "Epoch [1/3], Batch [11746/21340], Train Loss: 3.2242, Train Perplexity: 25.1340\n",
            "Epoch [1/3], Batch [11747/21340], Train Loss: 3.1589, Train Perplexity: 23.5443\n",
            "Epoch [1/3], Batch [11748/21340], Train Loss: 3.0637, Train Perplexity: 21.4062\n",
            "Epoch [1/3], Batch [11749/21340], Train Loss: 3.2046, Train Perplexity: 24.6460\n",
            "Epoch [1/3], Batch [11750/21340], Train Loss: 3.3044, Train Perplexity: 27.2314\n",
            "Epoch [1/3], Batch [11751/21340], Train Loss: 3.3043, Train Perplexity: 27.2283\n",
            "Epoch [1/3], Batch [11752/21340], Train Loss: 3.1820, Train Perplexity: 24.0947\n",
            "Epoch [1/3], Batch [11753/21340], Train Loss: 3.3405, Train Perplexity: 28.2320\n",
            "Epoch [1/3], Batch [11754/21340], Train Loss: 3.2607, Train Perplexity: 26.0669\n",
            "Epoch [1/3], Batch [11755/21340], Train Loss: 3.1769, Train Perplexity: 23.9731\n",
            "Epoch [1/3], Batch [11756/21340], Train Loss: 3.2477, Train Perplexity: 25.7309\n",
            "Epoch [1/3], Batch [11757/21340], Train Loss: 3.1621, Train Perplexity: 23.6199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Beam Search"
      ],
      "metadata": {
        "id": "mSEmQMRVCgi5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOnS8rz_9XGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}